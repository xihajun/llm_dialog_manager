from llm_dialog_manager import Agent

def split_transcript_with_time(full_transcript: str, chunk_duration=600):
    """
    Split timestamped transcript into chunks of specified duration.
    
    Args:
        full_transcript (str): Input transcript with timestamps in format [MM:SS -> MM:SS]
        chunk_duration (int): Duration of each chunk in seconds (default: 600 seconds = 10 minutes)
    
    Returns:
        List[Tuple[str, str, str]]: List of (start_time, end_time, chunk_text) tuples
    """
    def time_to_seconds(time_str):
        """Convert HH:MM:SS or MM:SS format to seconds"""
        parts = time_str.strip().split(':')
        if len(parts) == 2:  # MM:SS format
            minutes, seconds = map(int, parts)
            return minutes * 60 + seconds
        elif len(parts) == 3:  # HH:MM:SS format
            hours, minutes, seconds = map(int, parts)
            return hours * 3600 + minutes * 60 + seconds
        else:
            raise ValueError(f"Invalid time format: {time_str}")

    def seconds_to_time(seconds, use_hours=True):
        """Convert seconds to HH:MM:SS or MM:SS format"""
        if use_hours:
            hours = seconds // 3600
            seconds %= 3600
            minutes = seconds // 60
            seconds %= 60
            return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
        else:
            minutes = seconds // 60
            seconds %= 60
            return f"{minutes:02d}:{seconds:02d}"

    chunks = []
    current_chunk = []
    current_start = None
    current_end = None
    
    # Split transcript into lines and process each line
    lines = full_transcript.strip().split('\n')
    
    for line in lines:
        # Skip empty lines
        if not line.strip():
            continue
            
        # Extract timestamp and text
        try:
            timestamp_part = line[line.find('[')+1:line.find(']')]
            start_time, end_time = map(str.strip, timestamp_part.split('->'))
            text = line[line.find(']')+1:].strip()
            
            start_seconds = time_to_seconds(start_time)
            end_seconds = time_to_seconds(end_time)
            
            # Initialize first chunk
            if current_start is None:
                current_start = start_time
            
            # Check if we need to start a new chunk
            if current_chunk and time_to_seconds(start_time) - time_to_seconds(current_start) >= chunk_duration:
                # Save current chunk
                chunks.append((current_start, current_end, '\n'.join(current_chunk)))
                # Start new chunk
                current_chunk = []
                current_start = start_time
            
            # Add line to current chunk
            current_chunk.append(line)
            current_end = end_time
            
        except Exception as e:
            print(f"Warning: Could not parse line: {line}")
            continue
    
    # Add the last chunk if there's anything left
    if current_chunk:
        chunks.append((current_start, current_end, '\n'.join(current_chunk)))
    
    return chunks

def summarize_chunk_with_time(agent, chunk_text: str, start_time: str, end_time: str) -> str:
    """
    对单个chunk文本做摘要，让模型在输出中包含时间戳线索。
    返回时，会包含一个合并了时间信息的摘要字符串。
    """
    agent = Agent("DeepSeek-R1", memory_enabled=False)
    
    # 给系统提示
    agent.add_message("system", "你是一个专业的摘要助理，会根据时间信息来输出摘要。")

    # 用户消息：带上 chunk 的时间范围和文本
    user_prompt = f"""
以下是时间段[{start_time} - {end_time}]的转录文本，请你总结重点并保留/提炼其中提到的关键时间点、主题要点等。

转录文本:
----------------
{chunk_text}
----------------

在输出中，请给出一个大纲形式，并在标题上标明本段的时间范围。例如：
[HH:MM - HH:MM] 主要内容
  - 主要论点
  - 重要引述(如有)
  - ...
"""
    agent.add_message("user", user_prompt)

    summary = agent.generate_response()
    
    # 这里返回的 summary 会包含类似：
    # [00:00 - 00:10] ...
    #   - ...
    # (由模型生成的文本)
    return summary

def summarize_all_chunks(chunks):
    """
    第一级：对每个chunk做摘要
    第二级：再把所有chunk的摘要进行合并，生成全局大纲(包含时间顺序)
    """
    agent = Agent("DeepSeek-R1", memory_enabled=False)

    # 1) 逐段做摘要
    chunk_summaries = []
    for (start_t, end_t, text) in chunks:
        summary = summarize_chunk_with_time(agent, text, start_t, end_t)
        chunk_summaries.append((start_t, end_t, summary))

    # 2) 第二级：合并所有摘要，做出最终的全局大纲
    agent.add_message("system", "你是一个资深的文字编辑，擅长对多段摘要进行时序化合并。")

    # 把所有片段的摘要拼成一个文本
    merged = "\n\n".join(
        [f"【{i+1}】段时间 {st} - {et} 的摘要：\n{summ}" 
         for i, (st, et, summ) in enumerate(chunk_summaries)]
    )

    user_prompt = f"""
下面是若干时间段的摘要，请你将它们进行合并，生成一个按时间顺序梳理的大纲，避免重复但保留关键信息。

要求：
1. 按照实际时间顺序（从最早到最晚）组织内容；
2. 在每个时段标题或段落中保留其 [开始时间 - 结束时间] 标识；
3. 若摘要间有重叠或重复，请进行适度精炼合并，但不要遗漏重点。
4. 输出时请使用分条、分段的 outline 格式，以便阅读。

------------
{merged}
------------
"""
    agent.add_message("user", user_prompt)
    final_outline = agent.generate_response()
    
    return final_outline

if __name__ == "__main__":

    # 假设这是你的完整转录文本，里头有详细的时间戳。
    full_transcript = """
[00:00 -> 00:05]  welcome to this Cuda programming course
[00:02 -> 00:08]  where you will learn to leverage gpus
[00:05 -> 00:10]  for high performance Computing the
[00:08 -> 00:13]  course starts with an overview of the
[00:10 -> 00:16]  deep learning ecosystem and guides you
[00:13 -> 00:19]  through setting up Cuda and reviewing
[00:16 -> 00:21]  essential C and C++ Concepts you'll
[00:19 -> 00:24]  explore GPU architecture and write your
[00:21 -> 00:27]  first Cuda kernels Advanced topics
[00:24 -> 00:30]  include optimizing matrix multiplication
[00:27 -> 00:32]  and extending pie torch with practical
[00:30 -> 00:35]  applications like implementing a
[00:32 -> 00:39]  multi-layer perception for the mest data
[00:35 -> 00:41]  set Elliot Alid created this course so
[00:39 -> 00:44]  what is Cuda or compute unified device
[00:41 -> 00:46]  architecture by Nvidia uh my name is
[00:44 -> 00:48]  Elliot and I'm an instructor on free
[00:46 -> 00:50]  code Camp as well as a student studying
[00:48 -> 00:54]  for my computer science degree so in
[00:50 -> 00:56]  this course I bring to you Cuda for deep
[00:54 -> 00:57]  learning but don't let that repel you if
[00:56 -> 00:58]  you're not in deep learning because
[00:57 -> 01:01]  there's still a lot that we're going to
[00:58 -> 01:03]  be able to cover uh many other fields of
[01:01 -> 01:05]  parallel programming so this is more
[01:03 -> 01:08]  oriented for deep learning but not
[01:05 -> 01:10]  specifically aimed at it um there's
[01:08 -> 01:12]  going to be a lot covered here so uh
[01:10 -> 01:15]  I'll show what the final prodject what
[01:12 -> 01:16]  the final project is first um so that
[01:15 -> 01:17]  you can get a feel forward and see kind
[01:16 -> 01:20]  of what we're going to end up building
[01:17 -> 01:21]  by the end um and then we'll just kind
[01:20 -> 01:23]  of go from there so before we get
[01:21 -> 01:26]  started with anything crazy I should
[01:23 -> 01:27]  include a disclaimer um this course may
[01:26 -> 01:29]  not be fully up to date by the time
[01:27 -> 01:30]  you're watching this if you're watching
[01:29 -> 01:32]  this 10 years years down the line from
[01:30 -> 01:33]  when I've released it it might not all
[01:32 -> 01:36]  be the same there might be things that
[01:33 -> 01:38]  are updated the new uh compute
[01:36 -> 01:39]  capabilities might be you know way
[01:38 -> 01:41]  better there might be a bunch of
[01:39 -> 01:43]  different stuff happening so I'm not too
[01:41 -> 01:47]  sure where the ecosystem will be at in
[01:43 -> 01:49]  10 years but as of 2024 this is pretty
[01:47 -> 01:50]  much the best you're going to get so
[01:49 -> 01:52]  just trying to include that and I
[01:50 -> 01:55]  thought I'd try to make everything uh
[01:52 -> 01:57]  not entirely centered around time so you
[01:55 -> 01:59]  can go back into this version uh or or
[01:57 -> 02:01]  certain Cuda versions and reproduce all
[01:59 -> 02:03]  the same stuff it just might be a little
[02:01 -> 02:05]  bit different down the line if you're
[02:03 -> 02:08]  watching this later on so why did I
[02:05 -> 02:10]  create this course exactly well a lot of
[02:08 -> 02:12]  these performance and kernel engineering
[02:10 -> 02:14]  jobs require a lot of knowledge they
[02:12 -> 02:16]  require a lot of experience in the
[02:14 -> 02:18]  industry uh and it's just really hard to
[02:16 -> 02:20]  get up to that point where you're able
[02:18 -> 02:23]  to compete with the top and the best of
[02:20 -> 02:24]  the best performance Engineers so these
[02:23 -> 02:27]  are the people that are writing the
[02:24 -> 02:30]  training runs for like gbt 4 gbt 5 all
[02:27 -> 02:32]  of this um you need a lot of skill to
[02:30 -> 02:34]  optimize a massive neural network
[02:32 -> 02:37]  training run and inference on a larg
[02:34 -> 02:40]  data center or compute cluster so this
[02:37 -> 02:42]  aims to prevent some of that manual
[02:40 -> 02:45]  weaving on your part still encouraging
[02:42 -> 02:47]  you to do so U on your own but prevent
[02:45 -> 02:49]  some of that hardcore labor of going
[02:47 -> 02:51]  through and really figuring things out
[02:49 -> 02:53]  on your own from scratch uh that's one
[02:51 -> 02:56]  of the reasons why I created this
[02:53 -> 02:58]  another one is like generally speaking
[02:56 -> 03:00]  the point of writing GPU kernels or
[02:58 -> 03:02]  playing with code at all on the GPU is
[03:00 -> 03:05]  to run something faster so if you have a
[03:02 -> 03:08]  nested Loop um you know it's like 4 I in
[03:05 -> 03:11]  range 4J in range four 4K in range
[03:08 -> 03:12]  whatever however many you want to put uh
[03:11 -> 03:15]  essentially what parallel programming
[03:12 -> 03:17]  and Cuda allow us to do is unroll those
[03:15 -> 03:19]  so if you take like for example four ion
[03:17 -> 03:22]  range you could take each little thing
[03:19 -> 03:25]  in that and run that instruction on a
[03:22 -> 03:27]  different CA cor so if you have 10,000
[03:25 -> 03:29]  cacor and you have 10,000 different
[03:27 -> 03:32]  iterations in your Loop then you can
[03:29 -> 03:35]  affect L do each iteration in a single
[03:32 -> 03:37]  instruction or a single thread on on the
[03:35 -> 03:39]  GPU so this is some of the things that
[03:37 -> 03:42]  allows us to do you're going to use your
[03:39 -> 03:44]  job of uh you're going to use your your
[03:42 -> 03:45]  knowledge of GPU architecture kernel
[03:44 -> 03:46]  launch configurations and a bunch of
[03:45 -> 03:48]  other cool stuff we end up learning in
[03:46 -> 03:51]  this course to make that code run as
[03:48 -> 03:54]  fast as possible uh and then the last
[03:51 -> 03:56]  one is really there's so much data
[03:54 -> 03:58]  nowadays they say we have way too much
[03:56 -> 04:00]  data but very little cleaned data I've
[03:58 -> 04:03]  taken everything from all the other
[04:00 -> 04:06]  video courses everything on the internet
[04:03 -> 04:08]  and YouTube uh and I put them in a
[04:06 -> 04:10]  single course so I filtered out a bunch
[04:08 -> 04:12]  of the nonsense a lot of you know the
[04:10 -> 04:14]  old stuff a lot of the new stuff that
[04:12 -> 04:17]  maybe isn't covered as well and kind of
[04:14 -> 04:19]  just projected into this one Masterpiece
[04:17 -> 04:21]  so this includes topics covered by paid
[04:19 -> 04:23]  courses as well I haven't actually paid
[04:21 -> 04:24]  for them but I kind of just looked at
[04:23 -> 04:25]  you know what are the chapters that they
[04:24 -> 04:28]  cover and then include some of those
[04:25 -> 04:30]  important Concepts in this course um I
[04:28 -> 04:31]  do have links for YouTube videos and all
[04:30 -> 04:33]  of these resources which I've gone
[04:31 -> 04:34]  through only the high quality ones but
[04:33 -> 04:36]  I've gone through a lot of these videos
[04:34 -> 04:39]  and resources and these are all going to
[04:36 -> 04:41]  be uh put in links inside of the um
[04:39 -> 04:43]  GitHub Link in the description so
[04:41 -> 04:45]  everything you need is going to be there
[04:43 -> 04:48]  um and I put a lot of all of those links
[04:45 -> 04:51]  in that um in that link so what are some
[04:48 -> 04:52]  use cases for Cuda parallel GPU
[04:51 -> 04:55]  programming what are some of the use
[04:52 -> 04:57]  cases for this well you have graphics
[04:55 -> 04:58]  and rate tracing so the computer
[04:57 -> 05:01]  Graphics that you're seeing in video
[04:58 -> 05:03]  games um you know user interfaces all of
[05:01 -> 05:06]  this you have fluid simulation for like
[05:03 -> 05:09]  physics um and modeling you know engine
[05:06 -> 05:10]  Dynamics you have video editing so the
[05:09 -> 05:13]  video that I'm editing for this right
[05:10 -> 05:16]  now is using uh parallel Computing to
[05:13 -> 05:18]  render uh crypto mining which a lot of
[05:16 -> 05:20]  you might be doing already that's going
[05:18 -> 05:22]  to be using uh you know your GPU
[05:20 -> 05:25]  hardware and some of the advantages of
[05:22 -> 05:27]  that to like mine through the the crypto
[05:25 -> 05:29]  mining problems and then you have 3D
[05:27 -> 05:30]  modeling and software like blender so
[05:29 -> 05:32]  when you have a bunch of different
[05:30 -> 05:33]  points going on and you have to render
[05:32 -> 05:36]  things it's essentially the same as
[05:33 -> 05:40]  video editing but just um 3D instead of
[05:36 -> 05:42]  2D so the last one which you probably
[05:40 -> 05:44]  guessed it already is deep learning so
[05:42 -> 05:46]  the number one use case for Cuda right
[05:44 -> 05:49]  now is primarily what I'll be covering
[05:46 -> 05:50]  in this course which is deep learning so
[05:49 -> 05:54]  we're not going to go as as deep into
[05:50 -> 05:56]  like say convolutions but uh to kind of
[05:54 -> 05:58]  understand how to optimize an algorithm
[05:56 -> 06:00]  like matrix multiplication uh we're
[05:58 -> 06:02]  going to go quite in depth with that
[06:00 -> 06:04]  so now you might ask Elliot what are the
[06:02 -> 06:06]  requirements or the prerequisites for
[06:04 -> 06:08]  this course so there are some that are
[06:06 -> 06:11]  more intellectual and academic and there
[06:08 -> 06:13]  are some that aren't so this is strictly
[06:11 -> 06:15]  for NVIDIA gpus in case you didn't catch
[06:13 -> 06:17]  on to that earlier um if you don't have
[06:15 -> 06:19]  one you can always consider renting uh
[06:17 -> 06:21]  the cheapest ones in the cloud um I
[06:19 -> 06:23]  advise you to look into the pricing
[06:21 -> 06:26]  before giving a definite no on the
[06:23 -> 06:28]  pricing for some of these Cloud gpus um
[06:26 -> 06:30]  at first I was actually surprised how
[06:28 -> 06:33]  low the cost was for some cloud
[06:30 -> 06:35]  instances um especially the non-comp
[06:33 -> 06:38]  compute demanding ones so if you have
[06:35 -> 06:39]  like only a CPU or like a ram intensive
[06:38 -> 06:42]  machine it might actually cost
[06:39 -> 06:44]  significantly less than one with gpus on
[06:42 -> 06:46]  it um the gpus one are still very cheap
[06:44 -> 06:49]  you can use things like vast AI which
[06:46 -> 06:50]  I'll cover a little bit more um you can
[06:49 -> 06:52]  use this for actually getting really
[06:50 -> 06:55]  cheap uh consumer grade Hardware that
[06:52 -> 06:57]  you can SSH into in the cloud um and
[06:55 -> 07:00]  then just do all of your experiments and
[06:57 -> 07:02]  go through the course on that
[07:00 -> 07:05]  you can continue uh you can continue
[07:02 -> 07:07]  running with any you know NVIDIA GTX RTX
[07:05 -> 07:09]  or data center level gpus so all of the
[07:07 -> 07:11]  Nvidia cards are pretty much supported
[07:09 -> 07:13]  for this uh maybe like the lower ones
[07:11 -> 07:16]  that are like 15 years old those might
[07:13 -> 07:18]  not work um but generally if you have
[07:16 -> 07:21]  like a GTX like 1660 or something like
[07:18 -> 07:24]  that it's like it's going to be fine
[07:21 -> 07:26]  um as for course prequisites Python
[07:24 -> 07:28]  Programming will help in understanding
[07:26 -> 07:30]  while we're implementing in lower
[07:28 -> 07:32]  languages so
[07:30 -> 07:34]  um just understanding the whole
[07:32 -> 07:36]  programming uh Concepts is really what's
[07:34 -> 07:38]  going to be needed here again all these
[07:36 -> 07:42]  different languages is just like a
[07:38 -> 07:43]  change in syntax right so um you know
[07:42 -> 07:46]  we're going to use basic differentiation
[07:43 -> 07:48]  and Vector calculus uh that'll make
[07:46 -> 07:50]  learning easier if you know it already
[07:48 -> 07:52]  um it's really only required for
[07:50 -> 07:53]  intuition behind back propagation and
[07:52 -> 07:56]  some of the stuff we're going to use to
[07:53 -> 07:58]  build neural networks from scratch um
[07:56 -> 08:00]  linear algebra will definitely make your
[07:58 -> 08:02]  life easier by not having to learn
[08:00 -> 08:05]  fundamental algorithms from scratch so
[08:02 -> 08:07]  like if you're not really intuitively um
[08:05 -> 08:10]  you know into matrix multiplication yet
[08:07 -> 08:12]  if you haven't really uh you know gone
[08:10 -> 08:14]  into that extensively it might be a
[08:12 -> 08:16]  little hard for you to catch up uh but
[08:14 -> 08:18]  matrix multiplication is very easy it's
[08:16 -> 08:20]  quite trivial in retrospect it's very
[08:18 -> 08:22]  it's very easy to understand um but just
[08:20 -> 08:24]  the intuition there and optimizing it
[08:22 -> 08:26]  might be a little hard if you haven't
[08:24 -> 08:28]  worked with it a lot already
[08:26 -> 08:31]  um then if you really care I would
[08:28 -> 08:34]  recommend just reviewing you know Matrix
[08:31 -> 08:36]  uh transpose matrix multiplication chain
[08:34 -> 08:38]  rule from calculus and then difference
[08:36 -> 08:40]  between gradients and derivatives um
[08:38 -> 08:42]  there's maybe a few more that I missed
[08:40 -> 08:43]  but those are like the general ideas
[08:42 -> 08:46]  that you're going to need for going into
[08:43 -> 08:48]  this um and then just a heads up uh if
[08:46 -> 08:50]  you are in a Windows machine this might
[08:48 -> 08:54]  be a little harder for you so I do have
[08:50 -> 08:56]  a little setup guide on Windows Hardware
[08:54 -> 08:58]  um but I I do everything here on on
[08:56 -> 09:00]  Ubuntu Linux so this is what I'm running
[08:58 -> 09:01]  uh just on my local machine here and
[09:00 -> 09:05]  this is what we're going to go through
[09:01 -> 09:08]  the course with um you can always use uh
[09:05 -> 09:11]  WSL on Windows to simulate a a Linux
[09:08 -> 09:13]  system or you can use uh Docker so
[09:11 -> 09:15]  Docker is an awesome tool that'll allow
[09:13 -> 09:18]  you to essentially fire up uh a little
[09:15 -> 09:20]  simulated Linux machine uh just in your
[09:18 -> 09:22]  terminal on Windows and you can just do
[09:20 -> 09:24]  everything through that uh I think it
[09:22 -> 09:26]  supports Nvidia gpus directly through
[09:24 -> 09:28]  Windows I'm not entirely sure yet I
[09:26 -> 09:30]  haven't tested that but um if you're on
[09:28 -> 09:32]  a Windows machine machine I would
[09:30 -> 09:35]  recommend uh WSL or
[09:32 -> 09:37]  Docker if you do run into errors or
[09:35 -> 09:40]  issues throughout this uh I do suggest
[09:37 -> 09:43]  you you check GitHub stack Overflow
[09:40 -> 09:45]  Nvidia developer forums pytorch docks uh
[09:43 -> 09:47]  if your issu is related to any of this
[09:45 -> 09:48]  course material so you know you have a
[09:47 -> 09:50]  lot of resources at your disposal if you
[09:48 -> 09:51]  need to resolve an error that doesn't
[09:50 -> 09:53]  come up in the course material uh you
[09:51 -> 09:55]  also have really powerful language
[09:53 -> 09:56]  models to use there's a lot of language
[09:55 -> 09:58]  models that have been released recently
[09:56 -> 10:00]  that are really really good at solving
[09:58 -> 10:02]  and addressing coding problem s so I do
[10:00 -> 10:06]  suggest you try those out um if all if
[10:02 -> 10:08]  all goes wrong right um all the all the
[10:06 -> 10:10]  code and notes for this are kept in the
[10:08 -> 10:11]  GitHub repo in the description the
[10:10 -> 10:13]  ecosystem is going to change all the
[10:11 -> 10:16]  time so in case this video isn't up to
[10:13 -> 10:17]  date uh the GitHub repo will be because
[10:16 -> 10:20]  I'm able to push that and actually make
[10:17 -> 10:22]  changes so if something is a little off
[10:20 -> 10:24]  in here you might want to go check in
[10:22 -> 10:25]  the repo and see like what it actually
[10:24 -> 10:27]  looks like so that you can actually
[10:25 -> 10:28]  write it properly and maybe there's a
[10:27 -> 10:32]  more optimized version thing things will
[10:28 -> 10:34]  change but you get the point uh I do
[10:32 -> 10:36]  suggest following uh the repo for
[10:34 -> 10:39]  maintaining a structured learning
[10:36 -> 10:41]  approach I include excal draw diagrams
[10:39 -> 10:43]  so this is going to help illustrate like
[10:41 -> 10:45]  high level ideas how we're going to
[10:43 -> 10:47]  approach things uh as well as how to do
[10:45 -> 10:50]  things on the level of Kernel
[10:47 -> 10:52]  optimization so all the way top down all
[10:50 -> 10:53]  of it excal draw is awesome for
[10:52 -> 10:55]  illustrating things and it's completely
[10:53 -> 10:58]  free so all the diagrams there will be
[10:55 -> 11:00]  included in the in the GitHub repo uh
[10:58 -> 11:03]  and in the course too
[11:00 -> 11:04]  um you know you can always uh reach out
[11:03 -> 11:07]  to me through my Discord server which
[11:04 -> 11:08]  will also be in the GitHub repo um and
[11:07 -> 11:10]  you can reach out to me through there
[11:08 -> 11:11]  and talk with the community there's
[11:10 -> 11:13]  going to be a lot of other students
[11:11 -> 11:16]  learning there's going to be a dedicated
[11:13 -> 11:18]  set of channels for this so in case you
[11:16 -> 11:20]  get stuck or wanted to discuss something
[11:18 -> 11:22]  or just have a cool chat in the server
[11:20 -> 11:24]  uh you can totally join that I do want
[11:22 -> 11:27]  to note early on that this course isn't
[11:24 -> 11:29]  on Cuda only so there's a few things
[11:27 -> 11:31]  that I cover outside of it including
[11:29 -> 11:34]  pytorch referencing uh going into like
[11:31 -> 11:37]  Triton and c and C++ with like
[11:34 -> 11:39]  externally not including Cuda just to
[11:37 -> 11:41]  you know help illustrate things on how
[11:39 -> 11:44]  how that the naive version of an
[11:41 -> 11:45]  algorithm works but uh so there's
[11:44 -> 11:47]  there's the code side and then there's
[11:45 -> 11:49]  also um I'm going to provide some
[11:47 -> 11:52]  prerequisites or not even prerequisites
[11:49 -> 11:54]  but rather just uh a good understanding
[11:52 -> 11:57]  about the whole deep learning ecosystem
[11:54 -> 11:59]  so this is actually what one of the next
[11:57 -> 12:01]  chapters is going to be about is how
[11:59 -> 12:03]  does the whole ecosystem work and where
[12:01 -> 12:05]  can I apply Cuda it would be a little
[12:03 -> 12:06]  silly of me to say here's how you
[12:05 -> 12:09]  optimize a kernel and make it run really
[12:06 -> 12:11]  really fast on your Hardware but not
[12:09 -> 12:14]  actually give you some solid use cases
[12:11 -> 12:15]  for that so you might already know what
[12:14 -> 12:16]  the use case is but in case you're just
[12:15 -> 12:19]  trying to learn Cuda and you might look
[12:16 -> 12:22]  at some ways that you can apply it I
[12:19 -> 12:24]  provide that Resource as well so spoiler
[12:22 -> 12:26]  alert but some takeaways you might get
[12:24 -> 12:28]  from this course is that through
[12:26 -> 12:30]  experiment experimentation and research
[12:28 -> 12:32]  you'll learn that the main GPU
[12:30 -> 12:34]  performance bottleneck is memory
[12:32 -> 12:36]  bandwidth so in deep learning we have
[12:34 -> 12:39]  these giant inscrutable matrices that
[12:36 -> 12:41]  cannot fit into the onchip memory at
[12:39 -> 12:44]  once so think about if you have like a
[12:41 -> 12:46]  giant cluster of gpus and each of them
[12:44 -> 12:48]  have really really fast tensor cores
[12:46 -> 12:50]  these are like super optimized for doing
[12:48 -> 12:52]  you know tensor operations in deep
[12:50 -> 12:55]  learning um but if you're doing these
[12:52 -> 12:58]  across many gpus you really have to
[12:55 -> 13:00]  exchange and and and mix and manage
[12:58 -> 13:03]  information between them so you end up
[13:00 -> 13:05]  sending electrons uh you know from this
[13:03 -> 13:07]  node to this node to this node to right
[13:05 -> 13:10]  and there's a lot of this communication
[13:07 -> 13:12]  that's going on so you really get a ton
[13:10 -> 13:14]  of speed from the compute inside of the
[13:12 -> 13:16]  chips but when it comes to communicating
[13:14 -> 13:18]  there's actually a a pretty big
[13:16 -> 13:22]  bottleneck there and that's you know one
[13:18 -> 13:25]  thing that you might take away from this
[13:22 -> 13:27]  um there's also on chip constraints too
[13:25 -> 13:31]  so you have like GPU vram which is going
[13:27 -> 13:33]  to be uh you know comparatively slow to
[13:31 -> 13:35]  what the on chip stuff is so vram is
[13:33 -> 13:37]  like off the actual you know cores and
[13:35 -> 13:39]  all this and then it has to communicate
[13:37 -> 13:41]  with the cores and all the the the
[13:39 -> 13:43]  shared memory on chip and all the
[13:41 -> 13:45]  registers and that ends up being a ball
[13:43 -> 13:49]  neck too so it's not just the the
[13:45 -> 13:51]  massive um the massive matrices
[13:49 -> 13:53]  communicating across a lot of gpus it's
[13:51 -> 13:54]  actually a lot of the onchip
[13:53 -> 13:56]  communication too so there's multiple
[13:54 -> 13:59]  bottleneck that that's arise or that
[13:56 -> 14:01]  that arise um but these are just things
[13:59 -> 14:03]  that you'll end up coming across and and
[14:01 -> 14:06]  being able to address later on through
[14:03 -> 14:07]  optimizations another key takeaway is
[14:06 -> 14:10]  would be to take an existing
[14:07 -> 14:11]  implementation and make it faster so a
[14:10 -> 14:13]  lot of the times you'll see a new
[14:11 -> 14:15]  research paper come out and you'll see a
[14:13 -> 14:18]  really cool algorithm but you might not
[14:15 -> 14:19]  know exactly how it works and so or or
[14:18 -> 14:21]  you maybe know maybe you know how it
[14:19 -> 14:23]  works and you just want to make it fast
[14:21 -> 14:25]  and you want to integrate it into Pi
[14:23 -> 14:26]  torch for example so this is something
[14:25 -> 14:28]  we're actually going to do in this
[14:26 -> 14:29]  course is we're going to uh we're going
[14:28 -> 14:31]  to build up uh an algorithm and we're
[14:29 -> 14:33]  going to optimize it and then we're GNA
[14:31 -> 14:35]  actually Port it into a pytorch
[14:33 -> 14:38]  extension so that you can call it in
[14:35 -> 14:40]  Python which is super cool um but just
[14:38 -> 14:42]  learning how to integrate your own
[14:40 -> 14:45]  research into things to make them faster
[14:42 -> 14:46]  to have it operate at production scale
[14:45 -> 14:48]  um these are some really important
[14:46 -> 14:50]  things that you'll have to do when you
[14:48 -> 14:55]  start working you know very deeply with
[14:50 -> 14:57]  Cuda um another thing is karpathy LL M.C
[14:55 -> 14:59]  a lot of you have probably heard of this
[14:57 -> 15:04]  um if you go search up LL M.C
[14:59 -> 15:06]  uh LL M.C on uh on you not not on
[15:04 -> 15:08]  YouTube on Google um you'll come across
[15:06 -> 15:12]  guy named Andre kpoy and he pretty much
[15:08 -> 15:15]  built up a giant gbt2 training run in C
[15:12 -> 15:18]  from scratch so it uses C and Cuda and
[15:15 -> 15:20]  all of it there's a ton of stuff in it
[15:18 -> 15:22]  and I really felt like it's hard to
[15:20 -> 15:24]  understand that at first um you know as
[15:22 -> 15:27]  someone who's not like super super
[15:24 -> 15:30]  enriched uh and have having done Cuda
[15:27 -> 15:31]  for like 20 years um
[15:30 -> 15:34]  it's kind of hard to understand that at
[15:31 -> 15:35]  first so having a really nice basis like
[15:34 -> 15:38]  this where you can actually understand
[15:35 -> 15:40]  how to use Cuda and where the where the
[15:38 -> 15:42]  real uh benefits are from it and how to
[15:40 -> 15:45]  use it that will allow you to read and
[15:42 -> 15:46]  approach kpoe lm. see a little better so
[15:45 -> 15:48]  that was one of the reasons why I
[15:46 -> 15:51]  actually made this is to make it easier
[15:48 -> 15:54]  for people to go into llm Doc and
[15:51 -> 15:56]  understand what's going on so in the
[15:54 -> 15:59]  GitHub link and the notion document
[15:56 -> 16:01]  inside of my GitHub repo uh you will see
[15:59 -> 16:04]  this in the intro section so just a
[16:01 -> 16:06]  bunch of cool videos on how uh Cuda
[16:04 -> 16:09]  Works how Transformers work a bunch of
[16:06 -> 16:11]  just really cool fun videos to you know
[16:09 -> 16:14]  really get you motivated and upbeat on
[16:11 -> 16:17]  uh all of this so got some technical
[16:14 -> 16:19]  stuff we got some fun videos by fireship
[16:17 -> 16:21]  um but generally speaking these are just
[16:19 -> 16:23]  some cool resources you can check out uh
[16:21 -> 16:25]  Cuda programming kudam mode is a really
[16:23 -> 16:27]  good server actually I highly recommend
[16:25 -> 16:29]  you join this it's just a Discord
[16:27 -> 16:32]  community of a bunch of people who are
[16:29 -> 16:34]  really into Cuda so I believe Andre gpoy
[16:32 -> 16:36]  is in here A bunch of really cool uh you
[16:34 -> 16:38]  know coders a bunch of Engineers are in
[16:36 -> 16:40]  here just to discussing how to uh how to
[16:38 -> 16:42]  get certain kernels working and and
[16:40 -> 16:45]  generally just Cuda stuff um hence why
[16:42 -> 16:47]  it's called cuda mode right so uh really
[16:45 -> 16:49]  cool server I highly recommend you join
[16:47 -> 16:53]  that as well as my server which is also
[16:49 -> 16:56]  in the GitHub repo but that's
[16:53 -> 16:57]  that so now we're going to go into a
[16:56 -> 17:00]  little bit about the Deep learning
[16:57 -> 17:01]  ecosystem right now so obviously this
[17:00 -> 17:03]  going this is not going to be up to date
[17:01 -> 17:06]  in five years so just you know take this
[17:03 -> 17:08]  with a grain of salt this is not uh this
[17:06 -> 17:10]  is not everything this is just what I
[17:08 -> 17:12]  found interesting to look at and focus
[17:10 -> 17:13]  on and and to be aware of in the
[17:12 -> 17:14]  ecosystem and how you can sort of
[17:13 -> 17:17]  interconnect things and understand
[17:14 -> 17:19]  what's going on so this doesn't actually
[17:17 -> 17:21]  go over anything highly technical with
[17:19 -> 17:23]  Cuda but I thought it's better to show
[17:21 -> 17:25]  you the ecosystem rather than just
[17:23 -> 17:26]  entering technical details blindly like
[17:25 -> 17:29]  if we just dump straight into Cuda
[17:26 -> 17:33]  kernels um you won't know how to connect
[17:29 -> 17:34]  the dots later on so when we uh when
[17:33 -> 17:35]  we're actually building out good
[17:34 -> 17:37]  algorithms it's like okay now you have
[17:35 -> 17:40]  the skills to do this where do you apply
[17:37 -> 17:42]  that so this is this is what that aims
[17:40 -> 17:44]  to give you just a bit of background um
[17:42 -> 17:46]  understanding the ecosystem will help
[17:44 -> 17:47]  you map out everything properly and it
[17:46 -> 17:49]  provides that initial motivation to
[17:47 -> 17:51]  learn so some parts are going to get
[17:49 -> 17:53]  really hard and when you have that
[17:51 -> 17:54]  higher level motivation to see like okay
[17:53 -> 17:56]  this is what I can actually build once I
[17:54 -> 17:59]  learn how to do this instead of just
[17:56 -> 18:04]  let's learn Cuda blindly that that seems
[17:59 -> 18:05]  a little naive um so going into it with
[18:04 -> 18:07]  like understanding what to do later on
[18:05 -> 18:11]  or what you can do I think is really
[18:07 -> 18:15]  important um again don't feel free uh
[18:11 -> 18:16]  don't don't feel binded to just watch uh
[18:15 -> 18:19]  watch me talk about a subject for 20
[18:16 -> 18:21]  hours um you may limit your learning if
[18:19 -> 18:23]  you just force yourself to sit down and
[18:21 -> 18:25]  and just just watch and listen to what
[18:23 -> 18:26]  I'm saying um I do encourage you to go
[18:25 -> 18:27]  down rabbit holes so if you find
[18:26 -> 18:30]  something that interests you in this
[18:27 -> 18:31]  section or other ones just totally just
[18:30 -> 18:34]  go down there that that's where you
[18:31 -> 18:36]  learn a ton right um but anyways I've
[18:34 -> 18:39]  I've organized this into several
[18:36 -> 18:42]  sections so research
[18:39 -> 18:43]  production um low level inference for
[18:42 -> 18:45]  Edge
[18:43 -> 18:48]  Computing ease of
[18:45 -> 18:51]  use compilers and
[18:48 -> 18:54]  miscellaneous so we start up at the top
[18:51 -> 18:57]  here was the easy ones we have pytorch
[18:54 -> 18:58]  we have pflow we have Jacks and fireship
[18:57 -> 19:01]  has videos on all these These are very
[18:58 -> 19:02]  well documented um I'll let you you know
[19:01 -> 19:03]  you can kind of just like read through
[19:02 -> 19:05]  these I'm not going to go over every
[19:03 -> 19:09]  single bullet point cuz it's already
[19:05 -> 19:12]  here um but yeah you have you have mlx
[19:09 -> 19:15]  developed by Apple for Apple silicon
[19:12 -> 19:17]  open source uh for Apple
[19:15 -> 19:20]  devices P torch lightning is like P
[19:17 -> 19:21]  torch but reduces boiler plate code so
[19:20 -> 19:23]  there's a Reddit post here which was
[19:21 -> 19:25]  interesting um when you do like when you
[19:23 -> 19:28]  set like your tf32 Precision to do
[19:25 -> 19:30]  tensor core computations in in pytorch
[19:28 -> 19:31]  um like that's boiler plate code so pie
[19:30 -> 19:32]  torch lightning is actually going to
[19:31 -> 19:34]  reduce that and it's going to remove
[19:32 -> 19:36]  that boiler plate so you don't have to
[19:34 -> 19:38]  worry about like including all those
[19:36 -> 19:42]  little optimizations and and and uh and
[19:38 -> 19:43]  hacks so when it comes to production
[19:42 -> 19:46]  this is there's typically two things
[19:43 -> 19:48]  that fall in here so you have training
[19:46 -> 19:49]  and inference and some of these will
[19:48 -> 19:52]  support two of them together some of
[19:49 -> 19:56]  them will just support one or the other
[19:52 -> 19:58]  um so in here we have VM which is quite
[19:56 -> 20:01]  interesting um
[19:58 -> 20:06]  search a BLM on
[20:01 -> 20:06]  GitHub actually go down and we can see
[20:06 -> 20:13]  um where did it go yeah LM impr and
[20:10 -> 20:16]  serving and then where did it go
[20:13 -> 20:19]  performance yeah so performance
[20:16 -> 20:20]  Benchmark againsts tensor rtln which is
[20:19 -> 20:22]  the next one that I'll actually talk
[20:20 -> 20:24]  about here um but they they Implement a
[20:22 -> 20:26]  bunch of like very like essentially
[20:24 -> 20:29]  Hardware GPU optimizations that we may
[20:26 -> 20:32]  talk about later on um
[20:29 -> 20:36]  but BLM is great um tensor RT is pretty
[20:32 -> 20:38]  much tensor runtime by Nvidia and they
[20:36 -> 20:40]  have a tensor RT LM so it's for like
[20:38 -> 20:42]  inferencing language models with all of
[20:40 -> 20:45]  these you know all these different
[20:42 -> 20:47]  optimizations um specifically for llm
[20:45 -> 20:51]  inference
[20:47 -> 20:53]  now Triton is Triton is something we're
[20:51 -> 20:55]  actually going to cover a bit more Tron
[20:53 -> 20:56]  was developed by opening eye we go here
[20:55 -> 20:59]  you can see
[20:56 -> 21:01]  this uh it tells you about like what the
[20:59 -> 21:03]  heck Triton is like what the motivation
[21:01 -> 21:05]  was where it came from um but if we look
[21:03 -> 21:08]  at this paper from Harvard this is
[21:05 -> 21:09]  actually where Triton originated from so
[21:08 -> 21:11]  try an Intermediate Language and
[21:09 -> 21:13]  compiler for child neural net
[21:11 -> 21:15]  computations child neural net
[21:13 -> 21:16]  computations is the key here this is
[21:15 -> 21:18]  where a lot of the performance comes
[21:16 -> 21:21]  from and you'll see this later on when
[21:18 -> 21:23]  we build fast algorithms tiling is where
[21:21 -> 21:25]  you have like a giant problem where
[21:23 -> 21:27]  you're you have to do linear algebra
[21:25 -> 21:31]  operations like on tensors and you have
[21:27 -> 21:33]  to do them fast on parallel uh parallel
[21:31 -> 21:35]  processors like gpus and so what you can
[21:33 -> 21:37]  do is you can tile The Matrix into a
[21:35 -> 21:40]  bunch of little like squares like
[21:37 -> 21:42]  subsquares and you can you can multiply
[21:40 -> 21:44]  them together so this way you don't have
[21:42 -> 21:46]  to do like an entire thing at once and
[21:44 -> 21:48]  then reserve it and and worry about all
[21:46 -> 21:50]  that stuff you can literally just select
[21:48 -> 21:53]  blocks and the parallel processors in
[21:50 -> 21:55]  Cuda are extremely good at processing
[21:53 -> 21:56]  those blocks because of the Cuda
[21:55 -> 22:00]  architecture which we will talk about
[21:56 -> 22:01]  later um but but try is interesting this
[22:00 -> 22:03]  is a whole paper which I'm not going to
[22:01 -> 22:07]  dig into in this course but a lot of
[22:03 -> 22:09]  interesting uh both compiler and um you
[22:07 -> 22:12]  know speed ups that you get from
[22:09 -> 22:13]  approaching things with a with a tiled
[22:12 -> 22:15]  um
[22:13 -> 22:18]  philosophy
[22:15 -> 22:20]  now toor just some other optimizations
[22:18 -> 22:23]  we'll get in performance is torch do
[22:20 -> 22:26]  compile so you do torch do compile and
[22:23 -> 22:28]  then Open Bracket model close bracket
[22:26 -> 22:31]  and this will literally just increase
[22:28 -> 22:32]  performance 30% out of the box it'll
[22:31 -> 22:34]  take that Dynamic graph that P torch
[22:32 -> 22:36]  builds and it'll statically it'll snap
[22:34 -> 22:38]  it into a static representation for
[22:36 -> 22:40]  production because we're using it for
[22:38 -> 22:43]  production uh and it'll just apply
[22:40 -> 22:44]  optimizations all all around um which we
[22:43 -> 22:46]  will dig more into this course like an
[22:44 -> 22:48]  example would be like kernel Fusion
[22:46 -> 22:50]  where instead of you know doing a
[22:48 -> 22:51]  separate function for each for each step
[22:50 -> 22:53]  you're like combining two or three
[22:51 -> 22:55]  operations into one single function uh
[22:53 -> 22:58]  and that like reduces some overhead comp
[22:55 -> 22:59]  computation that you have to do there so
[22:58 -> 23:00]  uh just a bunch of these little
[22:59 -> 23:02]  optimizations that torch talk compile
[23:00 -> 23:05]  does uh extremely recommend for
[23:02 -> 23:07]  production uh torch script is a little
[23:05 -> 23:09]  older but there's an article here on
[23:07 -> 23:11]  Torch script
[23:09 -> 23:15]  so torch
[23:11 -> 23:16]  script um I haven't actually used but
[23:15 -> 23:18]  there are some more discussions here
[23:16 -> 23:20]  that you can follow um I know it's a
[23:18 -> 23:22]  little older so I typically just resort
[23:20 -> 23:23]  to Tor shock comp pile for most things
[23:22 -> 23:26]  um but it's it's here in case you want
[23:23 -> 23:27]  that and then Onyx runtime is also
[23:26 -> 23:29]  interesting I should probably should
[23:27 -> 23:32]  have put Onyx before Onyx runtime but it
[23:29 -> 23:35]  is what it is um Onyx runtime is pretty
[23:32 -> 23:37]  much on top of Onyx so you have this
[23:35 -> 23:39]  this thing called Onyx which exports a
[23:37 -> 23:40]  model from either pytorch or tensor
[23:39 -> 23:44]  floor whatever you want down to this
[23:40 -> 23:44]  Onyx format that's intercompatibility
[23:58 -> 24:02]  uh it's like a Onyx file extension that
[24:00 -> 24:06]  you use for storing neural net uh
[24:02 -> 24:08]  weights and tensors so uh Onyx runtime
[24:06 -> 24:12]  essentially takes that and allows you to
[24:08 -> 24:14]  just run it faster so that was built by
[24:12 -> 24:16]  Microsoft uh and then a cool little
[24:14 -> 24:18]  project I came across and that chat jbt
[24:16 -> 24:21]  recommended I put into this course was
[24:18 -> 24:25]  detectron 2 so it's uh it's interesting
[24:21 -> 24:27]  you might find it useful but um
[24:25 -> 24:30]  developed by Facebook and it's
[24:27 -> 24:33]  essentially a computer vision library
[24:30 -> 24:35]  that uses uh image detection and
[24:33 -> 24:37]  segmentation algorithms
[24:35 -> 24:39]  so just a bunch of like really cool
[24:37 -> 24:41]  computer vision stuff that it has bunch
[24:39 -> 24:44]  of different neural net architectures
[24:41 -> 24:45]  and hats that it employs and it's just
[24:44 -> 24:48]  one of those fun things that you might
[24:45 -> 24:50]  want to mess around with um then we go
[24:48 -> 24:53]  to low level which is what this course
[24:50 -> 24:55]  is based on in case you haven't read the
[24:53 -> 24:58]  the the title it's on
[24:55 -> 25:01]  Cuda uh Cuda is compute unified device
[24:58 -> 25:03]  architect Ure uh programming language uh
[25:01 -> 25:05]  programming platform rather for NVIDIA
[25:03 -> 25:07]  gpus um and there's a bunch of stuff
[25:05 -> 25:11]  which we'll dig into later
[25:07 -> 25:13]  um rock M qu equivalent for AMD gpus and
[25:11 -> 25:17]  then you have opencl so this is more
[25:13 -> 25:19]  General um built for CPUs gpus uh dsps
[25:17 -> 25:21]  other types of Hardware so just like a
[25:19 -> 25:24]  general purpose Computing language open
[25:21 -> 25:27]  open source um and then we have Edge
[25:24 -> 25:28]  Computing and embed systems so what the
[25:27 -> 25:32]  heck does Edge Computing mean l what is
[25:28 -> 25:35]  Edge Computing um think of the Tesla
[25:32 -> 25:38]  Fleet that Tesla has so there's a bunch
[25:35 -> 25:39]  of cars that are maybe running into
[25:38 -> 25:42]  accidents occasionally and so they want
[25:39 -> 25:43]  to report this back to the Tesla data
[25:42 -> 25:46]  center to train on and improve the
[25:43 -> 25:48]  models so you'll have a bunch of these
[25:46 -> 25:49]  this essentially this Fleet and the
[25:48 -> 25:51]  purpose of edge Computing is to have
[25:49 -> 25:52]  them own doing their each of them doing
[25:51 -> 25:55]  their own local
[25:52 -> 25:56]  computation and then whenever you're do
[25:55 -> 25:58]  an update you're just going to send that
[25:56 -> 26:01]  back and you're able to have like the
[25:58 -> 26:02]  centralized entity that I guess the
[26:01 -> 26:04]  centralized data center is our entity
[26:02 -> 26:06]  here and it's just going to do some
[26:04 -> 26:09]  training on all those on all that new
[26:06 -> 26:10]  data and uh that that's pretty much what
[26:09 -> 26:14]  it is it's just like a decentralized
[26:10 -> 26:16]  Computing if you will um so you know you
[26:14 -> 26:18]  have um you have like tensorflow light
[26:16 -> 26:20]  which is like a a light version a
[26:18 -> 26:23]  lightweight version of tensorflow and
[26:20 -> 26:25]  then pytorch mobile is same thing um
[26:23 -> 26:27]  what I mean there's always optimizations
[26:25 -> 26:29]  you can do in Cuda and like just plain
[26:27 -> 26:31]  pie torch that'll just make stuff run
[26:29 -> 26:34]  fast either way but there is py mobile
[26:31 -> 26:37]  for that um then you have corl which is
[26:34 -> 26:40]  for Apple products so like the Mac OS
[26:37 -> 26:43]  watch TV all this
[26:40 -> 26:45]  um then you have ease of use which isn't
[26:43 -> 26:46]  like entirely Cuda related but I thought
[26:45 -> 26:49]  I'd still mention this because some of
[26:46 -> 26:51]  these are really awesome so you have uh
[26:49 -> 26:53]  you have fast AI which I'm not going to
[26:51 -> 26:56]  talk about a lot but you can you can
[26:53 -> 26:59]  look you can look into this maybe
[26:56 -> 27:03]  separately um so they have their own
[26:59 -> 27:03]  they have their own thing here but
[27:04 -> 27:10]  um yeah I'm not going to I'm not going
[27:06 -> 27:12]  to go over fast AI but they uh they have
[27:10 -> 27:15]  some interesting
[27:12 -> 27:17]  stuff Onyx which we talked about before
[27:15 -> 27:19]  stands for open neural network exchange
[27:17 -> 27:21]  so the x is capital and that's where the
[27:19 -> 27:26]  X comes from um literally you just do
[27:21 -> 27:28]  torsa onyx. export model um and then
[27:26 -> 27:30]  dummy input and then just whatever the
[27:28 -> 27:32]  the file name is so you can look more
[27:30 -> 27:34]  into the torch docks and Onyx as to how
[27:32 -> 27:36]  to do this on both P torch and
[27:34 -> 27:38]  tensorflow and whatever else you want
[27:36 -> 27:41]  but this is how you would export an onyx
[27:38 -> 27:43]  format
[27:41 -> 27:45]  um and then this is the tensor FL
[27:43 -> 27:47]  equivalent so this is essentially this
[27:45 -> 27:49]  like nice little image that I got where
[27:47 -> 27:53]  like it kind of binds with everything so
[27:49 -> 27:55]  P toor Tor flow carass um C Cafe which
[27:53 -> 27:58]  was which was initially what P torch was
[27:55 -> 28:01]  using um Cafe was one of Cafe was one of
[27:58 -> 28:05]  those uh original parts in the pytorch
[28:01 -> 28:06]  ecosystem um from a while back um so
[28:05 -> 28:08]  that that just kind of shows how they
[28:06 -> 28:11]  can interconnect together so you like
[28:08 -> 28:13]  export in one of these and then you can
[28:11 -> 28:15]  import back into any one of these uh
[28:13 -> 28:17]  plus Onyx runtime which runs
[28:15 -> 28:20]  faster and then you have weights and
[28:17 -> 28:22]  biases so I got a little snippet from
[28:20 -> 28:24]  the internet as to like what this looks
[28:22 -> 28:25]  like but pretty much allows you to track
[28:24 -> 28:27]  your training runs and a bunch of
[28:25 -> 28:30]  different charts and statistics about
[28:27 -> 28:32]  how your models are are performing so uh
[28:30 -> 28:35]  when I'm doing like when I want to train
[28:32 -> 28:37]  like a clo a clothing uh recognition
[28:35 -> 28:39]  model I can literally have all of these
[28:37 -> 28:42]  different ones so accuracy on sandals
[28:39 -> 28:43]  shirts trousers pullovers boots right
[28:42 -> 28:47]  boots is like kind of chaotic and
[28:43 -> 28:49]  pullovers just kind of worked fast um
[28:47 -> 28:50]  and then this one too so you can kind of
[28:49 -> 28:52]  just track a bunch of things and
[28:50 -> 28:53]  understand what how your models are
[28:52 -> 28:55]  performing and then show that to like
[28:53 -> 28:57]  maybe your maybe your uh employer
[28:55 -> 29:00]  whatever or whoever is maybe your
[28:57 -> 29:02]  manager and just kind of get things done
[29:00 -> 29:04]  that way and document things easily
[29:02 -> 29:06]  without having to use same matap plot
[29:04 -> 29:10]  lib um it's all just kind of tracked and
[29:06 -> 29:11]  imported and taken care of for you um
[29:10 -> 29:13]  and then Cloud providers these are
[29:11 -> 29:15]  actually quite important to know not
[29:13 -> 29:16]  necessarily on the lowlevel part of like
[29:15 -> 29:18]  Cuda but these are still good to know
[29:16 -> 29:22]  because they play a major role in the
[29:18 -> 29:24]  ecosystem um you have AWS so AWS is a
[29:22 -> 29:26]  major one I personally use aws's
[29:24 -> 29:28]  products and prefer them I'm not
[29:26 -> 29:31]  endorsing like not sponsoring them but
[29:28 -> 29:34]  um not sponsored by them but I do use ad
[29:31 -> 29:37]  us products and uh the two main things
[29:34 -> 29:39]  here for ML stuff is ec2 instances so
[29:37 -> 29:41]  these are like used universally you just
[29:39 -> 29:43]  fire up a like a remote machine you can
[29:41 -> 29:45]  SSH into it and then do whatever you
[29:43 -> 29:47]  want and you can use all the specs like
[29:45 -> 29:49]  it's literally uh command line access
[29:47 -> 29:51]  and you could do whatever you want um
[29:49 -> 29:53]  and then you have Sage maker so it's a
[29:51 -> 29:55]  little bit easier and more ml focused
[29:53 -> 29:57]  but you can run jupyter notebooks on a
[29:55 -> 29:58]  cluster so instead of worrying about a
[29:57 -> 30:01]  command line and having having to fire
[29:58 -> 30:02]  things up in like um in vs code like VSS
[30:01 -> 30:05]  code
[30:02 -> 30:08]  SSH you just uh run a jupyter notebook
[30:05 -> 30:10]  literally like in the browser or you can
[30:08 -> 30:14]  uh just SSH into uh The sagemaker
[30:10 -> 30:16]  Notebook I believe um and then you have
[30:14 -> 30:19]  the uh the data labeling part which is
[30:16 -> 30:20]  very big in the world today so where
[30:19 -> 30:22]  does all the data come from that we're
[30:20 -> 30:25]  training models on well this is exactly
[30:22 -> 30:27]  where it is um if you go AWS sagemaker
[30:25 -> 30:29]  and then you find like the the labeling
[30:27 -> 30:31]  part or mechanic Turk I believe is
[30:29 -> 30:34]  believe is what it's called that's where
[30:31 -> 30:37]  all of the labeling on AWS takes place
[30:34 -> 30:39]  so uh you know big stuff there uh
[30:37 -> 30:41]  typically costs like a decent amount of
[30:39 -> 30:43]  money for people to label your stuff but
[30:41 -> 30:45]  that's that's where you find it um and
[30:43 -> 30:47]  then model training and deployment you
[30:45 -> 30:48]  that's that's also supported by Sage
[30:47 -> 30:50]  maker so you want to like deploy your
[30:48 -> 30:53]  own llama 3 variant it's like go there
[30:50 -> 30:56]  you go Sage maker um then Google Cloud I
[30:53 -> 30:58]  don't use as much they have vertex Ai
[30:56 -> 31:01]  and their VM machines which are like 2
[30:58 -> 31:02]  equivalent then you have Microsoft Azure
[31:01 -> 31:05]  which I haven't actually used that much
[31:02 -> 31:06]  so um it's just like another top three
[31:05 -> 31:07]  like these are the top three players in
[31:06 -> 31:10]  the ecosystem and then you kind of break
[31:07 -> 31:12]  down to open AI fast Ai and Lambda Labs
[31:10 -> 31:14]  so open AI provides their own like
[31:12 -> 31:15]  fine-tuning services and you can you
[31:14 -> 31:17]  know everyone knows open AI you can
[31:15 -> 31:18]  literally go on the website and just
[31:17 -> 31:22]  navigate around there and figure out
[31:18 -> 31:24]  what you want to do with models um fast
[31:22 -> 31:28]  AI so I haven't entirely gotten a
[31:24 -> 31:29]  picture here yet but if I go to
[31:28 -> 31:32]  bass at
[31:29 -> 31:34]  AI um I go to the
[31:32 -> 31:37]  console hopefully it doesn't expose
[31:34 -> 31:40]  anything bad um but like yeah I can
[31:37 -> 31:42]  select any of
[31:40 -> 31:44]  these it's just like a bunch of rigs
[31:42 -> 31:46]  that I can rent for an hourly right get
[31:44 -> 31:51]  all the specs on them everything um and
[31:46 -> 31:53]  it's great so you know I set RTX 370s
[31:51 -> 31:55]  which is like my graphics card and mine
[31:53 -> 31:58]  costs about you know 1 cent per hour
[31:55 -> 32:01]  which is which is embarrassingly cheap
[31:58 -> 32:04]  but uh yeah this one oh this one is more
[32:01 -> 32:06]  expensive but yeah so so vastia is
[32:04 -> 32:07]  awesome you can use these like any GPU
[32:06 -> 32:09]  you can pretty much select it and just
[32:07 -> 32:10]  use it on the Fly and it's like hosted
[32:09 -> 32:14]  by someone else in the world that you
[32:10 -> 32:17]  SSH into and do stuff from
[32:14 -> 32:19]  um then you have Lambda Labs which I
[32:17 -> 32:19]  sech set
[32:22 -> 32:28]  up actually find Lambda here Lambda
[32:25 -> 32:32]  Cloud y so
[32:28 -> 32:35]  uh data center dgx systems like
[32:32 -> 32:37]  literally you have the Blackwell gpus
[32:35 -> 32:41]  you have the
[32:37 -> 32:44]  h100s um yeah just pretty much GPU
[32:41 -> 32:46]  infrastructure specifically um and it's
[32:44 -> 32:48]  like I believe a bit cheaper than the
[32:46 -> 32:51]  big three providers like AWS Google and
[32:48 -> 32:53]  Microsoft so uh Lambda Labs is commonly
[32:51 -> 32:55]  used but typically you would rent things
[32:53 -> 32:57]  in a cluster so you're paying like
[32:55 -> 32:59]  multiple hundreds or thousands or tens
[32:57 -> 33:00]  of thousands of dollars per hour for
[32:59 -> 33:02]  these so if you're in a company and
[33:00 -> 33:04]  you're trying to get like cheap cheap
[33:02 -> 33:07]  gpus that are data center quality you
[33:04 -> 33:10]  might want to look at Lambda
[33:07 -> 33:13]  um and then compilers so I'm not like a
[33:10 -> 33:15]  compiler expert but mainly you're going
[33:13 -> 33:18]  to have things like xlaa so this is what
[33:15 -> 33:21]  is powering Jacks um you're going to
[33:18 -> 33:25]  have lvm which I'm not an expert I hav't
[33:21 -> 33:26]  build compiler so um I'll let you look
[33:25 -> 33:28]  into that there's a ton of resources on
[33:26 -> 33:31]  lvm
[33:28 -> 33:35]  um it is it stands for low-level virtual
[33:31 -> 33:35]  machine I believe
[33:37 -> 33:43]  um go to lvm
[33:40 -> 33:45]  project
[33:43 -> 33:47]  um a toolkit for the construction of
[33:45 -> 33:50]  Highly optimized compilers optimizers
[33:47 -> 33:53]  and runtime environments
[33:50 -> 33:56]  um multiple components um component
[33:53 -> 34:00]  compiles C C++ Objective C and objective
[33:56 -> 34:02]  C++ code into lvm bit code um and then
[34:00 -> 34:05]  into object files so it's essentially
[34:02 -> 34:07]  used for developing stuff in cc++ and
[34:05 -> 34:07]  compilers and
[34:07 -> 34:16]  general then you have ml ml which is
[34:12 -> 34:16]  what is ml look at this
[34:17 -> 34:26]  again multi-level intermediate
[34:19 -> 34:28]  representation so this was uh ML and lvm
[34:26 -> 34:31]  were mainly developed by Chris flater
[34:28 -> 34:33]  which um I also have a course um on on
[34:31 -> 34:36]  the on the programming language that his
[34:33 -> 34:37]  company built called modular the
[34:36 -> 34:39]  programming language is called Mojo you
[34:37 -> 34:42]  can search that up on free code camp and
[34:39 -> 34:44]  go learn Mojo too U but it's like a
[34:42 -> 34:46]  pretty much just an AI programming
[34:44 -> 34:48]  language for doing like Fast tensor
[34:46 -> 34:53]  operations
[34:48 -> 34:56]  um so this was moved um it's part of the
[34:53 -> 34:58]  lvm project and uh there's some
[34:56 -> 35:01]  interesting stuff there it's it's
[34:58 -> 35:04]  somewhat newer so there's you
[35:01 -> 35:06]  know interesting interesting changes
[35:04 -> 35:10]  it's not it's not like super ancient
[35:06 -> 35:12]  um but uh the main ones that I'll be
[35:10 -> 35:14]  able to talk about are like nvcc so
[35:12 -> 35:17]  that's like the Auda compiler Nvidia
[35:14 -> 35:20]  Cuda compiler
[35:17 -> 35:21]  um and you know there's an architecture
[35:20 -> 35:23]  here which I haven't like fully
[35:21 -> 35:25]  memorized yet but uh the Nvidia Cuda
[35:23 -> 35:27]  compiler is what we're what we're going
[35:25 -> 35:29]  to be using to essentially compile our
[35:27 -> 35:31]  Cuda scripts and kernels and have them
[35:29 -> 35:34]  you know into binary so that we can run
[35:31 -> 35:36]  them fast so uh you know these are
[35:34 -> 35:38]  interesting interesting compiler
[35:36 -> 35:39]  infrastructure I'll probably add to this
[35:38 -> 35:42]  with some better descriptions on like
[35:39 -> 35:44]  what these are but uh this is like the
[35:42 -> 35:45]  general overview and then for
[35:44 -> 35:48]  miscellaneous I had I could not leave
[35:45 -> 35:51]  out hugging face so last but not least
[35:48 -> 35:53]  it's like hugging face right um You
[35:51 -> 35:56]  probably already know what it is
[35:53 -> 35:58]  but I'll look at it up just in case so
[35:56 -> 36:02]  on hugging face you have a bunch of
[35:58 -> 36:04]  things um you have models data sets uh
[36:02 -> 36:06]  and then spaces and that's like pretty
[36:04 -> 36:08]  much all you need to know so if you go
[36:06 -> 36:12]  to models you
[36:08 -> 36:15]  can oh maybe it'll take a second to load
[36:12 -> 36:18]  um
[36:15 -> 36:21]  here we have multimodal computer vision
[36:18 -> 36:23]  MLP audio tabular reinforcement learning
[36:21 -> 36:24]  and then graph machine learning so
[36:23 -> 36:26]  there's a bunch of cool stuff you can do
[36:24 -> 36:28]  here but most of it is language models
[36:26 -> 36:30]  right now um I know like recently
[36:28 -> 36:33]  released some of
[36:30 -> 36:35]  their I believe it's like image Maybe
[36:33 -> 36:36]  video Generation stuff I can't remember
[36:35 -> 36:38]  specifically but this is where you'll
[36:36 -> 36:39]  see all like the new open source models
[36:38 -> 36:41]  uh that you can just pretty much
[36:39 -> 36:43]  download and run run in P torch like
[36:41 -> 36:44]  that uh you just need enough Hardware
[36:43 -> 36:47]  you just need good enough Hardware to
[36:44 -> 36:48]  run these and it'll just it'll just work
[36:47 -> 36:50]  um and then you have the actual data
[36:48 -> 36:53]  sets for these models that that you
[36:50 -> 36:57]  train them on so um you know you can go
[36:53 -> 37:01]  like 3D data sets which is interesting
[36:57 -> 37:01]  um a lot of it is just going to be text
[37:01 -> 37:05]  so um if I remove
[37:08 -> 37:13]  that yeah Vision data awesome Auto math
[37:12 -> 37:14]  text so just all these all these data
[37:13 -> 37:17]  sets are here that the models are
[37:14 -> 37:19]  trained on um and then you have spaces
[37:17 -> 37:21]  which is where you can actually use
[37:19 -> 37:23]  models um this is where people will like
[37:21 -> 37:25]  host things or get sponsors with custom
[37:23 -> 37:27]  Hardware setups uh and they'll be able
[37:25 -> 37:30]  to just essentially host these models
[37:27 -> 37:32]  and you can try them out and use them so
[37:30 -> 37:34]  hogging face is awesome it's a major
[37:32 -> 37:36]  player in the whole ecosystem and I
[37:34 -> 37:39]  could not leave it out
[37:36 -> 37:41]  but uh yeah that's that's pretty much it
[37:39 -> 37:43]  for the Deep learning ecosystem I'll see
[37:41 -> 37:46]  you in the next
[37:43 -> 37:48]  part so doing the setup on Windows we
[37:46 -> 37:50]  just need to open up our terminal and
[37:48 -> 37:53]  run as administrator I'm starting with
[37:50 -> 37:56]  Windows uh we just enable permissions
[37:53 -> 37:58]  ensure that it's the system 32 directory
[37:56 -> 38:02]  going to navigate get over to the turn
[37:58 -> 38:05]  Windows features on and off um we're
[38:02 -> 38:08]  going to scroll up and look for
[38:05 -> 38:10]  hyperv um ensure that box is checked off
[38:08 -> 38:13]  and then we're going to look for virtual
[38:10 -> 38:16]  machine platform ensure that is checked
[38:13 -> 38:18]  off and then or checked on rather and
[38:16 -> 38:21]  then you have a Windows subsystem for
[38:18 -> 38:24]  Linux make sure that's also checked on
[38:21 -> 38:26]  um in order to get this working you will
[38:24 -> 38:28]  need uh to enable virtualization on your
[38:26 -> 38:30]  machine
[38:28 -> 38:33]  so uh you know once the windows
[38:30 -> 38:35]  subsystem is on you can do wl. exe and
[38:33 -> 38:38]  you'll see you know a bunch of options
[38:35 -> 38:41]  so install distribution and we see an
[38:38 -> 38:46]  example there WSL install distribution
[38:41 -> 38:48]  Yu we can go ahead and enter enable and
[38:46 -> 38:50]  we'll just wait for that to complete
[38:48 -> 38:53]  I've sped this up a little bit because
[38:50 -> 38:54]  it takes some time uh realistically it
[38:53 -> 38:56]  takes more than you know a few seconds
[38:54 -> 38:58]  to do this so I'll speed some of these
[38:56 -> 39:00]  things up
[38:58 -> 39:03]  um you has been installed
[39:00 -> 39:07]  awesome changes will not be effective
[39:03 -> 39:10]  until system is rebooted um yeah so we
[39:07 -> 39:14]  run it again um we have this command
[39:10 -> 39:16]  that it's asking us to run so WSL exe
[39:14 -> 39:18]  install no distribution that installs
[39:16 -> 39:20]  correctly um and we get the same thing
[39:18 -> 39:23]  again so we just do a system
[39:20 -> 39:25]  restart now after we've restarted you
[39:23 -> 39:28]  might be greeted with this
[39:25 -> 39:31]  terminal uh your BTU and then uh the
[39:28 -> 39:32]  other command prompt so when you're
[39:31 -> 39:34]  greeted with that if you're just greeted
[39:32 -> 39:37]  with the command prompt you do
[39:34 -> 39:38]  WSL uh and you can get into here and
[39:37 -> 39:40]  just enter a username and a password
[39:38 -> 39:43]  that you're going to use now you should
[39:40 -> 39:46]  be logged into your uh little simulated
[39:43 -> 39:48]  uh Linux environment so once we're in
[39:46 -> 39:50]  here there's a few commands we need to
[39:48 -> 39:53]  run so we're going to update and we're
[39:50 -> 39:55]  going to upgrade everything so just type
[39:53 -> 39:56]  in the commands as you see them there's
[39:55 -> 39:58]  some that we're going to be able to copy
[39:56 -> 40:00]  and paste in so just end that password
[39:58 -> 40:02]  you set earlier I've time-elapsed this
[40:00 -> 40:04]  not time-lapsed but I've sped this one
[40:02 -> 40:07]  up again so that was just a bunch of
[40:04 -> 40:08]  things uh updating um if we go and
[40:07 -> 40:11]  install some other packages that we'll
[40:08 -> 40:13]  need later like WG curl and git we'll
[40:11 -> 40:15]  see that those are also installed as a
[40:13 -> 40:18]  part of the update and upgrade
[40:15 -> 40:20]  commands um and then we just install
[40:18 -> 40:24]  Python 3 pip uh this is just going to be
[40:20 -> 40:28]  python essentially for our machine and
[40:24 -> 40:30]  uh that also runs too so that does not
[40:28 -> 40:33]  come by default apparently um so we just
[40:30 -> 40:36]  need to install that manually but that's
[40:33 -> 40:37]  okay we navigate over to Chrome and we
[40:36 -> 40:40]  search Up Cuda toolkit this is what
[40:37 -> 40:41]  we're looking for Cuda toolkit download
[40:40 -> 40:43]  so you just navigate to the latest one
[40:41 -> 40:46]  it might be 12.5 it might be 12.6
[40:43 -> 40:49]  whatever it is for you go to Linux pick
[40:46 -> 40:52]  your architecture and use WSL you to
[40:49 -> 40:53]  remember we're using uh WSL and then
[40:52 -> 40:55]  just do the run file it's the easiest
[40:53 -> 40:58]  one least amount of instructions you
[40:55 -> 41:01]  have to do um so so going to do the
[40:58 -> 41:03]  first one so w get uh you can just right
[41:01 -> 41:07]  click in the terminal if normal pasting
[41:03 -> 41:07]  doesn't work
[41:07 -> 41:13]  um let's maybe highlight the whole
[41:11 -> 41:18]  thing awesome so that's going to take
[41:13 -> 41:21]  some time to upgrade and uh I'll see you
[41:18 -> 41:24]  guys on the other side okay so now we're
[41:21 -> 41:25]  in the little accept part so just only
[41:24 -> 41:28]  check off the to Cuda toolkit there and
[41:25 -> 41:31]  then you should be good to install
[41:28 -> 41:34]  now we've done the runsh file which was
[41:31 -> 41:35]  the second part of the command and it
[41:34 -> 41:37]  tells us in the summary that we need to
[41:35 -> 41:40]  add some things to our path so I've just
[41:37 -> 41:42]  pulled up this here um but this wasn't
[41:40 -> 41:45]  really working too much so I went off
[41:42 -> 41:48]  and generated some other uh you know
[41:45 -> 41:51]  more upto-date commands with chat GPT uh
[41:48 -> 41:53]  and figured out uh the act the the
[41:51 -> 41:55]  proper ones
[41:53 -> 41:57]  so you'll see those in a second here
[41:55 -> 41:59]  once I pulled them up but this is just
[41:57 -> 42:02]  the this is just a reference so this is
[41:59 -> 42:05]  one of the things we have to do so we
[42:02 -> 42:07]  could just Vim into our bash RC file um
[42:05 -> 42:09]  and then I'll just just pretty much type
[42:07 -> 42:11]  along with me here and then we'll we'll
[42:09 -> 42:13]  save this file so feel free to use Nano
[42:11 -> 42:16]  or Vim whatever whatever you feel
[42:13 -> 42:19]  comfortable with I'm using Vim here but
[42:16 -> 42:22]  uh you know Nano isn't too hard either
[42:19 -> 42:24]  so going to set a Cuda home um user
[42:22 -> 42:29]  local Cuda and then we're going to
[42:24 -> 42:32]  export another one uh called path and as
[42:29 -> 42:35]  a part of that path um we're just
[42:32 -> 42:37]  essentially going to include Cuda home
[42:35 -> 42:40]  and then the binary for
[42:37 -> 42:43]  that and then last but not least um
[42:40 -> 42:47]  we're just going to export the LD
[42:43 -> 42:47]  Library like it also said in the
[42:51 -> 42:59]  summary and then lip 64 to end it
[42:55 -> 43:02]  off awesome so now we can just contrl C
[42:59 -> 43:08]  contrl W and contrl
[43:02 -> 43:11]  Q or col w q we exit that um and then we
[43:08 -> 43:14]  can oh I noticed we missed something so
[43:11 -> 43:17]  Cuda 12.5 instead of just Cuda um so we
[43:14 -> 43:21]  can go ahead and go back into this and
[43:17 -> 43:21]  then just find that part and add
[43:21 -> 43:25]  um and just right click and paste that
[43:24 -> 43:28]  back in and then just delete the last
[43:25 -> 43:32]  Cuda part awesome
[43:28 -> 43:35]  C 12.5 sweet now we can just exit that
[43:32 -> 43:39]  again and
[43:35 -> 43:41]  Source then we just do nvcc uh-- version
[43:39 -> 43:44]  which is the Nvidia Cuda compiler so
[43:41 -> 43:45]  that's working and then Nvidia s SMI so
[43:44 -> 43:48]  we can actually track our GPU stats as
[43:45 -> 43:49]  long as these are both working um we've
[43:48 -> 43:52]  done the job correctly so if you're not
[43:49 -> 43:53]  get if you're getting errors with nvcc
[43:52 -> 43:55]  or
[43:53 -> 43:58]  nvmi uh that's that's not good you need
[43:55 -> 44:01]  to figure that out uh I don't of course
[43:58 -> 44:04]  cover all the errors but
[44:01 -> 44:06]  um that aside we're going to go ahead
[44:04 -> 44:07]  and set up a little Cuda test just to
[44:06 -> 44:09]  make sure that everything's working
[44:07 -> 44:12]  properly and that we can execute a docu
[44:09 -> 44:15]  or Cuda script so I just made a
[44:12 -> 44:18]  directory called cuda setup test and
[44:15 -> 44:19]  we're going to just Vim into um that
[44:18 -> 44:21]  that directory and we're going to edit
[44:19 -> 44:23]  and we're going to make a new main. cuu
[44:21 -> 44:25]  file and inside of here I'm just going
[44:23 -> 44:28]  to go ahead and paste uh some functions
[44:25 -> 44:30]  so uh we include the Cuda runtime header
[44:28 -> 44:32]  we include the io stream which is a part
[44:30 -> 44:36]  of C++ that allows us to use things like
[44:32 -> 44:39]  C out we declare the namespace STD for
[44:36 -> 44:42]  for standard and then we see out hello
[44:39 -> 44:46]  world in our in main function so if we
[44:42 -> 44:49]  do nvcc um out main binary and then
[44:46 -> 44:51]  main. cuu we should uh be able to run
[44:49 -> 44:53]  this binary and get Hello World
[44:51 -> 44:56]  awesome so if this works first try for
[44:53 -> 44:57]  you that's awesome if it didn't that's
[44:56 -> 44:59]  not so awesome but but you should be
[44:57 -> 45:02]  able to figure it out just by navigating
[44:59 -> 45:04]  uh forums so like GitHub the stuff I
[45:02 -> 45:05]  recommended before just navigate around
[45:04 -> 45:08]  and figure out how to install the Cuda
[45:05 -> 45:10]  toolkit for Windows um it pretty much
[45:08 -> 45:13]  applies the same to yuntu I'm going to
[45:10 -> 45:14]  go over some brief instructions here but
[45:13 -> 45:16]  I'm going to switch over to Ubuntu
[45:14 -> 45:18]  because that's what my whole thing is
[45:16 -> 45:19]  based on that's where I do all of my
[45:18 -> 45:22]  stuff and where everything is set up and
[45:19 -> 45:23]  optimized for so I'll see you on the
[45:22 -> 45:26]  other side if we can go ahead and open a
[45:23 -> 45:27]  Chrome tab here and just type in Cuda
[45:26 -> 45:31]  toolkit downlo
[45:27 -> 45:36]  mod so we go to this one on your BTU
[45:31 -> 45:39]  same thing we go Linux x64 is mine might
[45:36 -> 45:40]  be different for you um
[45:39 -> 45:44]  YouTu this
[45:40 -> 45:48]  one run file local um you can do this
[45:44 -> 45:50]  you can also do Network or local so for
[45:48 -> 45:53]  me I did network but that was a little
[45:50 -> 45:54]  while back and having to uninstall it
[45:53 -> 45:57]  and then reinstall it again just gives
[45:54 -> 45:59]  me a bunch of weird Graphics uh errors
[45:57 -> 46:03]  so I'm not going to do that and mess
[45:59 -> 46:04]  with my operating system too much but
[46:03 -> 46:06]  you should be able to just plug this
[46:04 -> 46:10]  directly into terminal so you should be
[46:06 -> 46:17]  able to just pop into here and uh plug
[46:10 -> 46:18]  these in W get the uh the Debian file um
[46:17 -> 46:20]  and then just the rest of this and
[46:18 -> 46:23]  install the Cuda toolkit and then just
[46:20 -> 46:25]  get the the Legacy Cuda drivers I just
[46:23 -> 46:29]  did this this Legacy Cuda drivers if
[46:25 -> 46:31]  that doesn't work do this one um and
[46:29 -> 46:37]  then you would of course want to just do
[46:31 -> 46:39]  do um nbcc version and then Nvidia SMI
[46:37 -> 46:43]  and you should see uh some useful stuff
[46:39 -> 46:46]  pop up here so uh if that if these don't
[46:43 -> 46:47]  work for you right away um you know you
[46:46 -> 46:49]  might want to just restart your computer
[46:47 -> 46:51]  that's usually the best option and then
[46:49 -> 46:52]  try something again um if you do already
[46:51 -> 46:54]  have these installed you don't even have
[46:52 -> 46:57]  to worry about it so I would probably
[46:54 -> 47:00]  check these first probably should said
[46:57 -> 47:04]  that first but uh yeah Ure uh the Auda
[47:00 -> 47:04]  compiler works and then Nvidia
[47:06 -> 47:13]  SMI so now we can finally get into some
[47:09 -> 47:15]  coding um in order to really understand
[47:13 -> 47:18]  how to use Cuda you need to First cover
[47:15 -> 47:20]  C and C++ so this course isn't actually
[47:18 -> 47:22]  about C and C++ so I'm just going to
[47:20 -> 47:24]  provide some resources for you guys to
[47:22 -> 47:26]  learn this stuff and then I'll jump into
[47:24 -> 47:28]  more some more advanced topics uh just
[47:26 -> 47:30]  to watch over and and review the
[47:28 -> 47:31]  subjects so for those of you who are new
[47:30 -> 47:36]  to this stuff for those of you who are
[47:31 -> 47:37]  new to lowlevel C C++ Cuda programming
[47:36 -> 47:40]  um I have some resources for you some
[47:37 -> 47:42]  good articles some good uh things to
[47:40 -> 47:45]  manage through and if you are already
[47:42 -> 47:48]  experience uh just pretty much skip this
[47:45 -> 47:50]  part or even even still look at it to
[47:48 -> 47:51]  maybe touch up on the basics and I'll
[47:50 -> 47:57]  cover some more advanced topics right
[47:51 -> 47:59]  after this so learning C and C++ is hard
[47:57 -> 48:00]  and so you really have to Define what
[47:59 -> 48:02]  the best resources are and how to
[48:00 -> 48:03]  actually learn things properly what is
[48:02 -> 48:07]  the best use of your time right this is
[48:03 -> 48:08]  a common dilemma that we have so I came
[48:07 -> 48:13]  across a Reddit article on best
[48:08 -> 48:15]  resources to learn C++ and it it pretty
[48:13 -> 48:17]  much said learn C+ plus.com plus a bunch
[48:15 -> 48:20]  of other links that you might like so
[48:17 -> 48:22]  learn C+ plus.com is good I've never
[48:20 -> 48:24]  used this before so I don't know how
[48:22 -> 48:26]  comparatively good it is um so that
[48:24 -> 48:28]  that's an option of course and then
[48:26 -> 48:30]  there's best way to learn C so looking
[48:28 -> 48:35]  through this I pretty much found that
[48:30 -> 48:37]  Everyone likes the um the modern C this
[48:35 -> 48:41]  one C programming a modern approach so
[48:37 -> 48:43]  it's a it's a newer book um but that's
[48:41 -> 48:44]  how people found best way to learn to C
[48:43 -> 48:46]  um if you are just trying to learn this
[48:44 -> 48:48]  for free and try to you know just go
[48:46 -> 48:52]  through the syntax and understand it as
[48:48 -> 48:54]  quickly as possible um there are some
[48:52 -> 48:56]  resources I would recommend and have
[48:54 -> 48:58]  looked through a little bit so freed
[48:56 -> 49:01]  code camp has some good stuff on this um
[48:58 -> 49:03]  C programming just a bunch of uh blogs
[49:01 -> 49:05]  essentially on how to just pretty much
[49:03 -> 49:06]  just learning the language and then you
[49:05 -> 49:08]  have
[49:06 -> 49:12]  C++ uh just you know maybe some more
[49:08 -> 49:16]  advanced things um you know
[49:12 -> 49:18]  libraries a bunch of modern C++ stuff um
[49:16 -> 49:20]  so free code Camp is a great resource
[49:18 -> 49:22]  and then the one that I've personally
[49:20 -> 49:25]  stuck with for a long time and continue
[49:22 -> 49:27]  to use is W3 schools so it's pretty much
[49:25 -> 49:31]  like a nice easy to read easy on the
[49:27 -> 49:34]  eyes documentation on or just an intro
[49:31 -> 49:36]  rather on how to use C and C++ so I have
[49:34 -> 49:40]  both here um I'd recommend if you're new
[49:36 -> 49:42]  to this just look over each of these and
[49:40 -> 49:44]  do do a bunch of practice questions on
[49:42 -> 49:46]  every single one of these um all these
[49:44 -> 49:47]  are super important there there's some
[49:46 -> 49:49]  of them you might not actually use
[49:47 -> 49:51]  explicitly in the course but it's still
[49:49 -> 49:54]  good to know it regardless just for you
[49:51 -> 49:56]  know having that uh you know lowlevel uh
[49:54 -> 49:58]  brain so that you can dissect problems
[49:56 -> 50:00]  with uh on Cuda applications that we may
[49:58 -> 50:02]  not cover in this
[50:00 -> 50:04]  course so I'd recommend just like
[50:02 -> 50:08]  looking through all of these go down all
[50:04 -> 50:10]  the way to like these examples um like
[50:08 -> 50:12]  everything and then same with C++ as
[50:10 -> 50:15]  well so all all your Basics your
[50:12 -> 50:19]  functions your classes um and then down
[50:15 -> 50:21]  to examples as well so uh that's that's
[50:19 -> 50:24]  pretty much all I have for the basics of
[50:21 -> 50:27]  CN C++ now we're going to go ahead and
[50:24 -> 50:28]  touch on pointers
[50:27 -> 50:32]  we're going to start off with pointers
[50:28 -> 50:32]  if you go to github.com
[50:32 -> 50:37]  inosi Cuda course um and then pop over
[50:35 -> 50:38]  to well you're not going to pop over to
[50:37 -> 50:40]  Dev this is going to be all pushed up
[50:38 -> 50:42]  and ready once you once you're seeing
[50:40 -> 50:44]  this but uh essentially you're just
[50:42 -> 50:46]  going to get C this into a directory of
[50:44 -> 50:49]  your choice uh and then we can go and
[50:46 -> 50:51]  get started with the c and C++ review so
[50:49 -> 50:52]  I have this all in my vs code here and
[50:51 -> 50:55]  it's all zoomed in and nice for you to
[50:52 -> 50:57]  see but um we we'll start off with a
[50:55 -> 51:01]  symbol pointer example
[50:57 -> 51:04]  so uh we initialize an integer integer X
[51:01 -> 51:06]  to 10 this a data this is the data part
[51:04 -> 51:09]  of X um and then we initialize a pointer
[51:06 -> 51:12]  type so this asteris uh that means we're
[51:09 -> 51:13]  doing a pointer to an integer uh and
[51:12 -> 51:17]  we're setting that we're setting the
[51:13 -> 51:19]  name equal to pointer and then this uh
[51:17 -> 51:22]  Ampersand is saying we're going to get
[51:19 -> 51:25]  the memory address of X so we have X
[51:22 -> 51:26]  here which is 10 and the ENT says uh
[51:25 -> 51:29]  we're going to get the memory address of
[51:26 -> 51:31]  of X which is um which is going to be
[51:29 -> 51:34]  the pointer to 10
[51:31 -> 51:38]  um and then we can we can just print
[51:34 -> 51:43]  this out so if I go GCC and we do
[51:38 -> 51:45]  Z 01 and then run that you'll see that
[51:43 -> 51:47]  we get an address so I have the pointer
[51:45 -> 51:49]  I have the pointer type here there's
[51:47 -> 51:53]  there's an index where you can find
[51:49 -> 51:57]  these um if I pull up a tab here and go
[51:53 -> 51:59]  uh Point uh print f
[51:57 -> 52:02]  uh like in for
[51:59 -> 52:04]  example so you have all these different
[52:02 -> 52:06]  things here on C+ plus.com that you can
[52:04 -> 52:09]  use um and these are just like the
[52:06 -> 52:12]  formats and stuff
[52:09 -> 52:14]  so we have uh we have a
[52:12 -> 52:17]  pointer it's this value that we're
[52:14 -> 52:20]  returning uh we get a memory address to
[52:17 -> 52:22]  the value 10 and to get 10 we're passing
[52:20 -> 52:25]  in pointer and then we do the asteris to
[52:22 -> 52:27]  dreference it so dfference means uh we
[52:25 -> 52:29]  we have this essentially we have this
[52:27 -> 52:31]  data thing which is 10 we have the
[52:29 -> 52:33]  memory address to 10 which is the level
[52:31 -> 52:35]  above and E reference is just going to
[52:33 -> 52:36]  go downwards just going to go back to
[52:35 -> 52:39]  back to that so we have this memory
[52:36 -> 52:43]  address just dreference and go back to
[52:39 -> 52:46]  10 um the next example here is a little
[52:43 -> 52:48]  bit tricky um but it's fine it's it's
[52:46 -> 52:50]  relatively intuitive so I don't expect
[52:48 -> 52:52]  it to be that hard but essentially what
[52:50 -> 52:56]  we're doing is we initialize a value to
[52:52 -> 52:59]  42 and then we make a uh a pointer an
[52:56 -> 53:01]  integer pointer type called pointer one
[52:59 -> 53:04]  and we set that equal to the memory
[53:01 -> 53:06]  address of value so it's ersan memory
[53:04 -> 53:08]  address of value um so it's going to be
[53:06 -> 53:11]  like 42 and then we create pointer 1
[53:08 -> 53:14]  which is a memory address or a pointer 2
[53:11 -> 53:16]  42 and then we do the same thing so we
[53:14 -> 53:18]  make a pointer to a pointer which is
[53:16 -> 53:20]  what the double asteris is for uh and
[53:18 -> 53:23]  then we do again Amber sand of pointer
[53:20 -> 53:26]  one so memory address of this pointer so
[53:23 -> 53:29]  then you have 10 you have 42 and then
[53:26 -> 53:32]  you have memory address which is the the
[53:29 -> 53:33]  you have the pointer to uh 42 then you
[53:32 -> 53:37]  have another one above that which is
[53:33 -> 53:39]  pointer to a pointer to to a value and
[53:37 -> 53:41]  then we just do that another time so
[53:39 -> 53:47]  it's pointer to a pointer to a pointer
[53:41 -> 53:50]  to a value um and uh this logic checks
[53:47 -> 53:53]  out and when we print this out we're
[53:50 -> 53:55]  going to return the integer type so D um
[53:53 -> 53:58]  and then we're just going to Triple D
[53:55 -> 54:00]  reference it so we have these multiple
[53:58 -> 54:02]  different layers so we're going up a
[54:00 -> 54:04]  pointer like level one pointer level two
[54:02 -> 54:06]  pointer level three and D referencing is
[54:04 -> 54:07]  just like going down a level so we go
[54:06 -> 54:09]  down one two three levels and back to
[54:07 -> 54:12]  back to that value of 42 which is an
[54:09 -> 54:16]  integer and we can safely return that so
[54:12 -> 54:20]  if I just go GCC and then compile
[54:16 -> 54:23]  02 we go and run that and we get a safe
[54:20 -> 54:26]  output value 42 awesome so now I pop
[54:23 -> 54:28]  over to number three which is where
[54:26 -> 54:29]  things start to get a little bit weird
[54:28 -> 54:32]  uh and initially this was kind of a
[54:29 -> 54:34]  funky topic for me as well but this is
[54:32 -> 54:35]  this is void pointers so void pointers
[54:34 -> 54:38]  are a little funny and they actually
[54:35 -> 54:40]  allow us to do a lot of tricks that uh
[54:38 -> 54:42]  allow for things like polymorphism and
[54:40 -> 54:46]  stuff um but we're not going to go over
[54:42 -> 54:49]  that a ton that's that's like other uh
[54:46 -> 54:51]  that's not covered in this review uh so
[54:49 -> 54:54]  we initialize an integer called num to
[54:51 -> 54:55]  10 we initialize a float called f num
[54:54 -> 54:58]  equal to
[54:55 -> 55:01]  3.14 um and then we have this this void
[54:58 -> 55:03]  pointer so what this means is like if
[55:01 -> 55:04]  you had an integer and then an asteris
[55:03 -> 55:08]  that would mean it's a pointer to an
[55:04 -> 55:11]  integer but void is no type so it's like
[55:08 -> 55:13]  a pointer to no type and that means we
[55:11 -> 55:16]  can actually change which type it is
[55:13 -> 55:19]  pointed to which is a cool little
[55:16 -> 55:22]  feature that you can do in C um so we
[55:19 -> 55:25]  say um void pointer is going to equal
[55:22 -> 55:29]  the memory address of num which is this
[55:25 -> 55:32]  right um and then what we can do here in
[55:29 -> 55:36]  this in this print part we essentially
[55:32 -> 55:37]  uh we take this we cast it to an INT
[55:36 -> 55:40]  pointer type that's what this part is
[55:37 -> 55:41]  for these brackets and then inside the
[55:40 -> 55:45]  int and then after the Asis the the
[55:41 -> 55:50]  pointer cast and then we dreference that
[55:45 -> 55:51]  so we have a void pointer which we cast
[55:50 -> 55:53]  to an
[55:51 -> 55:55]  integer we cast to an integer type it's
[55:53 -> 55:57]  originally holding the memory address of
[55:55 -> 55:59]  int
[55:57 -> 56:01]  um and then we dreference that after
[55:59 -> 56:03]  it's casted so it's going to go up to
[56:01 -> 56:05]  this memory address then it's going to
[56:03 -> 56:08]  go back to 10 which is the value of
[56:05 -> 56:13]  num and then we essentially just do the
[56:08 -> 56:14]  same thing for this F num here um I I
[56:13 -> 56:17]  have nice little descriptions here that
[56:14 -> 56:20]  you can read on your own uh and and then
[56:17 -> 56:22]  a fun little fact so Malik actually
[56:20 -> 56:25]  returns a void pointer but we see it
[56:22 -> 56:27]  point to a specific data type after the
[56:25 -> 56:29]  cast so
[56:27 -> 56:32]  what you typically see Malik as like
[56:29 -> 56:33]  these these opening brackets uh these
[56:32 -> 56:35]  this brackets and then you have the
[56:33 -> 56:38]  actual cast inside of it so what we did
[56:35 -> 56:39]  over here is what you see in Malik so
[56:38 -> 56:41]  it's actually returning a void pointer
[56:39 -> 56:43]  and then you cast that to a specific
[56:41 -> 56:46]  like integer or a floating Point uh
[56:43 -> 56:50]  pointer um and and then you can use that
[56:46 -> 56:55]  for something like like an array um so
[56:50 -> 56:55]  if we go ahead and just GCC compile this
[56:58 -> 57:04]  and then run we get our integer which is
[57:02 -> 57:08]  uh integer type of course and then we
[57:04 -> 57:11]  get our our float 3.14 which is a uh
[57:08 -> 57:13]  which is a float type so void pointers
[57:11 -> 57:14]  are not not void pointers sorry null
[57:13 -> 57:16]  pointers are really interesting and you
[57:14 -> 57:19]  probably found void pointers interesting
[57:16 -> 57:22]  as well um but these are a little bit
[57:19 -> 57:25]  different so null pointers can actually
[57:22 -> 57:28]  make our code more robust through if
[57:25 -> 57:31]  statements um going remove those binary
[57:28 -> 57:34]  files for now to clean some space up um
[57:31 -> 57:39]  but we we initialize a pointer to null
[57:34 -> 57:40]  um if we try to print this out um it's
[57:39 -> 57:43]  it's going to essentially return like
[57:40 -> 57:46]  there's nothing right there's like no
[57:43 -> 57:48]  space actually here there's nothing that
[57:46 -> 57:49]  you can't you can't use that pointer for
[57:48 -> 57:52]  anything because it's null that's the
[57:49 -> 57:54]  whole idea here so what we can do is we
[57:52 -> 57:57]  can check if the pointer is equal to
[57:54 -> 57:59]  n um and then we just essentially just
[57:57 -> 58:02]  report that maybe we throw an error
[57:59 -> 58:04]  message or we we we put a warning up uh
[58:02 -> 58:06]  cannot dreference this right if the
[58:04 -> 58:09]  pointer doesn't have anything you can't
[58:06 -> 58:13]  dreference nothing to something you
[58:09 -> 58:16]  can't do that so uh we actually change
[58:13 -> 58:19]  that up and we we allocate memory uh to
[58:16 -> 58:23]  pointer so to this pointer so that we
[58:19 -> 58:24]  can use it safely later on so uh I'm
[58:23 -> 58:27]  just going to actually compile this so
[58:24 -> 58:31]  you can so you can see what it's
[58:27 -> 58:33]  doing um initial pointer value is nil so
[58:31 -> 58:36]  we have the pointer type we cast to a
[58:33 -> 58:39]  void pointer which is going to be n null
[58:36 -> 58:41]  of course so n that checks out pointer
[58:39 -> 58:43]  is null cannot D reference good so this
[58:41 -> 58:46]  this was
[58:43 -> 58:49]  true after allocation so this is where
[58:46 -> 58:56]  we get into this part Malik is going to
[58:49 -> 58:57]  return uh a void pointer the size of int
[58:56 -> 58:59]  so that means there is actually
[58:57 -> 59:00]  something there now there there is
[58:59 -> 59:03]  something there it doesn't have an
[59:00 -> 59:06]  explicit data type but we have something
[59:03 -> 59:10]  there that's like in I think 32 bits so
[59:06 -> 59:13]  four bytes um and then we check if
[59:10 -> 59:15]  pointer is equal to n again um and it
[59:13 -> 59:18]  doesn't print memory allocation failed
[59:15 -> 59:20]  so that's that's good and then we get to
[59:18 -> 59:24]  number four after allocation pointer
[59:20 -> 59:27]  value is this so we cast this to a uh
[59:24 -> 59:29]  void pointer uh and we can actually see
[59:27 -> 59:31]  this this this memory address we can
[59:29 -> 59:34]  actually see that it works um and then
[59:31 -> 59:36]  we can you know we we know that this
[59:34 -> 59:39]  exists now so we can use it for
[59:36 -> 59:41]  something so um we we essentially check
[59:39 -> 59:45]  it for null safe to use now uh and then
[59:41 -> 59:48]  we have this uh we have this dreference
[59:45 -> 59:50]  pointer so you dreference that memory
[59:48 -> 59:53]  address back to the data part and we set
[59:50 -> 59:56]  that equal to 42 so now you have this uh
[59:53 -> 59:57]  safe to use um you have this you safe to
[59:56 -> 01:00:00]  use memory address and the data
[59:57 -> 01:00:03]  associated with that memory address and
[01:00:00 -> 01:00:05]  then we can free that pointer um set to
[01:00:03 -> 01:00:10]  n after freeing uh and then we see that
[01:00:05 -> 01:00:13]  it's it's it's null again so uh yeah if
[01:00:10 -> 01:00:17]  pointer is null so we we know that it is
[01:00:13 -> 01:00:20]  null safely avoided use after free
[01:00:17 -> 01:00:23]  awesome so the the whole point here is
[01:00:20 -> 01:00:25]  that we can use no pointers to uh do
[01:00:23 -> 01:00:26]  little tricks and we can make our code
[01:00:25 -> 01:00:29]  Mar
[01:00:26 -> 01:00:31]  by checking if it is null we can avoid
[01:00:29 -> 01:00:33]  running into unexpected errors like seg
[01:00:31 -> 01:00:35]  faults and other weird things that are
[01:00:33 -> 01:00:36]  hard to trace back right sometimes you
[01:00:35 -> 01:00:37]  don't want to have to go through all of
[01:00:36 -> 01:00:40]  that just to figure out an error so it's
[01:00:37 -> 01:00:41]  better to just uh write more robust code
[01:00:40 -> 01:00:44]  in the first place and ensure that it
[01:00:41 -> 01:00:46]  works properly so in this example
[01:00:44 -> 01:00:48]  there's quite a few things going on here
[01:00:46 -> 01:00:51]  but I'll try to I'll try to explain this
[01:00:48 -> 01:00:53]  as best as possible so we have an array
[01:00:51 -> 01:00:56]  uh we just declare an integer array five
[01:00:53 -> 01:00:59]  numbers and then we have an uh pointer
[01:00:56 -> 01:01:01]  an integer pointer equal to array so in
[01:00:59 -> 01:01:03]  C because we're just leaving it as AR
[01:01:01 -> 01:01:05]  Ray alone it's going to point to the
[01:01:03 -> 01:01:07]  first element uh because that's that's
[01:01:05 -> 01:01:09]  how memory is lined up right it's going
[01:01:07 -> 01:01:11]  to point to essentially where does this
[01:01:09 -> 01:01:13]  thing start uh and that's going to be
[01:01:11 -> 01:01:16]  that's going to be 12 of course but we
[01:01:13 -> 01:01:19]  know that an array is a pointer on its
[01:01:16 -> 01:01:21]  own if we don't dreference or if we
[01:01:19 -> 01:01:23]  don't index that array um it's just
[01:01:21 -> 01:01:26]  going to be a pointer alone right so if
[01:01:23 -> 01:01:30]  I if I print f
[01:01:26 -> 01:01:33]  uh and then we go we just go
[01:01:30 -> 01:01:35]  array sure I'll let get up co-pilot
[01:01:33 -> 01:01:39]  complete
[01:01:35 -> 01:01:39]  that not not correct
[01:01:41 -> 01:01:47]  yeah we'll see that this array if we
[01:01:45 -> 01:01:49]  don't dreference it uh just just on its
[01:01:47 -> 01:01:52]  own it is a memory address it's a
[01:01:49 -> 01:01:56]  pointer to this array um so we set We
[01:01:52 -> 01:02:00]  Set uh let me just delete that we set
[01:01:56 -> 01:02:01]  uh an an integer pointer equal to that
[01:02:00 -> 01:02:05]  so that's a that's a memory address that
[01:02:01 -> 01:02:07]  we have um awesome now we look at our
[01:02:05 -> 01:02:09]  position one which I printed out that's
[01:02:07 -> 01:02:13]  going to be 12 so we have that that
[01:02:09 -> 01:02:15]  start of the array in memory uh and then
[01:02:13 -> 01:02:16]  we just dreference that number so it
[01:02:15 -> 01:02:19]  goes back to
[01:02:16 -> 01:02:21]  12 that's what this asteris is for and
[01:02:19 -> 01:02:24]  we print as the the integer
[01:02:21 -> 01:02:26]  type awesome now we have a for Loop
[01:02:24 -> 01:02:29]  going on here so what the heck is this
[01:02:26 -> 01:02:33]  doing we have we have I starts at zero
[01:02:29 -> 01:02:34]  we want to stop it uh uh when whenever
[01:02:33 -> 01:02:36]  it go whenever it equals 5 we stop it
[01:02:34 -> 01:02:39]  and then we increment by one each time
[01:02:36 -> 01:02:41]  so inside of here we have an integer
[01:02:39 -> 01:02:44]  type and that's going to be the D
[01:02:41 -> 01:02:46]  referenced pointer so whatever pointer
[01:02:44 -> 01:02:49]  is we're going to dreference that back
[01:02:46 -> 01:02:50]  to the original value that it was um and
[01:02:49 -> 01:02:52]  then in here we're going to do the
[01:02:50 -> 01:02:53]  pointer type of the actual pointer
[01:02:52 -> 01:02:56]  itself so this is the memory address
[01:02:53 -> 01:02:58]  that we're seeing um and then we're just
[01:02:56 -> 01:02:59]  going to increment this each time each
[01:02:58 -> 01:03:01]  time this for Loop goes we're going to
[01:02:59 -> 01:03:04]  iterate it once and we're going to
[01:03:01 -> 01:03:06]  increase we're going to increase uh
[01:03:04 -> 01:03:09]  pointer which is the memory address
[01:03:06 -> 01:03:12]  we're going to increment that
[01:03:09 -> 01:03:14]  so uh I had a little example here that I
[01:03:12 -> 01:03:16]  wrote out um these obviously won't be
[01:03:14 -> 01:03:19]  like the same examples every time uh
[01:03:16 -> 01:03:23]  they're going to be different but uh
[01:03:19 -> 01:03:27]  notice how this pointer is incremented
[01:03:23 -> 01:03:31]  by four bytes right so this is is not in
[01:03:27 -> 01:03:35]  uh bits this is in bytes so if we do
[01:03:31 -> 01:03:38]  eight bits in a bite times 4 bytes we
[01:03:35 -> 01:03:39]  get 32 bits which is the integer 32 type
[01:03:38 -> 01:03:41]  the classical integer
[01:03:39 -> 01:03:43]  32 so hopefully it makes a little bit
[01:03:41 -> 01:03:47]  more sense now about how memory is laid
[01:03:43 -> 01:03:49]  out uh at least on the CPU so yeah we
[01:03:47 -> 01:03:53]  just have these essentially skipped by
[01:03:49 -> 01:03:56]  four bytes every time uh 4 bytes time 8
[01:03:53 -> 01:04:00]  bits per BTE is 32 bits and we get our
[01:03:56 -> 01:04:03]  n32 from that I also make a point that
[01:04:00 -> 01:04:06]  uh well right now pointers are not in 32
[01:04:03 -> 01:04:08]  bits in size but we'll see why having
[01:04:06 -> 01:04:13]  them as in32 right now can be a major
[01:04:08 -> 01:04:17]  issue so if you actually do um if we go
[01:04:13 -> 01:04:19]  Python and we do 2 to the 32 we'll get
[01:04:17 -> 01:04:26]  this number so look how big this is this
[01:04:19 -> 01:04:27]  is uh 1 2 3 4 5 6 7 8 uh 9 so that's
[01:04:26 -> 01:04:32]  that's about
[01:04:27 -> 01:04:36]  4.2 that's about 4 GB um in po of 2 of
[01:04:32 -> 01:04:38]  course that's 4 GB so if you have say 8
[01:04:36 -> 01:04:41]  gab of memory which isn't actually that
[01:04:38 -> 01:04:45]  much on this machine I actually have 64
[01:04:41 -> 01:04:47]  so I have 64 GB of memory taken up by
[01:04:45 -> 01:04:48]  like a single array I mean that
[01:04:47 -> 01:04:49]  obviously wouldn't happen that's a
[01:04:48 -> 01:04:52]  really large array but let's just say I
[01:04:49 -> 01:04:54]  have like one that's like you know 64 GB
[01:04:52 -> 01:04:56]  long well we're actually going to get
[01:04:54 -> 01:04:58]  overflow when we try to index that way
[01:04:56 -> 01:05:01]  so you're going to see later on that
[01:04:58 -> 01:05:04]  it's more useful to use uh a certain
[01:05:01 -> 01:05:08]  type for these pointers so if we do 2 to
[01:05:04 -> 01:05:10]  the 64 so u a double Precision integer
[01:05:08 -> 01:05:13]  so N64 you'll see that this is like
[01:05:10 -> 01:05:15]  extremely massive um this is
[01:05:13 -> 01:05:19]  like I don't know somewhere in the
[01:05:15 -> 01:05:21]  exabytes it's like ridiculously high
[01:05:19 -> 01:05:24]  um yeah just we we're we're going to
[01:05:21 -> 01:05:27]  deal with this later but right now this
[01:05:24 -> 01:05:29]  is the general intuition for how uh how
[01:05:27 -> 01:05:31]  these pointers are printed out so if I
[01:05:29 -> 01:05:33]  scroll up a little bit you'll just see
[01:05:31 -> 01:05:36]  uh we print out the dereferenced pointer
[01:05:33 -> 01:05:38]  um at that uh essentially take that
[01:05:36 -> 01:05:39]  memory address and then we dreference it
[01:05:38 -> 01:05:41]  based on the index that it currently is
[01:05:39 -> 01:05:44]  so we just bump up the index one we we
[01:05:41 -> 01:05:46]  jump ahead 32 bits or four bytes and
[01:05:44 -> 01:05:48]  then we print that that value out uh
[01:05:46 -> 01:05:52]  each time and you see that vertically
[01:05:48 -> 01:05:57]  here um I just
[01:05:52 -> 01:05:58]  maybe run that again after it's cleared
[01:05:57 -> 01:06:00]  you'll see that it's it's just like
[01:05:58 -> 01:06:03]  vertically just skips ahead as we'd
[01:06:00 -> 01:06:05]  expect and then this pointer uh which is
[01:06:03 -> 01:06:07]  just the that memory address and then I
[01:06:05 -> 01:06:09]  just put this out for testing sake you
[01:06:07 -> 01:06:11]  don't need to worry about that um but
[01:06:09 -> 01:06:13]  yeah this is the general intuition on uh
[01:06:11 -> 01:06:15]  how how these things are laid out in
[01:06:13 -> 01:06:18]  memory so number six is an interesting
[01:06:15 -> 01:06:21]  one it kind of goes back to example
[01:06:18 -> 01:06:23]  number two where we have this uh we have
[01:06:21 -> 01:06:25]  this value and then pointer pointer
[01:06:23 -> 01:06:27]  pointer just Stacks up uh that that's
[01:06:25 -> 01:06:30]  essentially what sixes so we have these
[01:06:27 -> 01:06:32]  two arrays array 1 and two and they're
[01:06:30 -> 01:06:35]  essentially just these vectors or these
[01:06:32 -> 01:06:39]  these these arrays 1 2 3 4 and then we
[01:06:35 -> 01:06:41]  have an array to uh 5 6 7 8 uh and then
[01:06:39 -> 01:06:44]  we just have an integer pointer uh to
[01:06:41 -> 01:06:46]  those arrays um and then we store
[01:06:44 -> 01:06:49]  another you could say array I just name
[01:06:46 -> 01:06:51]  it Matrix to differentiate and we store
[01:06:49 -> 01:06:53]  those pointers uh essentially on top of
[01:06:51 -> 01:06:55]  each other so what it looks like is we
[01:06:53 -> 01:06:59]  have this we have this Matrix so it's
[01:06:55 -> 01:07:02]  pointer one and pointer two pointer one
[01:06:59 -> 01:07:04]  is the uh it's the it's the pointer to
[01:07:02 -> 01:07:06]  that array the array one and then
[01:07:04 -> 01:07:09]  pointer two is the is the memory address
[01:07:06 -> 01:07:11]  for for array two so essentially what we
[01:07:09 -> 01:07:14]  have it is if we if we look at this uh
[01:07:11 -> 01:07:17]  this array of of like this Matrix it has
[01:07:14 -> 01:07:19]  pointer one pointer two if we flip it so
[01:07:17 -> 01:07:21]  instead of like this and this we we
[01:07:19 -> 01:07:23]  stack them on top of each other so it's
[01:07:21 -> 01:07:25]  pointer one then pointer two it actually
[01:07:23 -> 01:07:29]  looks like a matrix so you'll have your
[01:07:25 -> 01:07:31]  array one which is 1 two 3 4 and then
[01:07:29 -> 01:07:32]  one underneath which is five six seven8
[01:07:31 -> 01:07:35]  and so it actually is like a it's it's
[01:07:32 -> 01:07:38]  like a grid right uh and if we iterate
[01:07:35 -> 01:07:38]  through this
[01:07:41 -> 01:07:44]  um
[01:07:45 -> 01:07:52]  oh you'll actually see um we just
[01:07:48 -> 01:07:55]  essentially iterate through these so uh
[01:07:52 -> 01:07:58]  we we we do J and four uh and we we
[01:07:55 -> 01:08:00]  print out the D reference Matrix at
[01:07:58 -> 01:08:04]  position I so that position I is going
[01:08:00 -> 01:08:08]  to give a memory address for
[01:08:04 -> 01:08:14]  uh it's going to be you know number two
[01:08:08 -> 01:08:16]  uh which is going to be these two arrays
[01:08:14 -> 01:08:17]  um and then we're going to dreference
[01:08:16 -> 01:08:20]  that which is going to give us our
[01:08:17 -> 01:08:21]  actual values and then we're going to
[01:08:20 -> 01:08:24]  iterate this each time of course as the
[01:08:21 -> 01:08:26]  for Loop goes on um and then we just do
[01:08:24 -> 01:08:29]  a next line so that it looks nice nice
[01:08:26 -> 01:08:30]  so that's uh that's pretty much all I
[01:08:29 -> 01:08:32]  have for pointers we're going to jump
[01:08:30 -> 01:08:35]  into custom types now which is more what
[01:08:32 -> 01:08:37]  I was talking about for this for these
[01:08:35 -> 01:08:39]  weird pointer sizes um we're going to
[01:08:37 -> 01:08:39]  dive into
[01:08:40 -> 01:08:45]  this so now I'm going to show you pretty
[01:08:42 -> 01:08:47]  much the equivalent of the torch dot
[01:08:45 -> 01:08:49]  long type so you might have seen this
[01:08:47 -> 01:08:50]  before if you're you know I assume you
[01:08:49 -> 01:08:52]  have some knowledge of python and
[01:08:50 -> 01:08:54]  pytorch so I figured that's the best way
[01:08:52 -> 01:08:57]  to illustrate this we have this size T
[01:08:54 -> 01:08:59]  this size _ T this is typically how you
[01:08:57 -> 01:09:01]  you write this out in C is you'll have
[01:08:59 -> 01:09:03]  uh whatever the type is and then you'll
[01:09:01 -> 01:09:05]  have underscore T to say that this is
[01:09:03 -> 01:09:10]  like a custom type that you made T is
[01:09:05 -> 01:09:13]  for type um and this is specifically for
[01:09:10 -> 01:09:16]  U like big big numbers right so the idea
[01:09:13 -> 01:09:17]  here is this is going to be an unsigned
[01:09:16 -> 01:09:20]  long so
[01:09:17 -> 01:09:25]  uint uh long
[01:09:20 -> 01:09:29]  so int it's going to be a uint 64 that's
[01:09:25 -> 01:09:31]  what it's going to be um if we do this
[01:09:29 -> 01:09:36]  in in torch you'll
[01:09:31 -> 01:09:37]  see I python import torch and then we
[01:09:36 -> 01:09:42]  can make
[01:09:37 -> 01:09:45]  X this just an array of integers uh and
[01:09:42 -> 01:09:49]  then we go x. dtype we'll see a torch.
[01:09:45 -> 01:09:51]  N64 so it's just like that 64 uh bit
[01:09:49 -> 01:09:54]  Precision that we want to store really
[01:09:51 -> 01:09:57]  big really big matrices right so
[01:09:54 -> 01:09:59]  especially in Cuda when you have uh like
[01:09:57 -> 01:10:02]  really really big tensors that are
[01:09:59 -> 01:10:04]  occupying like multiple gigabytes um
[01:10:02 -> 01:10:06]  like on my GPU I have 8 gigabyt of
[01:10:04 -> 01:10:09]  storage so if we're using
[01:10:06 -> 01:10:11]  in32 uh that you might get some overflow
[01:10:09 -> 01:10:14]  errors or you might get some just some
[01:10:11 -> 01:10:15]  some unexpected bad behavior that is
[01:10:14 -> 01:10:17]  going to like mess with things so you
[01:10:15 -> 01:10:20]  don't necessarily want that and that's
[01:10:17 -> 01:10:21]  the whole point of the size type um I
[01:10:20 -> 01:10:25]  kind of wanted to show you that this
[01:10:21 -> 01:10:27]  isn't so bad after all so you know just
[01:10:25 -> 01:10:29]  to step through this we have we have the
[01:10:27 -> 01:10:33]  same array that we went over last time
[01:10:29 -> 01:10:37]  we do this size T type um we use size
[01:10:33 -> 01:10:39]  and then size of array divided by uh
[01:10:37 -> 01:10:42]  divided by the size of an individual
[01:10:39 -> 01:10:47]  integer so it's like the total size of
[01:10:42 -> 01:10:49]  this uh entire thing divided by the uh
[01:10:47 -> 01:10:53]  the size of each individual thing in it
[01:10:49 -> 01:10:57]  so you get the total length of the array
[01:10:53 -> 01:10:59]  um uh and so if we go 01 I already
[01:10:57 -> 01:11:01]  compiled this uh you'll see we get that
[01:10:59 -> 01:11:05]  five right so there's five elements in
[01:11:01 -> 01:11:07]  here uh we print this size out um I'll
[01:11:05 -> 01:11:11]  go over this this Zu part in a second
[01:11:07 -> 01:11:15]  here um but we get that output five and
[01:11:11 -> 01:11:19]  then this eight um we print out the size
[01:11:15 -> 01:11:24]  of this so that's in uh that's in bytes
[01:11:19 -> 01:11:28]  by the way we we when we do a like size
[01:11:24 -> 01:11:33]  of int we if we do like let's see print
[01:11:28 -> 01:11:35]  F uh we'll just say int
[01:11:33 -> 01:11:40]  size uh and then we'll
[01:11:35 -> 01:11:43]  go uh sure in in bites we'll do that uh
[01:11:40 -> 01:11:45]  if I just compile this
[01:11:43 -> 01:11:47]  again we'll see int size and bytes so
[01:11:45 -> 01:11:50]  this is an
[01:11:47 -> 01:11:54]  int32 this just just an in32 and it's
[01:11:50 -> 01:11:56]  four bytes or 32 bits so when we have
[01:11:54 -> 01:11:59]  this that means it's 64 bits just to put
[01:11:56 -> 01:12:02]  that in perspective there so uh when we
[01:11:59 -> 01:12:04]  go to this size T we'll see it's an
[01:12:02 -> 01:12:08]  unsigned type def so we do a type
[01:12:04 -> 01:12:09]  definition unsign so it's a it's only
[01:12:08 -> 01:12:10]  going to be positive because we're
[01:12:09 -> 01:12:12]  storing like a you know you can't have
[01:12:10 -> 01:12:14]  like negative size that's the logic and
[01:12:12 -> 01:12:17]  then we have this long which means it's
[01:12:14 -> 01:12:19]  going to be uh it's going to Lally tell
[01:12:17 -> 01:12:22]  the operating system where we we want 64
[01:12:19 -> 01:12:24]  bits not 32 we want it to be long um and
[01:12:22 -> 01:12:27]  then just the the size T type so we can
[01:12:24 -> 01:12:29]  actually go into this and we can see oh
[01:12:27 -> 01:12:31]  type def size type and then we make this
[01:12:29 -> 01:12:34]  we declare this thing where does this
[01:12:31 -> 01:12:37]  come from oh right here long unsigned
[01:12:34 -> 01:12:38]  integer boom just like that super easy
[01:12:37 -> 01:12:42]  right
[01:12:38 -> 01:12:44]  um and uh I guess to sort of clarify
[01:12:42 -> 01:12:47]  like what the whole deal is here we
[01:12:44 -> 01:12:50]  could just pop back to this link if I
[01:12:47 -> 01:12:53]  just open it on my second screen here
[01:12:50 -> 01:12:53]  and pop it over
[01:12:54 -> 01:13:00]  oh uh we can search for
[01:12:57 -> 01:13:03]  the I'll make this a bit bigger I don't
[01:13:00 -> 01:13:08]  know can I make it bigger there we go so
[01:13:03 -> 01:13:11]  if I go up uh we'll see that we have
[01:13:08 -> 01:13:15]  this uh we have this we have this Z and
[01:13:11 -> 01:13:19]  then we have the U so Z is
[01:13:15 -> 01:13:22]  uh Z is just going to be uh I can't
[01:13:19 -> 01:13:24]  remember exactly what this was for but
[01:13:22 -> 01:13:26]  uh then we have this U which is
[01:13:24 -> 01:13:29]  essentially just the unsigned in right
[01:13:26 -> 01:13:31]  so it's U is unsigned um and then we
[01:13:29 -> 01:13:34]  have this size T type which is what the
[01:13:31 -> 01:13:37]  which is what the Z is for um and then
[01:13:34 -> 01:13:39]  we just we just map that out so uh you
[01:13:37 -> 01:13:42]  know if we had a uh if we had just a
[01:13:39 -> 01:13:44]  regular integer it would still be size T
[01:13:42 -> 01:13:46]  because you can have you know it's still
[01:13:44 -> 01:13:50]  an integer you can do stuff with it but
[01:13:46 -> 01:13:52]  when we have a u uh then you know it's
[01:13:50 -> 01:13:54]  just kind of explicitly you're going to
[01:13:52 -> 01:13:56]  have this size T type so that that's
[01:13:54 -> 01:14:00]  kind of how we M things there and we can
[01:13:56 -> 01:14:02]  use the Z followed by the U uh to
[01:14:00 -> 01:14:05]  properly print that out to print out the
[01:14:02 -> 01:14:07]  type of that so you know you have all
[01:14:05 -> 01:14:09]  these other ones too like uh pointer
[01:14:07 -> 01:14:11]  diff
[01:14:09 -> 01:14:13]  type uh but that's that's generally the
[01:14:11 -> 01:14:13]  intuition
[01:14:14 -> 01:14:19]  there and then next up I just wanted to
[01:14:16 -> 01:14:21]  cover uh declaring your own custom types
[01:14:19 -> 01:14:24]  so you know we saw one in the in the
[01:14:21 -> 01:14:26]  standard uh C library the standard iio
[01:14:24 -> 01:14:28]  uh uh C library the standard definition
[01:14:26 -> 01:14:32]  Library whatever you want to call it the
[01:14:28 -> 01:14:34]  headers um and it's also important that
[01:14:32 -> 01:14:35]  we can declare our own because we might
[01:14:34 -> 01:14:37]  even we might need to use these and we
[01:14:35 -> 01:14:39]  actually will use these later on in the
[01:14:37 -> 01:14:41]  course so typically how it goes is you
[01:14:39 -> 01:14:44]  do this thing called a type def uh which
[01:14:41 -> 01:14:45]  is a type definition and for let's just
[01:14:44 -> 01:14:47]  say we're going to make a point for
[01:14:45 -> 01:14:49]  example it has an X and A Y and their
[01:14:47 -> 01:14:51]  floating Point values so we do this
[01:14:49 -> 01:14:52]  struct which is going to have some
[01:14:51 -> 01:14:54]  elements inside of it it's going to have
[01:14:52 -> 01:14:57]  some it's going to have some of these I
[01:14:54 -> 01:14:58]  guess objects you could say these items
[01:14:57 -> 01:15:01]  uh and it's going to have a float X and
[01:14:58 -> 01:15:03]  a float Y and then we declare we we say
[01:15:01 -> 01:15:05]  this is going to be a type point we do
[01:15:03 -> 01:15:08]  type definition uh it's going to be a
[01:15:05 -> 01:15:10]  struct and we're going to make that a
[01:15:08 -> 01:15:12]  point type essentially and then inside
[01:15:10 -> 01:15:15]  of our int main here since this is
[01:15:12 -> 01:15:17]  already declared we can say a point type
[01:15:15 -> 01:15:19]  like you know you could like you would
[01:15:17 -> 01:15:21]  do an INT P or something we're just
[01:15:19 -> 01:15:23]  replacing replacing int or whatever that
[01:15:21 -> 01:15:25]  type is with point and we're making that
[01:15:23 -> 01:15:28]  variable named p and then we're
[01:15:25 -> 01:15:30]  populating it with some values here so
[01:15:28 -> 01:15:35]  this is going to be our float X and our
[01:15:30 -> 01:15:38]  float y um and then if we go uh you know
[01:15:35 -> 01:15:41]  size of Point here if I just compile
[01:15:38 -> 01:15:42]  this and then run uh you'll see size of
[01:15:41 -> 01:15:46]  point is
[01:15:42 -> 01:15:48]  eight so 8 bytes is four bytes plus four
[01:15:46 -> 01:15:50]  more bytes so each of these is a float
[01:15:48 -> 01:15:53]  32 number that occupies four bytes in
[01:15:50 -> 01:15:55]  memory so when you add these together it
[01:15:53 -> 01:15:57]  occupies a total of eight bytes so this
[01:15:55 -> 01:16:01]  point this point uh type is going to
[01:15:57 -> 01:16:03]  cover uh 8 bytes or 64 bits in memory um
[01:16:01 -> 01:16:06]  and then just this other C++ script is
[01:16:03 -> 01:16:08]  literally the identical to this so you
[01:16:06 -> 01:16:12]  can you can declare things the exact
[01:16:08 -> 01:16:14]  same way in um in C++ except you might
[01:16:12 -> 01:16:17]  just want to use like the io stream
[01:16:14 -> 01:16:20]  instead of uh like you maybe in C++ you
[01:16:17 -> 01:16:23]  would comment this out and you would go
[01:16:20 -> 01:16:26]  uh include uh IO
[01:16:23 -> 01:16:28]  stream and and then you would use uh
[01:16:26 -> 01:16:32]  using namespace
[01:16:28 -> 01:16:34]  STD and then you would uh then you would
[01:16:32 -> 01:16:36]  see out
[01:16:34 -> 01:16:39]  that
[01:16:36 -> 01:16:41]  oh identifier SE is
[01:16:39 -> 01:16:44]  undefined okay I guess not I don't know
[01:16:41 -> 01:16:46]  why that's not working um but but you
[01:16:44 -> 01:16:50]  get the point so very minimal changes so
[01:16:46 -> 01:16:52]  now if we pop over to type casting uh I
[01:16:50 -> 01:16:54]  have two files in here so just a just a
[01:16:52 -> 01:16:57]  single C file and then a read me so
[01:16:54 -> 01:17:01]  inside here I have uh static cast
[01:16:57 -> 01:17:03]  Dynamic cast uh constant casts and
[01:17:01 -> 01:17:05]  reinterpret cast so we're only going to
[01:17:03 -> 01:17:07]  be covering these static cast because
[01:17:05 -> 01:17:09]  these are the safe ones these are mainly
[01:17:07 -> 01:17:13]  what you're going to end up using if any
[01:17:09 -> 01:17:15]  um so in here it it's it's very simple
[01:17:13 -> 01:17:16]  you have just a like say a floating
[01:17:15 -> 01:17:20]  Point number
[01:17:16 -> 01:17:23]  6969 and then we have we just declare an
[01:17:20 -> 01:17:25]  INT and all we do is just have this F
[01:17:23 -> 01:17:28]  and then we do brackets int
[01:17:25 -> 01:17:30]  we we put these we just literally put
[01:17:28 -> 01:17:34]  this right in front of it and that'll
[01:17:30 -> 01:17:36]  statically typ cast uh the float 69.6 n
[01:17:34 -> 01:17:38]  to an INT and what this will do is it'll
[01:17:36 -> 01:17:42]  just truncate that last part so in
[01:17:38 -> 01:17:45]  memory in essentially in binary and bits
[01:17:42 -> 01:17:47]  it'll be laid out as um it'll have that
[01:17:45 -> 01:17:49]  first part so the first uh the first
[01:17:47 -> 01:17:51]  integer piece and then it'll have the
[01:17:49 -> 01:17:53]  decimal and then it'll have the uh it'll
[01:17:51 -> 01:17:56]  have the it'll have the decimal bits
[01:17:53 -> 01:17:58]  afterwards so it'll it'll be like that
[01:17:56 -> 01:18:00]  decimal and then after so what it'll do
[01:17:58 -> 01:18:04]  is when it's when it's typ casting it'll
[01:18:00 -> 01:18:06]  just uh truncate this part and then give
[01:18:04 -> 01:18:11]  give us more Precision for these uh for
[01:18:06 -> 01:18:12]  these essentially int 32 bits so uh it
[01:18:11 -> 01:18:14]  that that's essentially all it's going
[01:18:12 -> 01:18:17]  to look like it's it's just going to be
[01:18:14 -> 01:18:19]  69 uh it's not going to have any decimal
[01:18:17 -> 01:18:23]  places it's going to effectively round
[01:18:19 -> 01:18:26]  down you could say um so if I just pop
[01:18:23 -> 01:18:28]  into
[01:18:26 -> 01:18:30]  uh type
[01:18:28 -> 01:18:33]  casting
[01:18:30 -> 01:18:37]  and run
[01:18:33 -> 01:18:40]  this you'll see uh we get this integer
[01:18:37 -> 01:18:43]  format so just truncated and then when
[01:18:40 -> 01:18:47]  we do a character um this is actually
[01:18:43 -> 01:18:49]  going to convert to uh asky so if you
[01:18:47 -> 01:18:52]  remember your asky tables I'll go ahead
[01:18:49 -> 01:18:56]  and bring this up on the side
[01:18:52 -> 01:18:56]  here asky table
[01:18:56 -> 01:19:01]  uh oh it's right literally right there
[01:18:58 -> 01:19:02]  okay ask.com
[01:19:01 -> 01:19:06]  um we can
[01:19:02 -> 01:19:08]  see that this one right here is an
[01:19:06 -> 01:19:11]  uppercase E
[01:19:08 -> 01:19:14]  right
[01:19:11 -> 01:19:17]  so uppercase
[01:19:14 -> 01:19:19]  E uppercase E look at that how how easy
[01:19:17 -> 01:19:22]  was that right it's not not crazy doing
[01:19:19 -> 01:19:24]  an integer to a character conversion so
[01:19:22 -> 01:19:25]  that's that that's all type casting is
[01:19:24 -> 01:19:26]  I'm not going to go over this
[01:19:25 -> 01:19:28]  extensively cuz it isn't like a crazy
[01:19:26 -> 01:19:30]  piece we end up using in Cuda but just
[01:19:28 -> 01:19:31]  to throw it out there and remind you
[01:19:30 -> 01:19:35]  guys of how how simple this type of
[01:19:31 -> 01:19:38]  thing is another topic I thought was uh
[01:19:35 -> 01:19:41]  briefly worth touching on was macros and
[01:19:38 -> 01:19:43]  Global variables so we can we
[01:19:41 -> 01:19:46]  essentially have these these basic ones
[01:19:43 -> 01:19:50]  like uh you know if if defined if not
[01:19:46 -> 01:19:53]  defined L if else and then end if uh so
[01:19:50 -> 01:19:54]  it it's we're essentially going to use
[01:19:53 -> 01:19:57]  these later on to declare
[01:19:54 -> 01:19:59]  hyperparameters and uh just different
[01:19:57 -> 01:20:01]  Global things that we'll need access to
[01:19:59 -> 01:20:03]  that we don't want to just pass in as an
[01:20:01 -> 01:20:05]  extra you know bloated function argument
[01:20:03 -> 01:20:06]  when you have like 20 arguments in a
[01:20:05 -> 01:20:08]  function you might want to reduce that
[01:20:06 -> 01:20:10]  and just declare some of those locally
[01:20:08 -> 01:20:11]  so you can use them wherever you want as
[01:20:10 -> 01:20:13]  long as they're not changing or you're
[01:20:11 -> 01:20:16]  not doing anything weird with them
[01:20:13 -> 01:20:20]  you're you're okay right so like in this
[01:20:16 -> 01:20:23]  example I do uh Pi like uppercase pi and
[01:20:20 -> 01:20:25]  then we set that equal to uh a double
[01:20:23 -> 01:20:29]  right so
[01:20:25 -> 01:20:31]  uh we can do these functions too so it's
[01:20:29 -> 01:20:33]  kind of like a a Lambda function if you
[01:20:31 -> 01:20:35]  will a Lambda function in Python we do
[01:20:33 -> 01:20:38]  this area and then we we pass whatever
[01:20:35 -> 01:20:41]  we want in so R is our radius and then
[01:20:38 -> 01:20:44]  we do essentially the the radius is piun
[01:20:41 -> 01:20:46]  r^ 2 so we just do PI * R * R and we get
[01:20:44 -> 01:20:49]  that so it's just a little Lambda
[01:20:46 -> 01:20:52]  function you can do as a macro um and
[01:20:49 -> 01:20:55]  then we have if not defined radius so
[01:20:52 -> 01:20:58]  radius isn't defined here if and DEP if
[01:20:55 -> 01:21:00]  not defin um theine radius and we set
[01:20:58 -> 01:21:04]  that value to seven so it's going to be
[01:21:00 -> 01:21:06]  an integer seven we end the if we end
[01:21:04 -> 01:21:09]  this this whole block and radius is a
[01:21:06 -> 01:21:12]  now a declared maer equal to
[01:21:09 -> 01:21:15]  seven uh and then we have some if logic
[01:21:12 -> 01:21:17]  down here so if radius uh you know it's
[01:21:15 -> 01:21:20]  bigger than 10 which is not we Define
[01:21:17 -> 01:21:21]  this so this this is like grade out um
[01:21:20 -> 01:21:24]  it's not smaller than five also grade
[01:21:21 -> 01:21:25]  out and then else U so it's just going
[01:21:24 -> 01:21:28]  to stay at seven and then it's going to
[01:21:25 -> 01:21:29]  end if so we can do if logic in here as
[01:21:28 -> 01:21:32]  well
[01:21:29 -> 01:21:37]  um and then if I just go ahead and pop
[01:21:32 -> 01:21:40]  out of this to uh macros and we go
[01:21:37 -> 01:21:40]  GCC like
[01:21:41 -> 01:21:47]  this we're going to oh
[01:21:48 -> 01:21:53]  double we'll do uh
[01:22:05 -> 01:22:13]  perfect so area of circle with radius 7
[01:22:11 -> 01:22:14]  um is going to be this much and that's
[01:22:13 -> 01:22:18]  that's a floating Point number which we
[01:22:14 -> 01:22:19]  have here so uh this radius I I was not
[01:22:18 -> 01:22:21]  careful there this radius is actually an
[01:22:19 -> 01:22:26]  integer type so we just set that back to
[01:22:21 -> 01:22:28]  D um and then it then it works Bel ly so
[01:22:26 -> 01:22:31]  uh that's that's how you do uh macros
[01:22:28 -> 01:22:33]  pretty easy this part is where my
[01:22:31 -> 01:22:34]  understanding gets a little bit fuzzy I
[01:22:33 -> 01:22:37]  haven't worked with compilers
[01:22:34 -> 01:22:39]  extensively but I have found some really
[01:22:37 -> 01:22:41]  good resources on uh the C and C++
[01:22:39 -> 01:22:44]  compilers so I've just provided some
[01:22:41 -> 01:22:47]  links here for you to go learn um GCC is
[01:22:44 -> 01:22:52]  the most popular C compiler so that's
[01:22:47 -> 01:22:54]  gnu uh C compiler um and then g++ is the
[01:22:52 -> 01:22:57]  same thing but for C++
[01:22:54 -> 01:22:59]  so uh you know I have different articles
[01:22:57 -> 01:23:02]  on here from free code camp that you can
[01:22:59 -> 01:23:05]  go and look at so uh there's there's
[01:23:02 -> 01:23:06]  this one and then we have the other one
[01:23:05 -> 01:23:09]  as
[01:23:06 -> 01:23:12]  well what does a compiler explain for
[01:23:09 -> 01:23:15]  beginners um so like an analogy
[01:23:12 -> 01:23:18]  essentially just converting uh
[01:23:15 -> 01:23:21]  converting your machine code down to uh
[01:23:18 -> 01:23:23]  down to assembly and and you have all
[01:23:21 -> 01:23:25]  these representations in between that
[01:23:23 -> 01:23:27]  that the computer work with to help
[01:23:25 -> 01:23:30]  understand things better uh and then
[01:23:27 -> 01:23:33]  it'll convert that assembly code down to
[01:23:30 -> 01:23:36]  uh essentially the uh CPU instructions
[01:23:33 -> 01:23:38]  in uh in bits and bytes so binary ones
[01:23:36 -> 01:23:40]  and zeros uh and that'll get sent
[01:23:38 -> 01:23:42]  through as as essentially electrons and
[01:23:40 -> 01:23:44]  charges through your through your
[01:23:42 -> 01:23:47]  circuitry in the computer and that is
[01:23:44 -> 01:23:50]  what actually executes this stuff um and
[01:23:47 -> 01:23:51]  then just the C C++ compiler is a little
[01:23:50 -> 01:23:54]  bit
[01:23:51 -> 01:23:56]  uh might be a little bit higher level I
[01:23:54 -> 01:23:59]  know C++ is a higher level language than
[01:23:56 -> 01:24:01]  c um but this these are just like really
[01:23:59 -> 01:24:04]  good explanations that I can't really
[01:24:01 -> 01:24:06]  top myself without messing up so I
[01:24:04 -> 01:24:09]  provided these links here um hopefully
[01:24:06 -> 01:24:12]  this all makes sense but uh we won't
[01:24:09 -> 01:24:14]  really need to understand too much about
[01:24:12 -> 01:24:18]  compilers Downstream it's good to know
[01:24:14 -> 01:24:20]  what they're doing but in order to debug
[01:24:18 -> 01:24:22]  code you just need to know kind of the
[01:24:20 -> 01:24:24]  architecture of what the compiler is
[01:24:22 -> 01:24:26]  what it's doing like where it's where
[01:24:24 -> 01:24:28]  it's stuff not necessarily like all the
[01:24:26 -> 01:24:30]  math and representations happening um
[01:24:28 -> 01:24:34]  it's good to know of course but it's not
[01:24:30 -> 01:24:35]  needed to write functioning code so uh
[01:24:34 -> 01:24:37]  we'll see that later on like we're just
[01:24:35 -> 01:24:39]  going to essentially type in these
[01:24:37 -> 01:24:40]  compiler flags and and all this and that
[01:24:39 -> 01:24:43]  that's what the next section on actually
[01:24:40 -> 01:24:45]  is uh it's on make files so make files
[01:24:43 -> 01:24:47]  are really useful they're going to help
[01:24:45 -> 01:24:51]  you uh be more efficient about
[01:24:47 -> 01:24:53]  developing C C++ and Cuda code so
[01:24:51 -> 01:24:56]  instead of going into here and just
[01:24:53 -> 01:24:58]  typing uh you know GCC every single time
[01:24:56 -> 01:25:00]  and maybe maybe you have autocomplete
[01:24:58 -> 01:25:01]  like me but either way you just want
[01:25:00 -> 01:25:03]  this to be a faster process and you want
[01:25:01 -> 01:25:06]  to be able to automate and and have more
[01:25:03 -> 01:25:07]  control over what happens uh and just
[01:25:06 -> 01:25:11]  manage things better make files is what
[01:25:07 -> 01:25:12]  you want so inside of make files it it
[01:25:11 -> 01:25:14]  looks really complicated and it's like
[01:25:12 -> 01:25:17]  learning this new language but it's
[01:25:14 -> 01:25:21]  really not it's not that bad um you can
[01:25:17 -> 01:25:24]  Define variables so like GCC equals GCC
[01:25:21 -> 01:25:26]  and in order to use these uh I can just
[01:25:24 -> 01:25:28]  do dollar sign and then brackets and
[01:25:26 -> 01:25:30]  then put that variable inside the
[01:25:28 -> 01:25:31]  variable name inside and it'll just
[01:25:30 -> 01:25:34]  pretty much reference this when it's
[01:25:31 -> 01:25:37]  called um this is a command by the way
[01:25:34 -> 01:25:39]  so you'll we'll see this in a second um
[01:25:37 -> 01:25:41]  I'm going to do a little experiment with
[01:25:39 -> 01:25:44]  the Cuda script here but we just have
[01:25:41 -> 01:25:46]  this nvcc maaps to the Nvidia Cuda
[01:25:44 -> 01:25:48]  compiler and then we have Cuda Flags so
[01:25:46 -> 01:25:50]  this is just a little thing that's like
[01:25:48 -> 01:25:51]  my GPU architecture we're going to see
[01:25:50 -> 01:25:54]  this later you don't have to worry about
[01:25:51 -> 01:25:59]  this now but this is just like U my GPU
[01:25:54 -> 01:26:02]  architecture is a uh 8.6 compute
[01:25:59 -> 01:26:04]  capability uh or
[01:26:02 -> 01:26:07]  compute compatibility I think it's
[01:26:04 -> 01:26:09]  capability um either way this is just we
[01:26:07 -> 01:26:11]  could just have flags and we can just
[01:26:09 -> 01:26:13]  have more variables that we plug into
[01:26:11 -> 01:26:14]  this stuff but if I go back to the read
[01:26:13 -> 01:26:18]  me for
[01:26:14 -> 01:26:20]  this we have these targets prerequisites
[01:26:18 -> 01:26:23]  and then our commands nested inside of
[01:26:20 -> 01:26:25]  that so how does this work exactly well
[01:26:23 -> 01:26:29]  I'm going to show you this um just by
[01:26:25 -> 01:26:33]  pretty much example so um if I
[01:26:29 -> 01:26:36]  go if I go I can actually delete this
[01:26:33 -> 01:26:40]  line if I just go make
[01:26:36 -> 01:26:42]  01 see how it makes a binary here and
[01:26:40 -> 01:26:45]  then I can run this binary and it'll say
[01:26:42 -> 01:26:48]  boo this this print F just it just
[01:26:45 -> 01:26:50]  prints boo like it works um just from
[01:26:48 -> 01:26:53]  make 01 so what we're doing is we have
[01:26:50 -> 01:26:55]  this make Command which is for make
[01:26:53 -> 01:26:58]  files kind of maps the there and then we
[01:26:55 -> 01:26:59]  have the 01 part which is the target so
[01:26:58 -> 01:27:01]  this this is the target this is left
[01:26:59 -> 01:27:04]  side of the colon and then after is
[01:27:01 -> 01:27:07]  their prerequisites so notice how I I
[01:27:04 -> 01:27:11]  removed that that other part the
[01:27:07 -> 01:27:15]  01c so this essentially means we're
[01:27:11 -> 01:27:17]  going to uh either confirm that this is
[01:27:15 -> 01:27:20]  that this already existed or has been
[01:27:17 -> 01:27:22]  done um and if it hasn't we're going to
[01:27:20 -> 01:27:25]  do it so you'll see that in these
[01:27:22 -> 01:27:28]  examples down here but
[01:27:25 -> 01:27:30]  just to fill in the rest so we have a
[01:27:28 -> 01:27:32]  bunch of things happening we have this
[01:27:30 -> 01:27:34]  we have this variable GCC which is just
[01:27:32 -> 01:27:38]  saying you know it's it's essentially
[01:27:34 -> 01:27:41]  just going uh you know
[01:27:38 -> 01:27:43]  GCC um it's just doing that and then we
[01:27:41 -> 01:27:46]  put this at sign in front of
[01:27:43 -> 01:27:50]  it which means don't print this out in
[01:27:46 -> 01:27:50]  the terminal so if I just uh remove this
[01:27:50 -> 01:27:54]  and
[01:27:50 -> 01:27:58]  go uh and just remove the at
[01:27:54 -> 01:28:00]  and go uh
[01:27:58 -> 01:28:04]  clean I'll just I'll just remove this
[01:28:00 -> 01:28:08]  for for so that it makes the most
[01:28:04 -> 01:28:10]  sense and then I go make uh 01 you'll
[01:28:08 -> 01:28:15]  see that it actually uh shows us this in
[01:28:10 -> 01:28:16]  the terminal um but if I put an at there
[01:28:15 -> 01:28:18]  then it doesn't right so that just
[01:28:16 -> 01:28:20]  removes it that just makes things more
[01:28:18 -> 01:28:22]  clean and just it's like a best practice
[01:28:20 -> 01:28:25]  just easy to see things um you want to
[01:28:22 -> 01:28:26]  maybe like look at the ones that uh
[01:28:25 -> 01:28:28]  might do weird things and you just want
[01:28:26 -> 01:28:31]  to like ensure that all your variables
[01:28:28 -> 01:28:34]  are correct um but this is a very simple
[01:28:31 -> 01:28:37]  you don't need to print this out um and
[01:28:34 -> 01:28:39]  then just jumping down to uh like number
[01:28:37 -> 01:28:44]  two here
[01:28:39 -> 01:28:45]  so uh number two is uh essentially the
[01:28:44 -> 01:28:49]  same thing
[01:28:45 -> 01:28:51]  so uh I'm not even going to execute that
[01:28:49 -> 01:28:54]  we don't we don't even need to run this
[01:28:51 -> 01:28:58]  um and then 03 is just the ca compiler
[01:28:54 -> 01:28:59]  so we do nvcc and then the Cuda Flags so
[01:28:58 -> 01:29:01]  which essentially what it's going to
[01:28:59 -> 01:29:05]  look like is it's going to go
[01:29:01 -> 01:29:10]  nvcc Das Arch
[01:29:05 -> 01:29:11]  uh get rid of that and then um it's
[01:29:10 -> 01:29:13]  going to pass in
[01:29:11 -> 01:29:16]  O
[01:29:13 -> 01:29:19]  03 CU that that's the binary we want it
[01:29:16 -> 01:29:22]  to be and then
[01:29:19 -> 01:29:24]  that that's going to run and we should
[01:29:22 -> 01:29:27]  be able to go 03 C and it's going to say
[01:29:24 -> 01:29:29]  boo from here right so just just the
[01:29:27 -> 01:29:31]  same thing um we just include like Cuda
[01:29:29 -> 01:29:34]  runtime and all this just to be fancy
[01:29:31 -> 01:29:35]  but it just it just outputs the the boot
[01:29:34 -> 01:29:38]  the boot
[01:29:35 -> 01:29:40]  command
[01:29:38 -> 01:29:42]  um yeah this this is a this is very
[01:29:40 -> 01:29:45]  simple example so it's essentially just
[01:29:42 -> 01:29:47]  converting uh it's converting these
[01:29:45 -> 01:29:49]  variable names and not printing anything
[01:29:47 -> 01:29:52]  out
[01:29:49 -> 01:29:53]  um we just go make 03 and it'll do the
[01:29:52 -> 01:29:55]  exact same thing see it takes a little
[01:29:53 -> 01:29:59]  while
[01:29:55 -> 01:30:02]  same exact thing um we have this clean
[01:29:59 -> 01:30:04]  command which is going to actually
[01:30:02 -> 01:30:07]  remove all these binaries so when we
[01:30:04 -> 01:30:08]  want to clean up everything uh and just
[01:30:07 -> 01:30:10]  you know make it nice and presentable
[01:30:08 -> 01:30:12]  like we haven't done work like what I'm
[01:30:10 -> 01:30:14]  doing with you guys is I'm deleting
[01:30:12 -> 01:30:16]  these binaries before doing the lessons
[01:30:14 -> 01:30:19]  uh because it looks ugly when I have
[01:30:16 -> 01:30:23]  more files so I can just go uh make
[01:30:19 -> 01:30:26]  clean gone right um so that that's just
[01:30:23 -> 01:30:27]  like a very very simple example and it's
[01:30:26 -> 01:30:29]  just it's just like make and then
[01:30:27 -> 01:30:31]  whatever the target is uh going back up
[01:30:29 -> 01:30:34]  to these
[01:30:31 -> 01:30:39]  ones these are a little interesting
[01:30:34 -> 01:30:41]  so 01 uncore obj which is for object
[01:30:39 -> 01:30:45]  this is going to uh
[01:30:41 -> 01:30:48]  GCC uh comp uh it's going to take in
[01:30:45 -> 01:30:50]  this C code and it's going to Output uh
[01:30:48 -> 01:30:52]  this o or this object file so it's going
[01:30:50 -> 01:30:53]  to it's going to essentially this but
[01:30:52 -> 01:30:58]  it's just going to do object file
[01:30:53 -> 01:31:02]  instead of a binary um and then this 01
[01:30:58 -> 01:31:05]  uh obj so this object uh execute
[01:31:02 -> 01:31:08]  run that's going to take in this this
[01:31:05 -> 01:31:10]  previous one as a prerequisite so that
[01:31:08 -> 01:31:12]  means this one has to already be
[01:31:10 -> 01:31:13]  complete and if it's not complete we're
[01:31:12 -> 01:31:18]  going to run it and make sure it's
[01:31:13 -> 01:31:19]  complete in order for this to work so uh
[01:31:18 -> 01:31:22]  we're going
[01:31:19 -> 01:31:24]  to uh compile this object file into a
[01:31:22 -> 01:31:27]  binary so you have the this like the C
[01:31:24 -> 01:31:31]  the C representation and then the object
[01:31:27 -> 01:31:33]  file which we do uh here and then this
[01:31:31 -> 01:31:36]  one takes that new object file and
[01:31:33 -> 01:31:39]  converts that into a binary and then we
[01:31:36 -> 01:31:46]  execute that binary on this line so if I
[01:31:39 -> 01:31:47]  go uh make and then 01 uh obj ex uh
[01:31:46 -> 01:31:49]  execute
[01:31:47 -> 01:31:50]  run and we don't have any of these files
[01:31:49 -> 01:31:54]  in
[01:31:50 -> 01:31:57]  here it's going to
[01:31:54 -> 01:32:00]  it's literally going to uh ensure that
[01:31:57 -> 01:32:03]  this is called first because it hasn't
[01:32:00 -> 01:32:05]  yet um so it's going to it's going to
[01:32:03 -> 01:32:07]  convert this C file into object and then
[01:32:05 -> 01:32:08]  it's going to convert that object into a
[01:32:07 -> 01:32:11]  binary and it's going to execute that
[01:32:08 -> 01:32:13]  binary so this is just I probably overe
[01:32:11 -> 01:32:16]  explain things a little bit but this is
[01:32:13 -> 01:32:18]  this is pretty much the idea on how you
[01:32:16 -> 01:32:21]  uh how you can automate uh just C
[01:32:18 -> 01:32:23]  compilation C++ compilation we're going
[01:32:21 -> 01:32:24]  to use this more for uh Cuda scripts
[01:32:23 -> 01:32:26]  down the road
[01:32:24 -> 01:32:28]  uh but that's that's the general idea
[01:32:26 -> 01:32:30]  and then this phony part at the top
[01:32:28 -> 01:32:32]  might look a little weird but this
[01:32:30 -> 01:32:36]  essentially means uh we're not we're not
[01:32:32 -> 01:32:39]  going to uh it's just like a a a way to
[01:32:36 -> 01:32:41]  make uh things easier to use make it so
[01:32:39 -> 01:32:44]  you don't run into errors I had a decent
[01:32:41 -> 01:32:47]  explanation here so um say we had a make
[01:32:44 -> 01:32:49]  file make file with a Target named clean
[01:32:47 -> 01:32:51]  so in this in this cleanup command that
[01:32:49 -> 01:32:53]  that makes everything nice again suppose
[01:32:51 -> 01:32:55]  we have a directory named clean in the
[01:32:53 -> 01:32:57]  same directory as the make file so here
[01:32:55 -> 01:33:00]  if we had something named clean um if we
[01:32:57 -> 01:33:02]  run make clean make will not run the
[01:33:00 -> 01:33:04]  command um it will not run the command
[01:33:02 -> 01:33:06]  in the Target clean because clean
[01:33:04 -> 01:33:06]  already
[01:33:07 -> 01:33:11]  exists instead it will see that the
[01:33:09 -> 01:33:13]  director clean already exists will not
[01:33:11 -> 01:33:14]  it'll not run it so in short we
[01:33:13 -> 01:33:16]  essentially take a bunch of mappings
[01:33:14 -> 01:33:19]  from Target names to
[01:33:16 -> 01:33:21]  commands um that's that's where this
[01:33:19 -> 01:33:22]  phony thing comes from so it's like a
[01:33:21 -> 01:33:24]  it's like a phony if you will I don't
[01:33:22 -> 01:33:27]  know what the philos was behind that
[01:33:24 -> 01:33:29]  naming but that's that's how it works um
[01:33:27 -> 01:33:32]  and then we have some just some other
[01:33:29 -> 01:33:34]  stuff in here so I already went it over
[01:33:32 -> 01:33:38]  the at symbol and then there's this one
[01:33:34 -> 01:33:42]  too so the the colon equals um I don't
[01:33:38 -> 01:33:45]  think we use this in here but uh equals
[01:33:42 -> 01:33:46]  is used for dividing variables or uh
[01:33:45 -> 01:33:48]  it's called a recursive assignment so
[01:33:46 -> 01:33:50]  both used for dividing variables both of
[01:33:48 -> 01:33:54]  these are this one is a recursive
[01:33:50 -> 01:33:57]  assignment so value of the variable is
[01:33:54 -> 01:33:58]  reevaluated each time it's used um and
[01:33:57 -> 01:34:01]  then this one is a simple assignment or
[01:33:58 -> 01:34:03]  immediate one it's evaluated only once
[01:34:01 -> 01:34:04]  at the point of definition so this is
[01:34:03 -> 01:34:06]  like typically the safer option you want
[01:34:04 -> 01:34:08]  to go down if you get really complicated
[01:34:06 -> 01:34:10]  make files you might end up running into
[01:34:08 -> 01:34:12]  weird things with these recursive
[01:34:10 -> 01:34:14]  assignments so generally it's safe to
[01:34:12 -> 01:34:17]  use these ones but it it looks it looks
[01:34:14 -> 01:34:19]  a little funny so I didn't include it in
[01:34:17 -> 01:34:22]  this example um we will we will use it
[01:34:19 -> 01:34:24]  down the road though so last but not
[01:34:22 -> 01:34:28]  least uh we have debers so debuggers are
[01:34:24 -> 01:34:30]  awesome for just uh an alternative to
[01:34:28 -> 01:34:32]  just adding print lines print just print
[01:34:30 -> 01:34:34]  this print that did we make it here yay
[01:34:32 -> 01:34:37]  we made it ah we failed whatever just
[01:34:34 -> 01:34:38]  adding those just blows your code up so
[01:34:37 -> 01:34:40]  having debuggers makes that a lot easier
[01:34:38 -> 01:34:43]  on you uh you can actually go down to
[01:34:40 -> 01:34:45]  literal assembly and see where the
[01:34:43 -> 01:34:47]  electrons are in your code uh like in
[01:34:45 -> 01:34:49]  your in your script like what is
[01:34:47 -> 01:34:51]  happening on the hardware so uh
[01:34:49 -> 01:34:53]  debuggers are super useful and in
[01:34:51 -> 01:34:57]  particular we're going to be talking
[01:34:53 -> 01:35:00]  about the GDB debugger for C and C++ so
[01:34:57 -> 01:35:03]  you use them for both um I'm not going
[01:35:00 -> 01:35:04]  to like explain these super intensely I
[01:35:03 -> 01:35:05]  don't feel comfortable in my own
[01:35:04 -> 01:35:10]  explanation for these because there's
[01:35:05 -> 01:35:13]  just a lot happening um but there are
[01:35:10 -> 01:35:15]  some commands that you generally want to
[01:35:13 -> 01:35:18]  be familiar with it's mostly just
[01:35:15 -> 01:35:21]  commands and knowing uh what to look for
[01:35:18 -> 01:35:23]  in your script so I have these in the
[01:35:21 -> 01:35:24]  readme file just just with some
[01:35:23 -> 01:35:27]  explanation
[01:35:24 -> 01:35:30]  um but a really good uh overview is here
[01:35:27 -> 01:35:35]  so this is done from lowle learning has
[01:35:30 -> 01:35:36]  an advertisement here but uh essentially
[01:35:35 -> 01:35:40]  it's just it's just a really good
[01:35:36 -> 01:35:41]  overview on uh GDB uh just going into
[01:35:40 -> 01:35:45]  assembly and doing a bunch of cool
[01:35:41 -> 01:35:49]  tricks and and debuging C code that way
[01:35:45 -> 01:35:49]  so I do recommend you watch that
[01:35:50 -> 01:35:54]  video okay we can finally take a
[01:35:52 -> 01:35:56]  breather now that was a lot
[01:35:54 -> 01:35:58]  uh but I really hope that this is going
[01:35:56 -> 01:35:59]  to sort of just ease that uh this is
[01:35:58 -> 01:36:01]  designed to be a more passive part where
[01:35:59 -> 01:36:03]  you can kind of just sit around and
[01:36:01 -> 01:36:05]  listen and watch and just kind of enjoy
[01:36:03 -> 01:36:08]  it uh there's no coding involved here at
[01:36:05 -> 01:36:11]  all um so I thought I'd provide some
[01:36:08 -> 01:36:13]  context on different types of Hardware
[01:36:11 -> 01:36:14]  what the whole purpose of gpus are I
[01:36:13 -> 01:36:15]  mean you probably already know what they
[01:36:14 -> 01:36:17]  are but I just want to provide that
[01:36:15 -> 01:36:19]  background just so that we're on the
[01:36:17 -> 01:36:22]  same page entirely um and just to
[01:36:19 -> 01:36:24]  provide some kind of internal I guess
[01:36:22 -> 01:36:26]  preparation uh for for the next part
[01:36:24 -> 01:36:29]  it's good to have these braks just to
[01:36:26 -> 01:36:31]  you know slow your mind a little bit and
[01:36:29 -> 01:36:36]  uh give yourself some time
[01:36:31 -> 01:36:38]  but first off we have these CPUs okay
[01:36:36 -> 01:36:40]  you already know what a CPU is it's
[01:36:38 -> 01:36:43]  general purpose High clock speed per
[01:36:40 -> 01:36:45]  core very few number of cores the on
[01:36:43 -> 01:36:48]  chip memory is very large so maybe you
[01:36:45 -> 01:36:49]  didn't know that the the caches on the
[01:36:48 -> 01:36:52]  chip are actually quite large compared
[01:36:49 -> 01:36:56]  to the GPU um this is because you know
[01:36:52 -> 01:36:58]  the memory band with from uh the CPU to
[01:36:56 -> 01:37:00]  the the ram slots on your computer those
[01:36:58 -> 01:37:01]  are going to be uh like that transfer
[01:37:00 -> 01:37:03]  speed is going to be very slow you have
[01:37:01 -> 01:37:05]  to move the electrons all the way from
[01:37:03 -> 01:37:07]  this part of the motherboard to this one
[01:37:05 -> 01:37:09]  and that takes time right you're you're
[01:37:07 -> 01:37:10]  just constantly waiting for data to
[01:37:09 -> 01:37:12]  arrive and that's like what takes up
[01:37:10 -> 01:37:15]  most of your time right so you have
[01:37:12 -> 01:37:16]  these big caches for just purposely like
[01:37:15 -> 01:37:21]  load pre-loading things on so that
[01:37:16 -> 01:37:23]  they're ready to use um you have lower
[01:37:21 -> 01:37:25]  latency so the whole idea of a CPU is to
[01:37:23 -> 01:37:27]  just just complete this task as quickly
[01:37:25 -> 01:37:30]  as possible and just return the value
[01:37:27 -> 01:37:31]  just complete it complete it fast um and
[01:37:30 -> 01:37:34]  then they have low throughput as well so
[01:37:31 -> 01:37:38]  low throughput means um it can't do as
[01:37:34 -> 01:37:41]  much comparatively it can't do as uh as
[01:37:38 -> 01:37:42]  much operations per second as a GPU can
[01:37:41 -> 01:37:44]  if you're talking about simple
[01:37:42 -> 01:37:46]  instructions if they're more complex
[01:37:44 -> 01:37:48]  ones like managing and loading data and
[01:37:46 -> 01:37:51]  doing like file reads and wres like
[01:37:48 -> 01:37:54]  that'll be faster but if you're talking
[01:37:51 -> 01:37:55]  like math and matrix multiplication
[01:37:54 -> 01:37:58]  uh it is going to be significantly
[01:37:55 -> 01:38:01]  slower throughput is more talked about
[01:37:58 -> 01:38:04]  as like operations per second so if I
[01:38:01 -> 01:38:05]  have a bunch of cores running at say uh
[01:38:04 -> 01:38:08]  2 billion clocks per second and I have
[01:38:05 -> 01:38:10]  6,000 of them versus six cores that are
[01:38:08 -> 01:38:11]  running at 5 billion clocks per second
[01:38:10 -> 01:38:13]  do the math right how many operations
[01:38:11 -> 01:38:15]  are you going to do on this one versus
[01:38:13 -> 01:38:16]  that one that's the whole point of a GPU
[01:38:15 -> 01:38:19]  way more cores a little bit lower clock
[01:38:16 -> 01:38:20]  speed um but way more cores it's is
[01:38:19 -> 01:38:24]  completely outnumbered CPU and that's
[01:38:20 -> 01:38:27]  why it's faster um and then on
[01:38:24 -> 01:38:28]  gpus we have the 90 hasn't been released
[01:38:27 -> 01:38:35]  yet but I thought it was funny to put
[01:38:28 -> 01:38:37]  there um gpus are very specialized so
[01:38:35 -> 01:38:39]  they can accomplish simpler instructions
[01:38:37 -> 01:38:41]  easier to handle ones hence why they
[01:38:39 -> 01:38:43]  have smaller controllers on them which
[01:38:41 -> 01:38:46]  you'll see in a second um they have a
[01:38:43 -> 01:38:49]  lower clock speed like I said way more
[01:38:46 -> 01:38:51]  cores and a lower cache so because you
[01:38:49 -> 01:38:52]  have that on chip memory because you
[01:38:51 -> 01:38:54]  actually have this vram that you can
[01:38:52 -> 01:38:56]  access that is on on the GPU and that
[01:38:54 -> 01:38:58]  this all of the Nvidia Hardware
[01:38:56 -> 01:39:01]  Engineers has essentially optimized for
[01:38:58 -> 01:39:03]  accessing that um you're able to get a
[01:39:01 -> 01:39:06]  lot higher memory bandwidth that way up
[01:39:03 -> 01:39:09]  in the like a high 100 gigabyte per
[01:39:06 -> 01:39:12]  second range um so it's like it's it's
[01:39:09 -> 01:39:14]  in the hundreds for sure um you have
[01:39:12 -> 01:39:16]  higher latency on this remember it's not
[01:39:14 -> 01:39:18]  for you know minimize the amount of time
[01:39:16 -> 01:39:20]  it takes to complete this task and then
[01:39:18 -> 01:39:22]  just done right it's it's more optimized
[01:39:20 -> 01:39:25]  for throughput so we already talked
[01:39:22 -> 01:39:28]  about this but um then you have these
[01:39:25 -> 01:39:30]  tpus which came across recently and
[01:39:28 -> 01:39:35]  these are for modern deep learning
[01:39:30 -> 01:39:36]  applications so tpus are for literally
[01:39:35 -> 01:39:38]  just processing tensors like you do fast
[01:39:36 -> 01:39:40]  matrix multiplication fast linear
[01:39:38 -> 01:39:42]  algebra that's that's what it is tensor
[01:39:40 -> 01:39:45]  is linear tensor operations is linear
[01:39:42 -> 01:39:48]  algebra um and it's just specialized for
[01:39:45 -> 01:39:50]  doing that so tpus are faster but way
[01:39:48 -> 01:39:52]  more expensive and specialized in
[01:39:50 -> 01:39:54]  typically not consumer grade Hardware so
[01:39:52 -> 01:39:57]  that's why we learn how to you know
[01:39:54 -> 01:39:58]  build on top of infrastructure with gpus
[01:39:57 -> 01:40:01]  cuz it's you know you can actually
[01:39:58 -> 01:40:04]  afford it you can have one at your house
[01:40:01 -> 01:40:07]  and for fairly low cost um and then you
[01:40:04 -> 01:40:09]  have these fbt which I don't expect you
[01:40:07 -> 01:40:11]  know what these are but these field
[01:40:09 -> 01:40:13]  programmable gate arrays are very
[01:40:11 -> 01:40:15]  specialized pieces of Hardware that
[01:40:13 -> 01:40:18]  essentially say uh instead of having to
[01:40:15 -> 01:40:20]  write like a or build a custom Hardware
[01:40:18 -> 01:40:22]  configuration uh like for a certain task
[01:40:20 -> 01:40:24]  for making something that you need to
[01:40:22 -> 01:40:26]  run really really fast
[01:40:24 -> 01:40:29]  um you can just program these you can
[01:40:26 -> 01:40:32]  just program the actual chips to do uh
[01:40:29 -> 01:40:34]  something more more fine grain on what
[01:40:32 -> 01:40:36]  you want so there's more control over it
[01:40:34 -> 01:40:39]  very expensive very low latency very
[01:40:36 -> 01:40:41]  high throughput uh high power all this
[01:40:39 -> 01:40:44]  right they're these are these are more
[01:40:41 -> 01:40:45]  expensive but uh they allow for
[01:40:44 -> 01:40:48]  modularity if you
[01:40:45 -> 01:40:50]  will um and then just for some
[01:40:48 -> 01:40:54]  background on GPU history so you know
[01:40:50 -> 01:40:56]  back in 1993 when Jensen started Nvidia
[01:40:54 -> 01:40:59]  they had the uh you know they the
[01:40:56 -> 01:41:03]  GeForce cards all of these I wasn't
[01:40:59 -> 01:41:06]  alive during this time um but you have
[01:41:03 -> 01:41:08]  uh then you start getting into you know
[01:41:06 -> 01:41:10]  after after the g47 you start getting
[01:41:08 -> 01:41:15]  into uh these these better ones so like
[01:41:10 -> 01:41:17]  the Tesla cards the fery uh the Kepler
[01:41:15 -> 01:41:19]  Maxwell Pascal and then volto is when
[01:41:17 -> 01:41:21]  things really started taking off the
[01:41:19 -> 01:41:24]  Pascal and the Volta cards then you have
[01:41:21 -> 01:41:27]  taring and then a which is what my card
[01:41:24 -> 01:41:29]  is based on and then you have Hopper
[01:41:27 -> 01:41:31]  which uh in case you didn't know Hopper
[01:41:29 -> 01:41:35]  cards are really really fast like the
[01:41:31 -> 01:41:38]  h100s and the h20s and even the recently
[01:41:35 -> 01:41:41]  released Nvidia Blackwell chips yeah
[01:41:38 -> 01:41:43]  those are ridiculous um this is this is
[01:41:41 -> 01:41:46]  a little bit outdated of course so there
[01:41:43 -> 01:41:48]  are actually uh like chips on here uh I
[01:41:46 -> 01:41:51]  just got like a an outdated screenshot
[01:41:48 -> 01:41:52]  but you get the idea this is how it
[01:41:51 -> 01:41:56]  progressed uh and then you have like the
[01:41:52 -> 01:41:57]  relative clock speed per core on here so
[01:41:56 -> 01:41:59]  you know some of these were like really
[01:41:57 -> 01:42:02]  really high but didn't have very many
[01:41:59 -> 01:42:04]  cores um and then Nvidia kind of figured
[01:42:02 -> 01:42:08]  out like okay we should just put more
[01:42:04 -> 01:42:11]  and more cores on these things right um
[01:42:08 -> 01:42:14]  and then you get the the overall uh I
[01:42:11 -> 01:42:17]  think floating Point performance is what
[01:42:14 -> 01:42:19]  this is so
[01:42:17 -> 01:42:21]  uh yeah you once you get to Volta it's
[01:42:19 -> 01:42:24]  it's starting to get uh it's starting to
[01:42:21 -> 01:42:27]  get really high so I think this is uh
[01:42:24 -> 01:42:30]  double Precision Giga
[01:42:27 -> 01:42:33]  flops so
[01:42:30 -> 01:42:36]  uh you know six essentially six Tera
[01:42:33 -> 01:42:38]  flops of compute on the FTA architecture
[01:42:36 -> 01:42:40]  which is pretty good um and then it gets
[01:42:38 -> 01:42:43]  better and better from there on mine I
[01:42:40 -> 01:42:45]  think I have right now it runs at a high
[01:42:43 -> 01:42:50]  of around
[01:42:45 -> 01:42:51]  23 uh 23 Tera flops on kuas so kuas is
[01:42:50 -> 01:42:53]  the fast linear algebra library that
[01:42:51 -> 01:42:55]  multiplies matrices really quickly and
[01:42:53 -> 01:42:59]  I'm on there I'm about to get pretty
[01:42:55 -> 01:43:02]  much like 20 uh 20 gig flops of single
[01:42:59 -> 01:43:04]  Precision compute um so that's that's
[01:43:02 -> 01:43:06]  that's that's quite
[01:43:04 -> 01:43:08]  good
[01:43:06 -> 01:43:10]  um what makes these things so fast for
[01:43:08 -> 01:43:15]  deep learning I didn't actually cover
[01:43:10 -> 01:43:17]  this um on the CPU you have very little
[01:43:15 -> 01:43:18]  cores you have these big control units
[01:43:17 -> 01:43:20]  that are taking up a ton of space you
[01:43:18 -> 01:43:23]  have all these caches everywhere that
[01:43:20 -> 01:43:26]  are flooding the thing um
[01:43:23 -> 01:43:28]  and like there you you're you're not
[01:43:26 -> 01:43:29]  giving that much uh leverage to the
[01:43:28 -> 01:43:31]  course right like the course can do
[01:43:29 -> 01:43:33]  Advanced complex instructions but there
[01:43:31 -> 01:43:35]  aren't that many of them so you can only
[01:43:33 -> 01:43:37]  do so much whereas if you have this
[01:43:35 -> 01:43:40]  other architecture where you have
[01:43:37 -> 01:43:44]  simpler instructions simpler controllers
[01:43:40 -> 01:43:46]  simpler uh registers smaller ones uh but
[01:43:44 -> 01:43:48]  a ton of cores like see how most of this
[01:43:46 -> 01:43:51]  is taken up by cores and then just
[01:43:48 -> 01:43:54]  caches and RAM um that's that's ideal
[01:43:51 -> 01:43:57]  for gpus so on here you're too
[01:43:54 -> 01:43:58]  essentially the the IDE the idea here is
[01:43:57 -> 01:44:00]  you're trying to put together a puzzle
[01:43:58 -> 01:44:03]  you're trying to put together a jigsaw
[01:44:00 -> 01:44:05]  puzzle and the point is is it doesn't
[01:44:03 -> 01:44:07]  matter which order you do it in so you
[01:44:05 -> 01:44:09]  you don't have to do like this row and
[01:44:07 -> 01:44:11]  then or you this column this column this
[01:44:09 -> 01:44:13]  like it doesn't matter you do this piece
[01:44:11 -> 01:44:15]  here this piece there do like a block
[01:44:13 -> 01:44:16]  like a chunk there it doesn't matter as
[01:44:15 -> 01:44:18]  long as it's all assembled together
[01:44:16 -> 01:44:19]  properly in the end that's what you care
[01:44:18 -> 01:44:23]  about and that's what the GPU is really
[01:44:19 -> 01:44:25]  good at so typically you'll do things as
[01:44:23 -> 01:44:27]  like uh you know one like multiple
[01:44:25 -> 01:44:29]  puzzle pieces at a time or multiple
[01:44:27 -> 01:44:30]  blocks of puzzle pieces at a time so
[01:44:29 -> 01:44:33]  you'll have like a 2 by two or like a
[01:44:30 -> 01:44:35]  4x4 thing of Jigsaw pieces that you'll
[01:44:33 -> 01:44:37]  maybe do at a time that that's like the
[01:44:35 -> 01:44:40]  that's the intuition behind uh Cuda and
[01:44:37 -> 01:44:42]  how how you program gpus to run fast and
[01:44:40 -> 01:44:44]  solve these problems quickly um on the
[01:44:42 -> 01:44:46]  CPU you might be able to only do like
[01:44:44 -> 01:44:48]  see there's only four cores here so you
[01:44:46 -> 01:44:50]  might be able to only do uh four given
[01:44:48 -> 01:44:53]  pieces of that puzzle at the same at
[01:44:50 -> 01:44:56]  once uh whereas GPU you you know let's
[01:44:53 -> 01:44:59]  say you have like 6,000 cores right uh
[01:44:56 -> 01:45:00]  if this if this puzzle has like I don't
[01:44:59 -> 01:45:03]  know say like
[01:45:00 -> 01:45:06]  12,000 um if it has if it has 12,000
[01:45:03 -> 01:45:08]  pieces well you can effectively do that
[01:45:06 -> 01:45:10]  in two operations because you're able to
[01:45:08 -> 01:45:12]  do the first 6,000 in one and then the
[01:45:10 -> 01:45:14]  other 6,000 in the second so it's
[01:45:12 -> 01:45:18]  effectively two operations that you do
[01:45:14 -> 01:45:21]  it in but if you divide 12,000 by 4 four
[01:45:18 -> 01:45:23]  meaning number of CPU cores um you
[01:45:21 -> 01:45:25]  actually get 3,000 operations see you
[01:45:23 -> 01:45:28]  can see how that can be drastically sped
[01:45:25 -> 01:45:30]  up um that's that's why gpus are so fast
[01:45:28 -> 01:45:33]  because you can because you can do that
[01:45:30 -> 01:45:34]  um now there there are some there are
[01:45:33 -> 01:45:36]  some common terms that we refer to these
[01:45:34 -> 01:45:38]  things through so CPU is the host you're
[01:45:36 -> 01:45:40]  going to see this in Cuda once we start
[01:45:38 -> 01:45:42]  writing kernels uh the CPU is called the
[01:45:40 -> 01:45:44]  host which is pretty obvious and then it
[01:45:42 -> 01:45:46]  just kind of just kind of makes sense
[01:45:44 -> 01:45:48]  for that to be named that way and then
[01:45:46 -> 01:45:50]  the GPU is the device so you have the
[01:45:48 -> 01:45:56]  host CPU and then GPU which is the
[01:45:50 -> 01:45:57]  device um the CPU is going to uh mainly
[01:45:56 -> 01:45:59]  the performance there is going to be
[01:45:57 -> 01:46:02]  latency in seconds so you're looking at
[01:45:59 -> 01:46:05]  latency how quick can I do a given task
[01:46:02 -> 01:46:06]  and the GPU is throughput in tasks per
[01:46:05 -> 01:46:08]  second so for example if you're doing a
[01:46:06 -> 01:46:12]  rendering task it's like how many pixels
[01:46:08 -> 01:46:14]  can I render per second um or how many
[01:46:12 -> 01:46:17]  uh I don't know how many pixels can I
[01:46:14 -> 01:46:21]  can I yeah sure render per second that's
[01:46:17 -> 01:46:25]  fine um in a typical Cuda program you're
[01:46:21 -> 01:46:26]  going to allocate at uh some memory on
[01:46:25 -> 01:46:28]  CPU memory so it's going to be a
[01:46:26 -> 01:46:31]  classical like C Malik that's what
[01:46:28 -> 01:46:34]  you're going to do and then once it's
[01:46:31 -> 01:46:37]  allocated on CPU or host you're going to
[01:46:34 -> 01:46:40]  copy from host to device or or CPU to
[01:46:37 -> 01:46:43]  GPU um and then once it's on the GPU you
[01:46:40 -> 01:46:45]  can actually launch a kernel which is
[01:46:43 -> 01:46:47]  what these parallel functions are
[01:46:45 -> 01:46:50]  essentially so on a CPU you have a a
[01:46:47 -> 01:46:52]  function then GPU you have a a kernel
[01:46:50 -> 01:46:54]  which is a GPU function that can be
[01:46:52 -> 01:46:56]  parallelized
[01:46:54 -> 01:46:58]  um and that's that's the main intuition
[01:46:56 -> 01:47:00]  there is you you start off with CPU move
[01:46:58 -> 01:47:02]  everything do everything really fast on
[01:47:00 -> 01:47:04]  GPU and then once you're done with the
[01:47:02 -> 01:47:05]  results you move them back and then do
[01:47:04 -> 01:47:07]  whatever you want with them from there
[01:47:05 -> 01:47:09]  you might even continue to just feed it
[01:47:07 -> 01:47:11]  more into into more and more kernels
[01:47:09 -> 01:47:13]  until the whole thing is done right uh
[01:47:11 -> 01:47:16]  but that's like that's the that's the
[01:47:13 -> 01:47:18]  ideal workflow is you have CPU and then
[01:47:16 -> 01:47:20]  this GPU thing is like an intermediary
[01:47:18 -> 01:47:24]  which you have to convert back to CPU to
[01:47:20 -> 01:47:26]  do something useful with um
[01:47:24 -> 01:47:28]  the kernel looks like a Serial program
[01:47:26 -> 01:47:29]  so if you we're going to look at these
[01:47:28 -> 01:47:32]  in a second here when we jump into
[01:47:29 -> 01:47:35]  actually writing these but uh it's it's
[01:47:32 -> 01:47:38]  going to be a very simple
[01:47:35 -> 01:47:40]  function and it's going to it's going to
[01:47:38 -> 01:47:42]  have a very few lines uh it's going to
[01:47:40 -> 01:47:44]  it's going to look like this basic
[01:47:42 -> 01:47:47]  serial script except it's going to have
[01:47:44 -> 01:47:49]  some key terms in it um these are mainly
[01:47:47 -> 01:47:50]  threads blocks and grids don't even
[01:47:49 -> 01:47:51]  worry about those terms right now we're
[01:47:50 -> 01:47:54]  going to get into those I'm going to
[01:47:51 -> 01:47:56]  explain the philosophy behind them uh
[01:47:54 -> 01:47:58]  I'm going to explain uh pretty much the
[01:47:56 -> 01:48:00]  whole Cuda architecture for you and just
[01:47:58 -> 01:48:02]  help you understand what the heck these
[01:48:00 -> 01:48:04]  things are for now some common terms to
[01:48:02 -> 01:48:07]  remember before we actually start
[01:48:04 -> 01:48:09]  jumping into this stuff are uh well
[01:48:07 -> 01:48:11]  first of all kernels so kernels is like
[01:48:09 -> 01:48:15]  a weird term um you might have thought
[01:48:11 -> 01:48:16]  like popcorn kernels like like this is
[01:48:15 -> 01:48:18]  what I thought I was like what popcorn
[01:48:16 -> 01:48:21]  Kels why are we using those on on
[01:48:18 -> 01:48:23]  computers that doesn't make sense um and
[01:48:21 -> 01:48:25]  then I jumped over to convolution
[01:48:23 -> 01:48:27]  kernels which is like uh when you do
[01:48:25 -> 01:48:28]  like a convolution operation you might
[01:48:27 -> 01:48:30]  have seen this in like cnns if you've
[01:48:28 -> 01:48:31]  done a lot of stuff in like maybe
[01:48:30 -> 01:48:34]  pytorch and you've like look through the
[01:48:31 -> 01:48:36]  intuition on that it's like a a sliding
[01:48:34 -> 01:48:39]  kernel that does a that does like an
[01:48:36 -> 01:48:42]  image processing thing on on yeah just
[01:48:39 -> 01:48:44]  images um that that the filter that
[01:48:42 -> 01:48:46]  slides and does calculations that's
[01:48:44 -> 01:48:48]  called a kernel so I was like uh is it
[01:48:46 -> 01:48:50]  that no no it's not it's not a
[01:48:48 -> 01:48:53]  convolution kernel uh it's also not a
[01:48:50 -> 01:48:54]  Linux kernel either but enough so
[01:48:53 -> 01:48:57]  there's lots of different kernels we
[01:48:54 -> 01:48:58]  have there's actually four kernels
[01:48:57 -> 01:49:01]  popcorn kernels convolution kernels
[01:48:58 -> 01:49:02]  Linux kernels uh but the best one is GPU
[01:49:01 -> 01:49:04]  kernels so that's the ones we're going
[01:49:02 -> 01:49:06]  to be working with um there's actually a
[01:49:04 -> 01:49:09]  little keyword that you highlight you
[01:49:06 -> 01:49:12]  goore Global uncore uncore and that
[01:49:09 -> 01:49:13]  defines a a kernel on the GPU so there's
[01:49:12 -> 01:49:17]  actually a way we can explicitly say
[01:49:13 -> 01:49:19]  that and uh yeah not not an external
[01:49:17 -> 01:49:21]  story I thought the same thing too it's
[01:49:19 -> 01:49:23]  like which one is it um but yeah so
[01:49:21 -> 01:49:25]  we're going to go into two threads
[01:49:23 -> 01:49:27]  blocks and grids that's going to be one
[01:49:25 -> 01:49:30]  of the main things in the next chapter
[01:49:27 -> 01:49:34]  um and then two more like sort of just
[01:49:30 -> 01:49:37]  lingo terms are gem so G mm uh this
[01:49:34 -> 01:49:41]  stands for General matrix multiplication
[01:49:37 -> 01:49:44]  so what this generally means is uh you
[01:49:41 -> 01:49:47]  have it's not just multiplying like a
[01:49:44 -> 01:49:52]  and a * bals C it's not it's not a mapal
[01:49:47 -> 01:49:54]  um entirely you actually do a ml um and
[01:49:52 -> 01:49:56]  then you have this Alpha you have this
[01:49:54 -> 01:49:59]  Alpha parameter which you scale the
[01:49:56 -> 01:50:03]  result of that by uh and then you add it
[01:49:59 -> 01:50:05]  to uh this beta scaler times this times
[01:50:03 -> 01:50:07]  The Matrix C which is the shape of the
[01:50:05 -> 01:50:09]  output Matrix so that's that's like a
[01:50:07 -> 01:50:11]  lot of linear algebra which I'm not
[01:50:09 -> 01:50:12]  going to cover right now but in case
[01:50:11 -> 01:50:16]  that's like in case that makes sense to
[01:50:12 -> 01:50:19]  you it's essentially this Alpha time uh
[01:50:16 -> 01:50:22]  time mL of A and B and then you add that
[01:50:19 -> 01:50:26]  to a scalar B * C which is the shape of
[01:50:22 -> 01:50:29]  that Matrix um that that's what a gem is
[01:50:26 -> 01:50:31]  so it's it's a mmal but with more uh and
[01:50:29 -> 01:50:34]  then you have S gem so that's just
[01:50:31 -> 01:50:36]  general MMO but with but with uh single
[01:50:34 -> 01:50:38]  Precision so it's explicitly single
[01:50:36 -> 01:50:40]  Precision um you can do a a half M Mo so
[01:50:38 -> 01:50:42]  like an
[01:50:40 -> 01:50:45]  fp6 uh you can do double mol you
[01:50:42 -> 01:50:46]  typically don't though it's like FP
[01:50:45 -> 01:50:52]  fp64
[01:50:46 -> 01:50:55]  um but yeah so generally speaking um gem
[01:50:52 -> 01:50:58]  s gem those are important then you have
[01:50:55 -> 01:51:02]  the CPU which is also called The Host
[01:50:58 -> 01:51:05]  which runs functions versus the GPU
[01:51:02 -> 01:51:07]  which is called the device and it runs
[01:51:05 -> 01:51:10]  kernels
[01:51:07 -> 01:51:12]  um and that that's pretty much it I hope
[01:51:10 -> 01:51:14]  I hope that wasn't too hard uh we're
[01:51:12 -> 01:51:17]  going to dig into some kernels now this
[01:51:14 -> 01:51:18]  this part's going to be uh a little bit
[01:51:17 -> 01:51:19]  it's going to be a little bit intensive
[01:51:18 -> 01:51:22]  you'll need to pay attention a little
[01:51:19 -> 01:51:24]  bit but uh it's going to be fun and I
[01:51:22 -> 01:51:26]  promised by the end uh you're going to
[01:51:24 -> 01:51:28]  be really enlightened you're going to
[01:51:26 -> 01:51:29]  like the first part of Cuda isn't
[01:51:28 -> 01:51:31]  actually that hard we're just going to
[01:51:29 -> 01:51:33]  cover very very basic kernels like
[01:51:31 -> 01:51:35]  vector addition it's not going to be
[01:51:33 -> 01:51:36]  that bad at all um but just to introduce
[01:51:35 -> 01:51:38]  you to the philosophy and the whole
[01:51:36 -> 01:51:41]  design principles of like basic uh Cuda
[01:51:38 -> 01:51:41]  seat
[01:51:41 -> 01:51:45]  programming okay so now things are going
[01:51:44 -> 01:51:48]  to get a little bit more technical but I
[01:51:45 -> 01:51:50]  figured we would kind of enter smoothly
[01:51:48 -> 01:51:52]  by just doing a fun and useful activity
[01:51:50 -> 01:51:55]  so I pulled up a bunch of Wikipedia
[01:51:52 -> 01:51:56]  articles on various GPU architectures
[01:51:55 -> 01:51:58]  then we're going to dive into what does
[01:51:56 -> 01:52:02]  your GPU actually look like like what
[01:51:58 -> 01:52:04]  what are the stats of that um so just
[01:52:02 -> 01:52:05]  like looking at these in general let's
[01:52:04 -> 01:52:07]  you know we're just looking for things
[01:52:05 -> 01:52:09]  that are like useful to know some maybe
[01:52:07 -> 01:52:11]  some little history kind of like the uh
[01:52:09 -> 01:52:13]  intro to GPU section that we previously
[01:52:11 -> 01:52:16]  did
[01:52:13 -> 01:52:18]  um so like Pascal was an older one that
[01:52:16 -> 01:52:20]  we used to have um there's a bunch of
[01:52:18 -> 01:52:23]  cool stuff about this so like the 1080
[01:52:20 -> 01:52:26]  and the and the and the 10 and the 10 70
[01:52:23 -> 01:52:27]  uh we both based off Pascal um you know
[01:52:26 -> 01:52:29]  you have a bunch of information about
[01:52:27 -> 01:52:34]  you know where it's from all of this um
[01:52:29 -> 01:52:36]  but what's really cool is um if we
[01:52:34 -> 01:52:38]  scroll down you have all the technical
[01:52:36 -> 01:52:41]  details on these things it's crazy how
[01:52:38 -> 01:52:42]  much Wikipedia has um but yeah like you
[01:52:41 -> 01:52:44]  we have these
[01:52:42 -> 01:52:47]  tables that
[01:52:44 -> 01:52:49]  will that will essentially tell us like
[01:52:47 -> 01:52:53]  which which generation which Generations
[01:52:49 -> 01:52:56]  had what so like you know texture cast
[01:52:53 -> 01:52:59]  per SM which we'll go into later
[01:52:56 -> 01:53:02]  um dedicated shared memory per uh SM or
[01:52:59 -> 01:53:04]  streaming multiprocessor L2 cach per
[01:53:02 -> 01:53:05]  chip right so you have all these
[01:53:04 -> 01:53:07]  statistics and you can sort of compare
[01:53:05 -> 01:53:10]  these over time so if we jump up to
[01:53:07 -> 01:53:14]  emper which is actually what my GPU my
[01:53:10 -> 01:53:19]  RTX 3070 is based off of
[01:53:14 -> 01:53:23]  um you scroll down and uh you get the
[01:53:19 -> 01:53:26]  same thing right so like L2 cache um and
[01:53:23 -> 01:53:31]  then L2 cach so like 512 kilobytes and
[01:53:26 -> 01:53:34]  then this one is um 40 megabytes right
[01:53:31 -> 01:53:37]  so you get some interesting comparisons
[01:53:34 -> 01:53:40]  here but
[01:53:37 -> 01:53:42]  um yeah these this is just kind of the
[01:53:40 -> 01:53:44]  stuff you want to be looking at uh when
[01:53:42 -> 01:53:46]  it comes to like GPU specs especially if
[01:53:44 -> 01:53:49]  you don't have one right you're going to
[01:53:46 -> 01:53:51]  find a lot of useful information here um
[01:53:49 -> 01:53:53]  you know going to Ampere like you have
[01:53:51 -> 01:53:56]  these a100 s that have been used to
[01:53:53 -> 01:53:58]  train very big models uh and that's it's
[01:53:56 -> 01:54:00]  Amper 100 right that's that's what it's
[01:53:58 -> 01:54:04]  called um
[01:54:00 -> 01:54:06]  so bunch of cool statistics um which
[01:54:04 -> 01:54:08]  different precisions and data types do
[01:54:06 -> 01:54:13]  they support so
[01:54:08 -> 01:54:17]  like the uh like for example
[01:54:13 -> 01:54:17]  Volta um Volta
[01:54:18 -> 01:54:22]  supports Volta doesn't support brain
[01:54:20 -> 01:54:24]  float 16 but a100 does support brain
[01:54:22 -> 01:54:26]  flat 16 so interesting stuff like that
[01:54:24 -> 01:54:29]  which you can sort of just do a side
[01:54:26 -> 01:54:31]  comparison of AD love lace this micro
[01:54:29 -> 01:54:34]  architecture is what is used in the 40
[01:54:31 -> 01:54:38]  series cards so ere is like the 30
[01:54:34 -> 01:54:41]  series um I believe if we actually go to
[01:54:38 -> 01:54:41]  Volta
[01:54:41 -> 01:54:46]  architecture uh Volta
[01:54:44 -> 01:54:50]  microarchitecture um I believe this is
[01:54:46 -> 01:54:53]  used in the the 20 series
[01:54:50 -> 01:54:54]  cards the 20
[01:54:53 -> 01:54:57]  it's going to be
[01:54:54 -> 01:55:00]  somewhere okay maybe
[01:54:57 -> 01:55:02]  not
[01:55:00 -> 01:55:04]  anyways you can find a bunch of cool
[01:55:02 -> 01:55:06]  statistics on these a love La is the 40
[01:55:04 -> 01:55:09]  series cards you get a bunch of info on
[01:55:06 -> 01:55:11]  that like the the L2 cache it's again
[01:55:09 -> 01:55:14]  bigger instead of 40 megabytes around
[01:55:11 -> 01:55:15]  here it's 96 which is great um L1 cache
[01:55:14 -> 01:55:19]  is actually what matters more so you
[01:55:15 -> 01:55:21]  know like 18 megabytes and and and so
[01:55:19 -> 01:55:24]  forth um then Hopper which is actually
[01:55:21 -> 01:55:27]  what the state-of-the-art gpus right now
[01:55:24 -> 01:55:29]  or close to state-ofthe-art is well the
[01:55:27 -> 01:55:31]  state-ofthe-art is actually the
[01:55:29 -> 01:55:33]  Blackwell uh micro architecture but the
[01:55:31 -> 01:55:35]  hopper is like also very like second
[01:55:33 -> 01:55:38]  most recent one and these are what the
[01:55:35 -> 01:55:41]  h100s are based on these are used to
[01:55:38 -> 01:55:43]  train models like gbt 4 Etc uh so you
[01:55:41 -> 01:55:45]  can you find a bunch of statistics on
[01:55:43 -> 01:55:49]  those without actually going and using
[01:55:45 -> 01:55:52]  one um but if we actually want to uh
[01:55:49 -> 01:55:54]  print some stuff about our GPU I'm just
[01:55:52 -> 01:55:57]  going to open a new terminal tab here um
[01:55:54 -> 01:56:02]  I'm going to drag this to the side if we
[01:55:57 -> 01:56:04]  pop over to um Cuda samples
[01:56:02 -> 01:56:05]  GitHub this is going to print some stuff
[01:56:04 -> 01:56:06]  about your
[01:56:05 -> 01:56:11]  GPU
[01:56:06 -> 01:56:14]  so if we uh take this and we just just
[01:56:11 -> 01:56:16]  get clone
[01:56:14 -> 01:56:17]  it it's going to take a second to do
[01:56:16 -> 01:56:20]  that but we can scroll down in the
[01:56:17 -> 01:56:23]  meantime and we can see uh if you're on
[01:56:20 -> 01:56:25]  Linux you would you would say CD into
[01:56:23 -> 01:56:28]  your whatever directory you desire and
[01:56:25 -> 01:56:30]  then and then make uh to actually make
[01:56:28 -> 01:56:32]  the binaries for running stuff with it
[01:56:30 -> 01:56:34]  so if we print this out here let me
[01:56:32 -> 01:56:37]  actually make
[01:56:34 -> 01:56:39]  this bit
[01:56:37 -> 01:56:44]  bigger
[01:56:39 -> 01:56:48]  um we can CD into Cuda samples and then
[01:56:44 -> 01:56:50]  inside of here we have uh samples so we
[01:56:48 -> 01:56:53]  go into our sample directory as seen
[01:56:50 -> 01:56:55]  there so CD into samples
[01:56:53 -> 01:56:57]  and then we have
[01:56:55 -> 01:56:59]  um we have a bunch of different ones so
[01:56:57 -> 01:57:00]  there's things you can experiment with
[01:56:59 -> 01:57:02]  here I don't know how how easy these are
[01:57:00 -> 01:57:05]  to use I haven't played with them yet
[01:57:02 -> 01:57:06]  but we're going to CD into utilities and
[01:57:05 -> 01:57:09]  notice how we have this device query
[01:57:06 -> 01:57:12]  thing here so this is actually going to
[01:57:09 -> 01:57:17]  turn into a
[01:57:12 -> 01:57:17]  um we can't execute that yet but if we
[01:57:17 -> 01:57:23]  make then we actually can and we can see
[01:57:20 -> 01:57:24]  a bunch of details about our GPU so I
[01:57:23 -> 01:57:28]  recommend you to do this on your own
[01:57:24 -> 01:57:29]  system but uh one cuicable device so
[01:57:28 -> 01:57:33]  there's one GPU plugged into my
[01:57:29 -> 01:57:37]  motherboard that GPU is the GeForce RTX
[01:57:33 -> 01:57:39]  370 the Cuda driver version is 12.5 as
[01:57:37 -> 01:57:41]  well as the runtime version uh Cuda
[01:57:39 -> 01:57:43]  capability so this is actually very
[01:57:41 -> 01:57:46]  important this 8.6 here yours is it
[01:57:43 -> 01:57:49]  might be different um you might have the
[01:57:46 -> 01:57:52]  same GPU you might not but this 8.6 is
[01:57:49 -> 01:57:54]  actually very critical in how we uh we
[01:57:52 -> 01:57:56]  know like what is supported on our GPU
[01:57:54 -> 01:57:59]  so there might be some operations that
[01:57:56 -> 01:58:01]  work and there might be some that aren't
[01:57:59 -> 01:58:03]  so if I just drag this um over to the
[01:58:01 -> 01:58:05]  side here we we don't need to worry
[01:58:03 -> 01:58:07]  about the rest of this as of right now
[01:58:05 -> 01:58:09]  maybe some of maybe some of this later
[01:58:07 -> 01:58:12]  but uh I'll just I'll just give you that
[01:58:09 -> 01:58:16]  information
[01:58:12 -> 01:58:19]  so if we go to the uh
[01:58:16 -> 01:58:22]  Cuda uh
[01:58:19 -> 01:58:25]  capability compute capability
[01:58:22 -> 01:58:28]  [Music]
[01:58:25 -> 01:58:28]  um what's it
[01:58:28 -> 01:58:37]  called sure GeForce products um see 8.6
[01:58:34 -> 01:58:37]  just like
[01:58:38 -> 01:58:42]  that uh I have to go back to the Cuda
[01:58:40 -> 01:58:43]  docs to actually get useful stuff about
[01:58:42 -> 01:58:46]  this uh
[01:58:43 -> 01:58:46]  Cuda Cuda
[01:58:48 -> 01:58:58]  docs so if we go to the
[01:58:53 -> 01:59:03]  uh Cuda C we'll just sure we'll do Cuda
[01:58:58 -> 01:59:09]  C++ um no we're not going to do Cuda
[01:59:03 -> 01:59:13]  C++ we're going to go down to uh Cuda C
[01:59:09 -> 01:59:16]  Cuda search Cuda C programming guide and
[01:59:13 -> 01:59:18]  inside of the cudas C programming guide
[01:59:16 -> 01:59:22]  yes
[01:59:18 -> 01:59:24]  um capability
[01:59:22 -> 01:59:27]  so like this for example this is what
[01:59:24 -> 01:59:31]  I'm looking for so thread block clusters
[01:59:27 -> 01:59:36]  in in uh two and then you go to
[01:59:31 -> 01:59:38]  2.2.1 it's thread block clusters
[01:59:36 -> 01:59:42]  um you only get these if you have a
[01:59:38 -> 01:59:44]  compute capability 9.0 or higher um so
[01:59:42 -> 01:59:46]  the higher the compute capability the
[01:59:44 -> 01:59:48]  better uh
[01:59:46 -> 01:59:50]  so I cannot actually use thread block
[01:59:48 -> 01:59:52]  clusters on mine because the
[01:59:50 -> 01:59:53]  architecture doesn't support it these
[01:59:52 -> 01:59:56]  are critical things you're going to
[01:59:53 -> 01:59:57]  watch out for and you know as you you
[01:59:56 -> 01:59:59]  might actually be able to take advantage
[01:59:57 -> 02:00:01]  of some features that someone else can't
[01:59:59 -> 02:00:04]  like if you have an a100 and someone
[02:00:01 -> 02:00:05]  else has a v00 you can actually do
[02:00:04 -> 02:00:07]  things that they can't and you can do
[02:00:05 -> 02:00:08]  things faster and more efficiently
[02:00:07 -> 02:00:10]  because of things that the architecture
[02:00:08 -> 02:00:13]  actually supports so these are these are
[02:00:10 -> 02:00:16]  things you're going to watch out for um
[02:00:13 -> 02:00:18]  but anyways uh with that being said we
[02:00:16 -> 02:00:21]  can actually jump into uh some stuff
[02:00:18 -> 02:00:23]  about the just essentially how does the
[02:00:21 -> 02:00:25]  Cuda architecture work how does how how
[02:00:23 -> 02:00:27]  do we write code and and how do how does
[02:00:25 -> 02:00:30]  that whole thing fit
[02:00:27 -> 02:00:32]  together so now we can actually get
[02:00:30 -> 02:00:35]  really into what Cuda is doing and the
[02:00:32 -> 02:00:37]  whole hierarchy of that so inside of
[02:00:35 -> 02:00:40]  here I've pulled up chapter 5 writing
[02:00:37 -> 02:00:42]  your own kernels um and then Cuda Basics
[02:00:40 -> 02:00:45]  and then just the readme and the ID
[02:00:42 -> 02:00:49]  exing or indexing file so if you do I
[02:00:45 -> 02:00:52]  believe control shift V on this
[02:00:49 -> 02:00:56]  or control I don't know what keybind it
[02:00:52 -> 02:00:59]  is contrl alt V contrl shift V there we
[02:00:56 -> 02:01:02]  go okay um and then we pull up this one
[02:00:59 -> 02:01:04]  on the side here just uh just for
[02:01:02 -> 02:01:08]  reference let's go through this sort of
[02:01:04 -> 02:01:11]  hand in hand
[02:01:08 -> 02:01:11]  so I'm going to zoom
[02:01:11 -> 02:01:15]  in we just printed this out we just
[02:01:14 -> 02:01:18]  printed out device query so we don't
[02:01:15 -> 02:01:21]  need to really cover that um but when it
[02:01:18 -> 02:01:22]  comes to sort of the more easy stuff to
[02:01:21 -> 02:01:24]  get a grasp on I mean we already went
[02:01:22 -> 02:01:26]  over this so you have the host which is
[02:01:24 -> 02:01:28]  the CPU and uses those RAM sticks on
[02:01:26 -> 02:01:31]  your motherboard and then the device or
[02:01:28 -> 02:01:34]  the GPU uses the onchip vram or video
[02:01:31 -> 02:01:39]  memory um for desktop
[02:01:34 -> 02:01:42]  PCS the the surface level run time
[02:01:39 -> 02:01:46]  typically goes um you C you you define
[02:01:42 -> 02:01:47]  some input on the host or the CPU uh
[02:01:46 -> 02:01:49]  which you then later want to run on the
[02:01:47 -> 02:01:52]  GPU but in first you have to actually
[02:01:49 -> 02:01:55]  Define it on the whole system memory
[02:01:52 -> 02:01:59]  and then you would copy that over to GPU
[02:01:55 -> 02:02:02]  memory um and then you would uh and then
[02:01:59 -> 02:02:04]  you would execute using that on GPU
[02:02:02 -> 02:02:06]  memory you would execute you you would
[02:02:04 -> 02:02:09]  launch a Cuda kernel and that Cuda
[02:02:06 -> 02:02:11]  kernel would use that uh GPU memory and
[02:02:09 -> 02:02:13]  do stuff with it and maybe do do some
[02:02:11 -> 02:02:15]  useful computation and then once that's
[02:02:13 -> 02:02:17]  done uh you would ensure everything is
[02:02:15 -> 02:02:19]  all synchronized up like nothing is
[02:02:17 -> 02:02:21]  nothing is still waiting you would you
[02:02:19 -> 02:02:23]  would synchronize everything and then
[02:02:21 -> 02:02:25]  you would transfer it back to CPU so
[02:02:23 -> 02:02:26]  that or or the host so that you can you
[02:02:25 -> 02:02:28]  know print it out or do something useful
[02:02:26 -> 02:02:30]  with it this is typically how the the
[02:02:28 -> 02:02:34]  runtime goes um and you'll see this in
[02:02:30 -> 02:02:36]  our later chapters um the naming scheme
[02:02:34 -> 02:02:39]  like how we actually um what what we
[02:02:36 -> 02:02:41]  actually name our pieces of data is
[02:02:39 -> 02:02:45]  quite critical so typically what you'll
[02:02:41 -> 02:02:47]  do is you go host or H and then
[02:02:45 -> 02:02:50]  underscore whatever the variable name is
[02:02:47 -> 02:02:52]  so if it's like Matrix a you do hore
[02:02:50 -> 02:02:55]  Matrix a that means it's defined on the
[02:02:52 -> 02:02:59]  host so you're going to do your your
[02:02:55 -> 02:03:01]  Malik your your your C Malik um with
[02:02:59 -> 02:03:03]  this and then you're going to do a Cuda
[02:03:01 -> 02:03:06]  mem copy which we will see later in a
[02:03:03 -> 02:03:07]  second um and that that's where you take
[02:03:06 -> 02:03:10]  this host and you you essentially
[02:03:07 -> 02:03:13]  transfer it over to this other to this
[02:03:10 -> 02:03:17]  other variable uh device so device a
[02:03:13 -> 02:03:19]  that's the GPU uh version of of that so
[02:03:17 -> 02:03:22]  it just exists on two different pieces
[02:03:19 -> 02:03:25]  of memory um and this is just for the
[02:03:22 -> 02:03:28]  set variable name a now we have this
[02:03:25 -> 02:03:31]  Global which you might have seen already
[02:03:28 -> 02:03:33]  um this is visible globally and this is
[02:03:31 -> 02:03:35]  very broad this is this is typically
[02:03:33 -> 02:03:39]  what a kernel is going to look like uh
[02:03:35 -> 02:03:42]  unless you are calling uh say a separate
[02:03:39 -> 02:03:45]  uh calling a separate kernel inside of
[02:03:42 -> 02:03:47]  another one you would use say device um
[02:03:45 -> 02:03:49]  but in this example we'll just stick
[02:03:47 -> 02:03:51]  with global um you know you can read a
[02:03:49 -> 02:03:52]  littleit little bit more into this if
[02:03:51 -> 02:03:54]  you want
[02:03:52 -> 02:03:56]  but we're going to use Global for the
[02:03:54 -> 02:03:57]  most part device we might see that later
[02:03:56 -> 02:04:01]  in the
[02:03:57 -> 02:04:03]  course and then host uh is only going to
[02:04:01 -> 02:04:06]  run on CPU so uh don't don't really
[02:04:03 -> 02:04:07]  worry about that um it's it's kind of
[02:04:06 -> 02:04:10]  just telling Cuda that you're going to
[02:04:07 -> 02:04:11]  run on on the on the CPU but you you may
[02:04:10 -> 02:04:13]  not actually need to because you're just
[02:04:11 -> 02:04:16]  going to use you know like the void
[02:04:13 -> 02:04:19]  instead of instead of global void right
[02:04:16 -> 02:04:23]  um now this this Cuda malic
[02:04:19 -> 02:04:25]  term memory allocation on the GP vram so
[02:04:23 -> 02:04:29]  that's the global memory on the GPU
[02:04:25 -> 02:04:32]  itself so in this example you do you
[02:04:29 -> 02:04:35]  know Define a bunch of uh a bunch of uh
[02:04:32 -> 02:04:42]  essenti essentially arrays so a pointer
[02:04:35 -> 02:04:44]  to a device uh a float array which is a
[02:04:42 -> 02:04:46]  pointer on the
[02:04:44 -> 02:04:50]  device uh and it's for a and then the
[02:04:46 -> 02:04:53]  same thing for B and C now you do kudam
[02:04:50 -> 02:04:55]  Malik meaning you allocate it on the GPU
[02:04:53 -> 02:04:57]  so you do the memory address for that so
[02:04:55 -> 02:04:59]  you put in the memory address for for
[02:04:57 -> 02:05:02]  this thing
[02:04:59 -> 02:05:03]  um and then you go essentially whatever
[02:05:02 -> 02:05:06]  the you know let's just say you have a
[02:05:03 -> 02:05:08]  size defined above right uh like it's
[02:05:06 -> 02:05:11]  maybe it's like a matrix for example and
[02:05:08 -> 02:05:14]  it's size it's like a you know Square
[02:05:11 -> 02:05:15]  Matrix size n byn and all you're going
[02:05:14 -> 02:05:18]  to do is say you know we want to
[02:05:15 -> 02:05:20]  allocate this much memory uh this memory
[02:05:18 -> 02:05:23]  address and that's just going to be a
[02:05:20 -> 02:05:26]  square Matrix of let's say you know 128
[02:05:23 -> 02:05:30]  * 128 time the size of whatever a float
[02:05:26 -> 02:05:33]  is so in that case I think it'll be four
[02:05:30 -> 02:05:35]  because a float is uh four bytes where
[02:05:33 -> 02:05:37]  each B is eight bits you have a floating
[02:05:35 -> 02:05:40]  Point 32 number do the math and then you
[02:05:37 -> 02:05:43]  end up with the total amount of uh bytes
[02:05:40 -> 02:05:45]  that you will need to allocate for uh
[02:05:43 -> 02:05:49]  this device uh a matrix and so on
[02:05:45 -> 02:05:52]  through b and c as well
[02:05:49 -> 02:05:54]  um now Cuda M Copy can copy from both
[02:05:52 -> 02:05:59]  device to host host to device or device
[02:05:54 -> 02:06:01]  to device for edge cases so um you know
[02:05:59 -> 02:06:05]  you would you would slide a little term
[02:06:01 -> 02:06:08]  in here camem copy host to device or CM
[02:06:05 -> 02:06:10]  copy device to host um and that's that's
[02:06:08 -> 02:06:11]  how that would go um we're actually
[02:06:10 -> 02:06:15]  going to see usage of this in a second
[02:06:11 -> 02:06:17]  here but um understanding camm copy is
[02:06:15 -> 02:06:19]  just going to just going to copy things
[02:06:17 -> 02:06:20]  around and then Cuda free is obviously
[02:06:19 -> 02:06:22]  just going to free memory up on the
[02:06:20 -> 02:06:23]  device so when you you're done with
[02:06:22 -> 02:06:26]  something or you don't need it anymore
[02:06:23 -> 02:06:28]  just you know free that up if it's a big
[02:06:26 -> 02:06:30]  if it's a big uh you know if it's just
[02:06:28 -> 02:06:33]  like an integer or whatever like just a
[02:06:30 -> 02:06:34]  float um like a float a equals 1 or
[02:06:33 -> 02:06:36]  something it's like you don't need to
[02:06:34 -> 02:06:37]  free that um but if it's a big array
[02:06:36 -> 02:06:41]  like this you're going to need to free
[02:06:37 -> 02:06:44]  that um
[02:06:41 -> 02:06:46]  now the nvcc compiler is something we'll
[02:06:44 -> 02:06:49]  dig into maybe a little more in the
[02:06:46 -> 02:06:53]  future um but this is all you really
[02:06:49 -> 02:06:58]  need to know so the host code is uh
[02:06:53 -> 02:07:01]  essentially the these nvcc will compile
[02:06:58 -> 02:07:04]  all of this down into uh something that
[02:07:01 -> 02:07:08]  the GPU can actually execute but the CPU
[02:07:04 -> 02:07:09]  is going to run it so uh CPU is going to
[02:07:08 -> 02:07:12]  interpret what that is saying and it's
[02:07:09 -> 02:07:14]  going to launch things and and tell the
[02:07:12 -> 02:07:18]  GPU to do things it's not just going to
[02:07:14 -> 02:07:20]  compile directly down to GPU right so uh
[02:07:18 -> 02:07:22]  when it needs to run on GPU when when it
[02:07:20 -> 02:07:24]  actually needs those instructions as to
[02:07:22 -> 02:07:26]  what to do it's going to get compiled
[02:07:24 -> 02:07:28]  down to PTX which is parallel thread
[02:07:26 -> 02:07:32]  execution instructions so that's like
[02:07:28 -> 02:07:36]  the GPU equivalent of x86 or assembly uh
[02:07:32 -> 02:07:38]  you know as it is for um CPU or host um
[02:07:36 -> 02:07:40]  and and then it's further going to
[02:07:38 -> 02:07:43]  compile that down to Shader assembly
[02:07:40 -> 02:07:44]  which we're not going to worry about um
[02:07:43 -> 02:07:47]  and this is just and this is stable
[02:07:44 -> 02:07:49]  across all of the different Nvidia gpus
[02:07:47 -> 02:07:51]  so you don't need to worry about that um
[02:07:49 -> 02:07:52]  and then just in time is just a type of
[02:07:51 -> 02:07:55]  compil
[02:07:52 -> 02:07:59]  so
[02:07:55 -> 02:08:01]  um Cuda hierarchy yes this is this is
[02:07:59 -> 02:08:03]  where things start to get a little bit
[02:08:01 -> 02:08:05]  intuitive
[02:08:03 -> 02:08:09]  so imagine you have like a imagine you
[02:08:05 -> 02:08:12]  have this giant 3D like a cubic volume
[02:08:09 -> 02:08:14]  uh and and this volume is called a grid
[02:08:12 -> 02:08:16]  right inside of this grid you're going
[02:08:14 -> 02:08:19]  to have a bunch of these smaller Cube
[02:08:16 -> 02:08:22]  cubic volumes those are called blocks uh
[02:08:19 -> 02:08:24]  and those those blocks are organized
[02:08:22 -> 02:08:27]  uh you know you can make them whatever
[02:08:24 -> 02:08:29]  size you want it's just like essentially
[02:08:27 -> 02:08:31]  uh can think of it like a like a like a
[02:08:29 -> 02:08:32]  prism or something uh and you have a
[02:08:31 -> 02:08:34]  bunch of these organized in this giant
[02:08:32 -> 02:08:36]  3D Volume which is the grid and those
[02:08:34 -> 02:08:38]  individual blocks have things inside of
[02:08:36 -> 02:08:40]  them called threads and those threads
[02:08:38 -> 02:08:42]  are going to do your math operations for
[02:08:40 -> 02:08:46]  you so there's there's a lot to unpack
[02:08:42 -> 02:08:48]  here but the the individual threads um
[02:08:46 -> 02:08:49]  can communicate inside of these blocks
[02:08:48 -> 02:08:52]  and that's an important part to remember
[02:08:49 -> 02:08:53]  for later when we're optimizing stuff
[02:08:52 -> 02:08:55]  but essentially the reason why we have
[02:08:53 -> 02:08:58]  all these different pieces inside of
[02:08:55 -> 02:09:01]  this massive grid is so that we can get
[02:08:58 -> 02:09:03]  the parallelism of gpus so when you have
[02:09:01 -> 02:09:04]  this block doing this you know doing
[02:09:03 -> 02:09:05]  this piece of the puzzle and then this
[02:09:04 -> 02:09:08]  doing another piece of the puzzle and
[02:09:05 -> 02:09:09]  they all kind of do their part and at
[02:09:08 -> 02:09:10]  the end if they all do their part
[02:09:09 -> 02:09:12]  successfully and they're all like
[02:09:10 -> 02:09:15]  synchronized and you make sure that
[02:09:12 -> 02:09:17]  everything works correctly um you know
[02:09:15 -> 02:09:20]  it's it's better than having a single
[02:09:17 -> 02:09:21]  CPU thread going through each individual
[02:09:20 -> 02:09:24]  thing in that problem and doing it one
[02:09:21 -> 02:09:25]  by one oras you have like a bunch of
[02:09:24 -> 02:09:28]  these blocks or these threads inside of
[02:09:25 -> 02:09:30]  blocks which are uh doing you know
[02:09:28 -> 02:09:32]  little independent operations it's doing
[02:09:30 -> 02:09:35]  a smaller number of operations and they
[02:09:32 -> 02:09:37]  have a lower clock speed uh but they're
[02:09:35 -> 02:09:38]  it's going to solve the problem much
[02:09:37 -> 02:09:41]  quicker because it's in
[02:09:38 -> 02:09:43]  parallel so that that's the whole idea
[02:09:41 -> 02:09:45]  here is you have this this 3D Volume
[02:09:43 -> 02:09:48]  called a grid inside of it you have
[02:09:45 -> 02:09:50]  these other uh you have these other 3D
[02:09:48 -> 02:09:52]  sort of Cubes or rectangles whatever um
[02:09:50 -> 02:09:54]  and inside of those you have threads and
[02:09:52 -> 02:09:59]  those threads are going to also do
[02:09:54 -> 02:10:02]  things so I need to breathe but we'll go
[02:09:59 -> 02:10:04]  into uh some more technical terms in a
[02:10:02 -> 02:10:07]  second here so going to these technical
[02:10:04 -> 02:10:09]  terms we can see this grid dim exists
[02:10:07 -> 02:10:13]  here in our kernel in our Global uh
[02:10:09 -> 02:10:17]  kernel we have a block idx which is you
[02:10:13 -> 02:10:22]  know these these three um
[02:10:17 -> 02:10:24]  and block dim exists uh here and here
[02:10:22 -> 02:10:28]  and here and here and here right you
[02:10:24 -> 02:10:31]  have all these uh and then thread idx so
[02:10:28 -> 02:10:35]  this grid dim is the number of blocks in
[02:10:31 -> 02:10:39]  the Grid at you know say like grid dim
[02:10:35 -> 02:10:43]  dot X is going to be uh in this in this
[02:10:39 -> 02:10:44]  volumetric uh grid what is the X
[02:10:43 -> 02:10:46]  dimension of that so like what is the
[02:10:44 -> 02:10:48]  length and then grid dim doy is going to
[02:10:46 -> 02:10:50]  be like what is the height and then
[02:10:48 -> 02:10:53]  maybe grid dim. Z is going to be the
[02:10:50 -> 02:10:56]  depth of that right um and then the
[02:10:53 -> 02:10:59]  block idx is going to it's not actually
[02:10:56 -> 02:11:02]  uh about the block itself but like where
[02:10:59 -> 02:11:06]  is it where is the block within the grid
[02:11:02 -> 02:11:08]  so the the grid dim is like how long is
[02:11:06 -> 02:11:10]  it how what is the what is like the size
[02:11:08 -> 02:11:14]  of the grid itself that is run on the
[02:11:10 -> 02:11:16]  GPU and then the block idx is where uh
[02:11:14 -> 02:11:19]  each individual block is so a block will
[02:11:16 -> 02:11:22]  have a block idx in both the uh maybe x
[02:11:19 -> 02:11:24]  y and Zed dimension and that'll be
[02:11:22 -> 02:11:28]  essentially its coordinates within that
[02:11:24 -> 02:11:32]  grid uh and then the block dim is how
[02:11:28 -> 02:11:34]  big that block is so grid fits a bunch
[02:11:32 -> 02:11:36]  of blocks into it a block fits a bunch
[02:11:34 -> 02:11:39]  of threads into it so the block dim is
[02:11:36 -> 02:11:42]  like how how big is this like smaller
[02:11:39 -> 02:11:45]  Cube or this or this rectangular prism
[02:11:42 -> 02:11:47]  um and then the thread idx is like which
[02:11:45 -> 02:11:48]  which thread is it within that block so
[02:11:47 -> 02:11:51]  you can see how this like spatial
[02:11:48 -> 02:11:52]  hierarchy goes down you have this 3D in
[02:11:51 -> 02:11:54]  the grid and then this 3D in the block
[02:11:52 -> 02:11:57]  so it's like kind of 6D if you think
[02:11:54 -> 02:11:58]  about it that way um I don't want to I
[02:11:57 -> 02:11:59]  don't want that to be intimidating
[02:11:58 -> 02:12:00]  though like six dimensional I don't want
[02:11:59 -> 02:12:03]  that to be intimidating it's just kind
[02:12:00 -> 02:12:05]  of how the it's it's it's an efficient
[02:12:03 -> 02:12:06]  way of of running things in parallel and
[02:12:05 -> 02:12:09]  a way of visualizing it as like a
[02:12:06 -> 02:12:12]  software abstraction right uh that's
[02:12:09 -> 02:12:14]  that's the idea there
[02:12:12 -> 02:12:17]  um and
[02:12:14 -> 02:12:19]  then threads like I mean I I I assume
[02:12:17 -> 02:12:21]  that this kind of this this spatial idea
[02:12:19 -> 02:12:23]  sort of makes sense now so now we can go
[02:12:21 -> 02:12:26]  into like why this works um and sort of
[02:12:23 -> 02:12:29]  like the more nitty-gritty of that so
[02:12:26 -> 02:12:32]  each thread um itself has local memory
[02:12:29 -> 02:12:34]  on it so registers which are very fast
[02:12:32 -> 02:12:38]  and is private to that individual thread
[02:12:34 -> 02:12:40]  so for example if you wanted to add um A
[02:12:38 -> 02:12:43]  and B where it's like 1 2 3 and then all
[02:12:40 -> 02:12:45]  the way up to n which is like the length
[02:12:43 -> 02:12:46]  and then 2 4 six so like counting by
[02:12:45 -> 02:12:49]  ones and then count B is counting by
[02:12:46 -> 02:12:53]  twos um each thread would do a single
[02:12:49 -> 02:12:57]  add so like thread at index say zero
[02:12:53 -> 02:13:00]  would be like a at the at the thread
[02:12:57 -> 02:13:03]  Index right and so the thread index
[02:13:00 -> 02:13:05]  itself tells you how to index into data
[02:13:03 -> 02:13:08]  and then you can use that element that
[02:13:05 -> 02:13:10]  it gets like from its own index from the
[02:13:08 -> 02:13:12]  thread index in that whole space and you
[02:13:10 -> 02:13:14]  can actually do operations with that so
[02:13:12 -> 02:13:18]  it's like a little hack of uh
[02:13:14 -> 02:13:20]  essentially both both getting the right
[02:13:18 -> 02:13:22]  elements of data uh and adding and doing
[02:13:20 -> 02:13:27]  math operations on them at the same time
[02:13:22 -> 02:13:30]  so we end up doing you know uh 1 + 2
[02:13:27 -> 02:13:33]  right with a single thread and it's in
[02:13:30 -> 02:13:36]  and it's accessing this index uh that
[02:13:33 -> 02:13:37]  the index the data based on its based on
[02:13:36 -> 02:13:39]  the thread's index and it's adding them
[02:13:37 -> 02:13:43]  together uh and then same thing for you
[02:13:39 -> 02:13:46]  know maybe thread thread two right um so
[02:13:43 -> 02:13:47]  that's just like kind of how the whole
[02:13:46 -> 02:13:49]  that that's how the whole like uh
[02:13:47 -> 02:13:52]  indexing thing pans out that's why it's
[02:13:49 -> 02:13:55]  so cool um and then warps and it's kind
[02:13:52 -> 02:13:57]  of interesting if you if you look at
[02:13:55 -> 02:14:01]  this Wikipedia article it's
[02:13:57 -> 02:14:04]  like warps warps and weft right so you
[02:14:01 -> 02:14:08]  have um you have these warps that are
[02:14:04 -> 02:14:08]  going through so like these these
[02:14:08 -> 02:14:13]  uh these these warps that are going
[02:14:11 -> 02:14:16]  through it like up and down and then the
[02:14:13 -> 02:14:17]  weft is like uh what the the warps are
[02:14:16 -> 02:14:21]  weaving through so they're like
[02:14:17 -> 02:14:25]  interlocked like this and so uh you you
[02:14:21 -> 02:14:29]  you could sort of think of of the uh
[02:14:25 -> 02:14:30]  warps as the uh as like what is what is
[02:14:29 -> 02:14:35]  going forward so you could say like a
[02:14:30 -> 02:14:39]  warp is a group of threads
[02:14:35 -> 02:14:43]  um warp and weft uh the vertical warp
[02:14:39 -> 02:14:46]  Yarns uh plural are held in stationary
[02:14:43 -> 02:14:48]  um and the horizontal we is drawn
[02:14:46 -> 02:14:50]  through them so you essentially have all
[02:14:48 -> 02:14:52]  these War like a bunch of threads
[02:14:50 -> 02:14:54]  essentially it's a bunch of threads that
[02:14:52 -> 02:14:56]  are that are going in and out and and
[02:14:54 -> 02:14:58]  you can think of these threads as like
[02:14:56 -> 02:14:59]  doing their own math operations and you
[02:14:58 -> 02:15:01]  have a bunch of them grouped together in
[02:14:59 -> 02:15:03]  a warp right that's that's the whole
[02:15:01 -> 02:15:06]  idea there
[02:15:03 -> 02:15:09]  um so you know like I said in
[02:15:06 -> 02:15:10]  the in the in the Wikipedia article warp
[02:15:09 -> 02:15:14]  is a set of
[02:15:10 -> 02:15:15]  Yarns um set of Yarns or other things
[02:15:14 -> 02:15:17]  stretch in place on a loom where the
[02:15:15 -> 02:15:19]  weft is introduced during the weaving
[02:15:17 -> 02:15:21]  process is regarded as the longitudinal
[02:15:19 -> 02:15:23]  set in a in a finished fabric with two
[02:15:21 -> 02:15:25]  or more sets of elements I I don't
[02:15:23 -> 02:15:26]  expect you to understand that it's just
[02:15:25 -> 02:15:29]  like the idea of threads are grouped
[02:15:26 -> 02:15:31]  into warps um and then the the whift
[02:15:29 -> 02:15:33]  kind of like it's that other uh
[02:15:31 -> 02:15:38]  perpendicular
[02:15:33 -> 02:15:40]  part um so warps are inside of blocks so
[02:15:38 -> 02:15:43]  remember we have the like the the grids
[02:15:40 -> 02:15:47]  and then blocks and then threads um
[02:15:43 -> 02:15:49]  inside of blocks you have warps which
[02:15:47 -> 02:15:51]  take care of threads so the blocks
[02:15:49 -> 02:15:54]  themselves aren't entirely handling the
[02:15:51 -> 02:15:56]  threads it's actually the warps that are
[02:15:54 -> 02:15:59]  doing a lot of that work
[02:15:56 -> 02:16:02]  so you typically organize a warp as like
[02:15:59 -> 02:16:05]  a group of threads like uh I believe the
[02:16:02 -> 02:16:07]  maximum is 32 threads so a warp will
[02:16:05 -> 02:16:09]  handle 32 threads at once within a
[02:16:07 -> 02:16:11]  block
[02:16:09 -> 02:16:14]  um there is no way of getting around
[02:16:11 -> 02:16:17]  using warps the warps scheduler makes
[02:16:14 -> 02:16:20]  the warps run so uh you could think of
[02:16:17 -> 02:16:21]  like maybe the warp scheduler as the as
[02:16:20 -> 02:16:25]  like the weft
[02:16:21 -> 02:16:26]  right so um it's it's sort of like going
[02:16:25 -> 02:16:28]  through and making sure they don't get
[02:16:26 -> 02:16:29]  like disentangled and all this and and
[02:16:28 -> 02:16:31]  ensuring that everything like works out
[02:16:29 -> 02:16:34]  properly you can use whatever analogy
[02:16:31 -> 02:16:36]  Works um but the the warp scheduler
[02:16:34 -> 02:16:39]  ensures that the warps which are group
[02:16:36 -> 02:16:41]  of threads run um and then you would
[02:16:39 -> 02:16:44]  have typically four warp schedulers per
[02:16:41 -> 02:16:47]  SM and SM is like the smaller like the
[02:16:44 -> 02:16:48]  the streaming multiprocessor on chip
[02:16:47 -> 02:16:51]  that's what those are and you can have
[02:16:48 -> 02:16:55]  four rep schedulers per SM so
[02:16:51 -> 02:16:59]  um you know do the math that's 128
[02:16:55 -> 02:16:59]  threads per SM
[02:17:01 -> 02:17:07]  um then we have blocks so blocks are
[02:17:05 -> 02:17:09]  interesting each block has shared memory
[02:17:07 -> 02:17:11]  um visible to all threads within that
[02:17:09 -> 02:17:12]  thread block so all of these like you
[02:17:11 -> 02:17:16]  know thread one can see the same stuff
[02:17:12 -> 02:17:19]  that thread 32 can uh they can or or
[02:17:16 -> 02:17:21]  even thread like I don't know like 500
[02:17:19 -> 02:17:24]  for that matter um
[02:17:21 -> 02:17:26]  um they can all see the same data so
[02:17:24 -> 02:17:28]  like within a warp they can they can
[02:17:26 -> 02:17:30]  kind of see their they can communicate
[02:17:28 -> 02:17:33]  faster but within a block they can still
[02:17:30 -> 02:17:36]  communicate very fast uh through this
[02:17:33 -> 02:17:37]  shared memory which we call the L1 cache
[02:17:36 -> 02:17:39]  and I'll dig into that in a second here
[02:17:37 -> 02:17:43]  when we go into the next section but the
[02:17:39 -> 02:17:49]  L1 cache is very important for Speed and
[02:17:43 -> 02:17:49]  optimizing uh kernels so uh
[02:17:52 -> 02:17:55]  yeah essentially just the same thing
[02:17:53 -> 02:18:00]  what I said uh shared memory shared
[02:17:55 -> 02:18:02]  memory is is more efficient um it's
[02:18:00 -> 02:18:04]  faster I think the maximum memory
[02:18:02 -> 02:18:07]  bandwidth you get with shared memory is
[02:18:04 -> 02:18:10]  like on the order of uh like 15
[02:18:07 -> 02:18:13]  terabytes per second and then uh Global
[02:18:10 -> 02:18:15]  vram so like when I do Nvidia SMI uh
[02:18:13 -> 02:18:19]  like
[02:18:15 -> 02:18:22]  this you can see that I get
[02:18:19 -> 02:18:25]  um like this this this is my this this
[02:18:22 -> 02:18:28]  is actually going at like maybe 6 or 700
[02:18:25 -> 02:18:30]  gabes a second Shar shared memory is
[02:18:28 -> 02:18:35]  like 15 terabytes so it's like really
[02:18:30 -> 02:18:38]  fast um and and blocks use uh shared
[02:18:35 -> 02:18:41]  memory which is you know uh on an
[02:18:38 -> 02:18:45]  individual SM so the SM will handle that
[02:18:41 -> 02:18:47]  um and then grids um during the chronal
[02:18:45 -> 02:18:50]  execution the threads within the blocks
[02:18:47 -> 02:18:51]  within the grid can access Global memory
[02:18:50 -> 02:18:53]  um so that's just like universally
[02:18:51 -> 02:18:55]  applied
[02:18:53 -> 02:18:57]  um you can you can make things you know
[02:18:55 -> 02:18:59]  more advanced if you want to use threads
[02:18:57 -> 02:19:02]  but it's going to default using the GPU
[02:18:59 -> 02:19:07]  V Ram that 8,192 megabytes that you just
[02:19:02 -> 02:19:11]  saw um it's going to contain a bunch of
[02:19:07 -> 02:19:13]  blocks um and the whole idea here is
[02:19:11 -> 02:19:15]  that with with grids and blocks and
[02:19:13 -> 02:19:16]  threads is that you you just have to
[02:19:15 -> 02:19:18]  worry like conceptually what is it doing
[02:19:16 -> 02:19:20]  you don't have to worry about how things
[02:19:18 -> 02:19:22]  are handled on the hardware because this
[02:19:20 -> 02:19:24]  whole Cuda this whole Cuda hierarchy is
[02:19:22 -> 02:19:26]  a software abstraction right so the so
[02:19:24 -> 02:19:28]  the hardware doesn't actually look like
[02:19:26 -> 02:19:30]  grids and blocks and threads like it
[02:19:28 -> 02:19:31]  doesn't objectively look like that it
[02:19:30 -> 02:19:34]  looks different and is compiled down to
[02:19:31 -> 02:19:37]  Shader assembly which doesn't actually
[02:19:34 -> 02:19:40]  look close to
[02:19:37 -> 02:19:41]  uh close to what this what this is right
[02:19:40 -> 02:19:43]  now and that is actually run on the
[02:19:41 -> 02:19:45]  hardware right so there's there's
[02:19:43 -> 02:19:47]  various different levels here that it's
[02:19:45 -> 02:19:49]  hard to sort of navigate through but
[02:19:47 -> 02:19:51]  this is a lot of this is kind of why I'm
[02:19:49 -> 02:19:54]  showing you this stuff if to give you a
[02:19:51 -> 02:19:59]  better grasp on that
[02:19:54 -> 02:20:01]  um so let's dig into what this uh ID
[02:19:59 -> 02:20:05]  exing script is actually doing now going
[02:20:01 -> 02:20:07]  into the actual Cuda indexing scheme
[02:20:05 -> 02:20:10]  like we saw with threads uh except we're
[02:20:07 -> 02:20:13]  going on the level of grids blocks uh
[02:20:10 -> 02:20:14]  and threads so everything uh this this
[02:20:13 -> 02:20:16]  script in particular is designed to
[02:20:14 -> 02:20:20]  print things uh useful things out for us
[02:20:16 -> 02:20:23]  so as we can see uh ID you know all
[02:20:20 -> 02:20:26]  these different block idx um all of
[02:20:23 -> 02:20:29]  these and uh oh I'm going to go into
[02:20:26 -> 02:20:32]  these in a second here but essentially
[02:20:29 -> 02:20:34]  like you you have the um if we go down
[02:20:32 -> 02:20:37]  um we we have we have all these terms
[02:20:34 -> 02:20:39]  that we Define right
[02:20:37 -> 02:20:45]  so
[02:20:39 -> 02:20:48]  uh block X block Y block Zed right so B
[02:20:45 -> 02:20:50]  and then T is for Threads so what I
[02:20:48 -> 02:20:53]  particularly mean here is uh this is the
[02:20:50 -> 02:20:57]  this is the block dim so inside of the
[02:20:53 -> 02:20:59]  grid you're going to have uh X like the
[02:20:57 -> 02:21:01]  length of the X Dimension is going to be
[02:20:59 -> 02:21:05]  two and then the height Dimension is
[02:21:01 -> 02:21:06]  three and the depth Dimension Z is four
[02:21:05 -> 02:21:08]  right so you're going to have this grid
[02:21:06 -> 02:21:11]  uh volume and it's going to be of that
[02:21:08 -> 02:21:14]  shape and then in in each individual
[02:21:11 -> 02:21:17]  block inside of that inside of that grid
[02:21:14 -> 02:21:20]  um you're going to have these thread
[02:21:17 -> 02:21:22]  dims uh which this is essentially the
[02:21:20 -> 02:21:24]  block Dimension this is the um this is
[02:21:22 -> 02:21:28]  the grid Dimension and this is the block
[02:21:24 -> 02:21:29]  Dimension um so the you're inside of
[02:21:28 -> 02:21:32]  each block you're going to have
[02:21:29 -> 02:21:34]  essentially four long four high and four
[02:21:32 -> 02:21:37]  deep right so it's going to be this this
[02:21:34 -> 02:21:41]  perfect Cube essentially um and
[02:21:37 -> 02:21:43]  so we go down we calc we can calculate
[02:21:41 -> 02:21:46]  the total number of blocks per grid so
[02:21:43 -> 02:21:48]  just essentially base time width time
[02:21:46 -> 02:21:49]  height your classical formula and same
[02:21:48 -> 02:21:51]  for Threads per block and we can get the
[02:21:49 -> 02:21:52]  total number
[02:21:51 -> 02:21:54]  in each of these and then we can go ah
[02:21:52 -> 02:21:58]  and print them out right so blocks per
[02:21:54 -> 02:22:00]  grid threads per block um and then total
[02:21:58 -> 02:22:02]  number of threads so we have a certain
[02:22:00 -> 02:22:04]  number of threads per block and then if
[02:22:02 -> 02:22:05]  we times that by the number of blocks we
[02:22:04 -> 02:22:07]  get the total number of
[02:22:05 -> 02:22:10]  threads now we have this other type down
[02:22:07 -> 02:22:13]  here called dim 3 which is specific to
[02:22:10 -> 02:22:15]  Cuda but this is essentially just the
[02:22:13 -> 02:22:18]  same thing as we saw before so blocks
[02:22:15 -> 02:22:21]  per grid so we have this these um these
[02:22:18 -> 02:22:23]  these grid Dimensions X x y and Zed and
[02:22:21 -> 02:22:26]  then same thing for uh threads within a
[02:22:23 -> 02:22:29]  block so the block the block Dimensions
[02:22:26 -> 02:22:31]  um meaning x y and Zed and so we plug
[02:22:29 -> 02:22:34]  these into our kernel which is called
[02:22:31 -> 02:22:37]  who am I we have this Global uh we have
[02:22:34 -> 02:22:40]  this Global header
[02:22:37 -> 02:22:42]  and we do these three um we do these
[02:22:40 -> 02:22:43]  these three symbols I can't remember
[02:22:42 -> 02:22:45]  what these are called it's like the less
[02:22:43 -> 02:22:48]  than or greater than symbol uh and then
[02:22:45 -> 02:22:51]  you put the uh total number of blocks
[02:22:48 -> 02:22:53]  per grid um or the grid Dimensions as
[02:22:51 -> 02:22:54]  the first parameter and then the threads
[02:22:53 -> 02:22:56]  per block as the second and then there's
[02:22:54 -> 02:22:58]  some other ones which you can do after
[02:22:56 -> 02:22:59]  and you'll see those in a second but
[02:22:58 -> 02:23:01]  these are all all you have to worry
[02:22:59 -> 02:23:02]  about right now so the grid dimensions
[02:23:01 -> 02:23:04]  and then the block
[02:23:02 -> 02:23:06]  Dimensions uh and then we c a device
[02:23:04 -> 02:23:07]  synchronized to ensure that everything
[02:23:06 -> 02:23:09]  is caught up and we can continue with
[02:23:07 -> 02:23:12]  whatever else we need to do um that this
[02:23:09 -> 02:23:14]  that would be used like practically um
[02:23:12 -> 02:23:15]  so now when we actually go up to here
[02:23:14 -> 02:23:17]  this is where things get a little bit
[02:23:15 -> 02:23:18]  spatially intuitive okay I'm not going
[02:23:17 -> 02:23:20]  to lie this part might be like one of
[02:23:18 -> 02:23:24]  the hardest to grasp but I'm going to
[02:23:20 -> 02:23:28]  try my best to explain so this block ID
[02:23:24 -> 02:23:30]  what is this well this is essentially
[02:23:28 -> 02:23:33]  you you can think of uh a bunch of
[02:23:30 -> 02:23:35]  apartments in an apartment
[02:23:33 -> 02:23:38]  complex um a bunch of floors within each
[02:23:35 -> 02:23:39]  apartment and then a number like a room
[02:23:38 -> 02:23:42]  on that floor right and so we're trying
[02:23:39 -> 02:23:45]  to find where we are within that apart
[02:23:42 -> 02:23:46]  apartment complex right so you can think
[02:23:45 -> 02:23:49]  of it as
[02:23:46 -> 02:23:51]  um you know your apartment is like a
[02:23:49 -> 02:23:56]  like a paint right it's like a singular
[02:23:51 -> 02:24:00]  pain in this in this volume um and so
[02:23:56 -> 02:24:03]  you in this one you uh in in the
[02:24:00 -> 02:24:08]  apartment complex you essentially do
[02:24:03 -> 02:24:09]  um grid dim dox so this this length part
[02:24:08 -> 02:24:12]  and then the Y
[02:24:09 -> 02:24:16]  component uh and then times whatever Z
[02:24:12 -> 02:24:18]  is so the block idx doz is wherever the
[02:24:16 -> 02:24:20]  block position is it's not any it's not
[02:24:18 -> 02:24:22]  like how big how big something is it's
[02:24:20 -> 02:24:24]  ual index or the position so you have
[02:24:22 -> 02:24:27]  this pain which is x * Y and then a
[02:24:24 -> 02:24:29]  depth which is z so it's like however
[02:24:27 -> 02:24:30]  big the pain is and then go that deep so
[02:24:29 -> 02:24:32]  it's like these panes that are like
[02:24:30 -> 02:24:34]  layered on top going depthwise right
[02:24:32 -> 02:24:37]  it's going deep and so you have these
[02:24:34 -> 02:24:39]  panes that are like going that way um
[02:24:37 -> 02:24:44]  and then you have
[02:24:39 -> 02:24:46]  the uh block idx doy time grid dim dox
[02:24:44 -> 02:24:49]  so you can think of this as uh like the
[02:24:46 -> 02:24:51]  the grid di is is like this like a floor
[02:24:49 -> 02:24:56]  right a floor within that apartment uh
[02:24:51 -> 02:24:58]  building and the block ID x.y is like
[02:24:56 -> 02:25:00]  which Which floor is it so you have to
[02:24:58 -> 02:25:03]  go up that number of floors to get there
[02:25:00 -> 02:25:05]  um so it's like you essentially start
[02:25:03 -> 02:25:06]  from the bottom you go like this many
[02:25:05 -> 02:25:08]  and then this many and then this many
[02:25:06 -> 02:25:09]  it's like each of those is like a bunch
[02:25:08 -> 02:25:11]  of rooms that you go through to get to
[02:25:09 -> 02:25:13]  the next floor right you eventually wrap
[02:25:11 -> 02:25:16]  up to this one and then you uh and then
[02:25:13 -> 02:25:19]  once you get to the actual X position
[02:25:16 -> 02:25:20]  which is like the x is the length you
[02:25:19 -> 02:25:22]  actually stop there so it's it's like
[02:25:20 -> 02:25:26]  you've went through like a number of
[02:25:22 -> 02:25:28]  pains like paines deep um rows High
[02:25:26 -> 02:25:30]  which is the number of floors um and
[02:25:28 -> 02:25:31]  then you end up with like this this
[02:25:30 -> 02:25:33]  final part which is like okay well what
[02:25:31 -> 02:25:35]  is the offset at this floor which I
[02:25:33 -> 02:25:38]  which my apartment is and it's like like
[02:25:35 -> 02:25:41]  right here it's like depth and then goes
[02:25:38 -> 02:25:43]  up depth goes up number of floors and
[02:25:41 -> 02:25:45]  then like this is where I am and that's
[02:25:43 -> 02:25:49]  how you uh that's how you find your
[02:25:45 -> 02:25:50]  block ID um like which apartment complex
[02:25:49 -> 02:25:52]  or which apartment building are you in
[02:25:50 -> 02:25:54]  with that that entire city or the
[02:25:52 -> 02:25:56]  empowerment complex right in a 3D
[02:25:54 -> 02:25:59]  scenario this block offset is
[02:25:56 -> 02:26:01]  essentially the number of you you take
[02:25:59 -> 02:26:03]  the total threads per block so the block
[02:26:01 -> 02:26:05]  Dimensions how many threads is in each
[02:26:03 -> 02:26:07]  you know this this many threads in X
[02:26:05 -> 02:26:09]  this many threads and Y and as many
[02:26:07 -> 02:26:10]  threads Z you you you multiply all those
[02:26:09 -> 02:26:15]  together you get the total threads per
[02:26:10 -> 02:26:17]  block or say people per apartment um and
[02:26:15 -> 02:26:20]  then times our apartment number so it's
[02:26:17 -> 02:26:22]  like uh the total number of threads up
[02:26:20 -> 02:26:26]  to your like essentially which which
[02:26:22 -> 02:26:29]  thread index um does your like how many
[02:26:26 -> 02:26:32]  threads are before your UH apartment how
[02:26:29 -> 02:26:35]  many people uh are before your apartment
[02:26:32 -> 02:26:36]  uh your apartment number that that's
[02:26:35 -> 02:26:38]  what we're saying here so we calculated
[02:26:36 -> 02:26:40]  this block ID from before and then we we
[02:26:38 -> 02:26:42]  just find like that but on the level of
[02:26:40 -> 02:26:46]  threads instead and that's the block
[02:26:42 -> 02:26:47]  offset for um calculating you know which
[02:26:46 -> 02:26:49]  thread we're at and then we can continue
[02:26:47 -> 02:26:51]  that and use the same analogy that we
[02:26:49 -> 02:26:54]  used in block ID except for thread
[02:26:51 -> 02:26:56]  offset so you know it's like thread ID
[02:26:54 -> 02:26:58]  x.x block ID x.x it's like these are
[02:26:56 -> 02:27:00]  just like mapped essentially except it's
[02:26:58 -> 02:27:03]  like a lower level in the hierarchy it's
[02:27:00 -> 02:27:07]  down to threads instead of
[02:27:03 -> 02:27:10]  um instead of uh blocks right and so you
[02:27:07 -> 02:27:11]  can you can calculate which person you
[02:27:10 -> 02:27:14]  are within that individual apartment
[02:27:11 -> 02:27:15]  like if it's like a if it's like a big
[02:27:14 -> 02:27:17]  apartment with like multiple floors and
[02:27:15 -> 02:27:18]  there's like multiple layers in it you
[02:27:17 -> 02:27:21]  could use that but you you get the point
[02:27:18 -> 02:27:23]  it's a it's a 3D anal ology um and we
[02:27:21 -> 02:27:25]  can find which person we are within that
[02:27:23 -> 02:27:29]  or which which thread essentially in
[02:27:25 -> 02:27:31]  that block it is and so when you add the
[02:27:29 -> 02:27:33]  block offset so the total number of
[02:27:31 -> 02:27:36]  threads leading up to your apartment
[02:27:33 -> 02:27:37]  plus which one you are within that uh
[02:27:36 -> 02:27:39]  within that apartment number then you
[02:27:37 -> 02:27:41]  can actually find which thread you are
[02:27:39 -> 02:27:43]  in the entire grid and then you can do
[02:27:41 -> 02:27:46]  stuff with that right and that's what we
[02:27:43 -> 02:27:48]  say sign the global ID to so Global
[02:27:46 -> 02:27:51]  person ID in the entire apartment
[02:27:48 -> 02:27:53]  complex um
[02:27:51 -> 02:27:55]  and that's that so there's a lot to
[02:27:53 -> 02:27:57]  unpack there feel free to rewatch some
[02:27:55 -> 02:27:59]  of this or even try to visualize some of
[02:27:57 -> 02:28:02]  this on your own maybe write it out um
[02:27:59 -> 02:28:04]  but when we actually go
[02:28:02 -> 02:28:09]  into when we go into our terminal here
[02:28:04 -> 02:28:15]  and go into five um then cud to
[02:28:09 -> 02:28:21]  Basics if we go um nvcc d o we go Zer we
[02:28:15 -> 02:28:24]  go 01 and then 01 like this
[02:28:21 -> 02:28:26]  um it'll compile this binary which we
[02:28:24 -> 02:28:29]  see here and we can just go ahead and
[02:28:26 -> 02:28:31]  execute that and it'll show us uh
[02:28:29 -> 02:28:35]  precisely all of this that we just that
[02:28:31 -> 02:28:38]  we just unpacked so I mean I can't I
[02:28:35 -> 02:28:42]  cannot put all of this on the screen but
[02:28:38 -> 02:28:42]  uh like for example if we look
[02:28:42 -> 02:28:49]  at like how it counts upward right
[02:28:46 -> 02:28:52]  so um you have you have all of your
[02:28:49 -> 02:28:56]  different Dimensions here and you can uh
[02:28:52 -> 02:29:00]  at the very end I believe it outputs the
[02:28:56 -> 02:29:04]  uh the particular thread offset so we
[02:29:00 -> 02:29:07]  notice that it's like 63 and then it
[02:29:04 -> 02:29:10]  jumps to 32 right so it's like 32 and
[02:29:07 -> 02:29:12]  then it goes for
[02:29:10 -> 02:29:14]  um it goes
[02:29:12 -> 02:29:19]  for uh
[02:29:14 -> 02:29:24]  32 numbers so if we go 30 I mean it's
[02:29:19 -> 02:29:28]  it's technically like minus one uh but
[02:29:24 -> 02:29:32]  the the best analogy you can use here is
[02:29:28 -> 02:29:35]  this is 32 threads right um that is a
[02:29:32 -> 02:29:37]  warp so when we talked about 32 threads
[02:29:35 -> 02:29:39]  in a warp this is exactly what it looks
[02:29:37 -> 02:29:42]  like so go back to here as well you'll
[02:29:39 -> 02:29:45]  see it stops at exactly 32 and you'll go
[02:29:42 -> 02:29:49]  up from there so it'll be like 0 to 31
[02:29:45 -> 02:29:51]  so it's like 0 1 2 3 4 so it's like 1 to
[02:29:49 -> 02:29:53]  31 it's 31 elements and then you'll have
[02:29:51 -> 02:29:55]  the additional zero which makes it 32
[02:29:53 -> 02:29:58]  that's just the indexing scheme right
[02:29:55 -> 02:30:00]  and then when you go from 32 to 63 it's
[02:29:58 -> 02:30:04]  it's the same idea um because you go
[02:30:00 -> 02:30:07]  from 0 to 63 instead of 1 to 64 so you
[02:30:04 -> 02:30:10]  do actually have 32 elements uh 32
[02:30:07 -> 02:30:12]  threads per warp in there um and then
[02:30:10 -> 02:30:15]  you can just see the global thread ID in
[02:30:12 -> 02:30:17]  the entire grid so when we when we
[02:30:15 -> 02:30:21]  actually multiply these up we have you
[02:30:17 -> 02:30:23]  know in the in the grid we have 2 * 3
[02:30:21 -> 02:30:26]  which is 6 and then that * 4 is
[02:30:23 -> 02:30:33]  24
[02:30:26 -> 02:30:34]  um 24 * 4 * 4 so that that is 16 and 16
[02:30:33 -> 02:30:38]  * 4 is is
[02:30:34 -> 02:30:42]  64 so you can see 1536 in this entire
[02:30:38 -> 02:30:45]  thing right and so um if we scroll like
[02:30:42 -> 02:30:48]  backwards we can see uh
[02:30:45 -> 02:30:52]  1535 it ends right there and that is the
[02:30:48 -> 02:30:55]  final one so um like for example block
[02:30:52 -> 02:30:58]  um it's this has two elements so it's
[02:30:55 -> 02:30:59]  going to be zero and one and then this
[02:30:58 -> 02:31:04]  is going to have three elements so it's
[02:30:59 -> 02:31:06]  going to be um 0 1 2 and then this is
[02:31:04 -> 02:31:10]  four elements so it's going to be 0 1 2
[02:31:06 -> 02:31:11]  3 is four right uh and then the threads
[02:31:10 -> 02:31:15]  because those each go up to four it's
[02:31:11 -> 02:31:18]  going to be 0 1 2 3 0 1 2 3 0 1 2 3 um
[02:31:15 -> 02:31:20]  and then you end up with um essentially
[02:31:18 -> 02:31:21]  whatever that number is in the end so
[02:31:20 -> 02:31:23]  you can see how this kind of all adds up
[02:31:21 -> 02:31:26]  and how this indexing scheme works and
[02:31:23 -> 02:31:28]  how we can use these to index pieces of
[02:31:26 -> 02:31:31]  data um using like the actual thread and
[02:31:28 -> 02:31:34]  block indexes and then and then do
[02:31:31 -> 02:31:35]  really fast parallel math with that um
[02:31:34 -> 02:31:37]  that's the whole idea here let's go
[02:31:35 -> 02:31:39]  ahead and jump into kernels
[02:31:37 -> 02:31:40]  now okay so now we're going to do a
[02:31:39 -> 02:31:42]  little bit of our math and we're
[02:31:40 -> 02:31:43]  actually going to you know see what
[02:31:42 -> 02:31:45]  these kernels are actually doing and
[02:31:43 -> 02:31:47]  seeing how they work under the hood so
[02:31:45 -> 02:31:49]  it's actually very simple this is the
[02:31:47 -> 02:31:50]  most simple it gets um but essentially
[02:31:49 -> 02:31:53]  we're just going to do some vector
[02:31:50 -> 02:31:56]  addition as a practice so adding these
[02:31:53 -> 02:32:00]  two together element wise 1 + 6 2 + 7 3
[02:31:56 -> 02:32:03]  + 8 Etc and we get all this um very very
[02:32:00 -> 02:32:05]  simple and easy to understand we can we
[02:32:03 -> 02:32:08]  have a CPU example here which is obvious
[02:32:05 -> 02:32:12]  and easy to look at
[02:32:08 -> 02:32:14]  um we have a GPU example which is
[02:32:12 -> 02:32:15]  actually a little weird it's a it's
[02:32:14 -> 02:32:18]  different than this because here we have
[02:32:15 -> 02:32:20]  a for Loop and here we have this this it
[02:32:18 -> 02:32:23]  term which is ID block idx time Block in
[02:32:20 -> 02:32:25]  plus thread IX and I'm going to explain
[02:32:23 -> 02:32:26]  this in a second here but this doesn't
[02:32:25 -> 02:32:28]  have a for Loop and essentially what
[02:32:26 -> 02:32:30]  this is doing like I talked about before
[02:32:28 -> 02:32:32]  is it's just unrolling this Loop so you
[02:32:30 -> 02:32:33]  know CPU is going to like do this
[02:32:32 -> 02:32:35]  iteration and this one and then this one
[02:32:33 -> 02:32:37]  and then this one the GPU is going to
[02:32:35 -> 02:32:39]  take all these individual iterations and
[02:32:37 -> 02:32:41]  distribute it across a bunch of blocks
[02:32:39 -> 02:32:43]  or or caor you could say uh and it's
[02:32:41 -> 02:32:45]  going to parallelize that operation and
[02:32:43 -> 02:32:48]  make it really really fast so instead of
[02:32:45 -> 02:32:50]  doing uh separately like 10 million
[02:32:48 -> 02:32:52]  different operations like in order order
[02:32:50 -> 02:32:55]  it's going to take roughly 10,000 time
[02:32:52 -> 02:32:58]  units um say you had you know 10,000
[02:32:55 -> 02:33:00]  cicor to split this across it's like
[02:32:58 -> 02:33:03]  well that's that that's actually a lot
[02:33:00 -> 02:33:04]  less now that's only about a thousand
[02:33:03 -> 02:33:07]  times depths you have to do so that it's
[02:33:04 -> 02:33:08]  it's sped up uh an insane amount just by
[02:33:07 -> 02:33:12]  Distributing it across and that's
[02:33:08 -> 02:33:13]  theoretical of course but um you know we
[02:33:12 -> 02:33:15]  initialize vectors this should be this
[02:33:13 -> 02:33:17]  should be very intuitive if you've
[02:33:15 -> 02:33:19]  written like any random stuff in random
[02:33:17 -> 02:33:20]  gens in C before it's going to
[02:33:19 -> 02:33:22]  essentially take
[02:33:20 -> 02:33:26]  uh a random integer between zero and
[02:33:22 -> 02:33:28]  Rand Max so Rand Max is this um very
[02:33:26 -> 02:33:30]  easy to understand it's going to be a
[02:33:28 -> 02:33:32]  floating Point number um and then a
[02:33:30 -> 02:33:34]  timing function just to measure
[02:33:32 -> 02:33:37]  execution time again in this script we
[02:33:34 -> 02:33:39]  are benchmarking so perform War warm-up
[02:33:37 -> 02:33:42]  runs get things you know fired up and
[02:33:39 -> 02:33:44]  then Benchmark CPU to GPU and see how
[02:33:42 -> 02:33:46]  well it does um but this isn't really
[02:33:44 -> 02:33:50]  the important part here what I wanted to
[02:33:46 -> 02:33:52]  mostly expand on is what's what's what
[02:33:50 -> 02:33:53]  things specifically here apply to Cuda
[02:33:52 -> 02:33:56]  and what do you really need to
[02:33:53 -> 02:33:58]  understand so we have this Cuda Malik
[02:33:56 -> 02:34:00]  which is the same as Malik except it's
[02:33:58 -> 02:34:02]  on GPU so it's going to do that it's
[02:34:00 -> 02:34:07]  going to allocate memory on on the
[02:34:02 -> 02:34:10]  global Dam or the vram on the GPU and uh
[02:34:07 -> 02:34:14]  all this really has is a device pointer
[02:34:10 -> 02:34:18]  and a size so we have this we have this
[02:34:14 -> 02:34:21]  device a this device a vector or array
[02:34:18 -> 02:34:23]  is uh declared here which is a pointer
[02:34:21 -> 02:34:25]  um and then we set the size for that
[02:34:23 -> 02:34:28]  right and this is just the memory
[02:34:25 -> 02:34:31]  address of that so uh we we allocate
[02:34:28 -> 02:34:33]  device memory with Cuda Malik um and
[02:34:31 -> 02:34:34]  then when we actually want to move the
[02:34:33 -> 02:34:36]  stuff that we've created on the host
[02:34:34 -> 02:34:39]  because remember we initialize these
[02:34:36 -> 02:34:41]  vectors on a global or or just a just a
[02:34:39 -> 02:34:43]  regular void CPU function so we actually
[02:34:41 -> 02:34:46]  have to copy these over now and how we
[02:34:43 -> 02:34:50]  do that is we just literally look at
[02:34:46 -> 02:34:52]  this destination source how big it is
[02:34:50 -> 02:34:56]  and what what kind of copy do we want to
[02:34:52 -> 02:34:57]  do so destination is device hence the d
[02:34:56 -> 02:35:02]  The Source is
[02:34:57 -> 02:35:05]  host it's size big um like we declared
[02:35:02 -> 02:35:07]  here and then CM copy host to device so
[02:35:05 -> 02:35:11]  CPU it's going to move to GPU and that's
[02:35:07 -> 02:35:11]  it very simple
[02:35:11 -> 02:35:17]  um we Define uh this numb blocks which
[02:35:15 -> 02:35:19]  is a little bit different than what we
[02:35:17 -> 02:35:22]  did in this indexing thing uh because
[02:35:19 -> 02:35:26]  it's not actually this dim 3 type as we
[02:35:22 -> 02:35:29]  saw before um it's it still works though
[02:35:26 -> 02:35:34]  the whole idea with this is that uh if
[02:35:29 -> 02:35:36]  instead of say uh if instead of having
[02:35:34 -> 02:35:39]  like 2 three 4 if we just wanted it to
[02:35:36 -> 02:35:44]  be like a length of uh what is this this
[02:35:39 -> 02:35:46]  is 24 2 * 3 * 4 is 24 so you would you
[02:35:44 -> 02:35:50]  could essentially set this to 24 and
[02:35:46 -> 02:35:54]  then set these to one and just having uh
[02:35:50 -> 02:35:56]  numb blocks and then putting this in in
[02:35:54 -> 02:35:59]  that uh in the kernel launch actually
[02:35:56 -> 02:36:02]  just converts the integer to like dim
[02:35:59 -> 02:36:04]  three and then it's like it's like numb
[02:36:02 -> 02:36:06]  blocks and then one and one so it's just
[02:36:04 -> 02:36:08]  like a it's just length only and it it's
[02:36:06 -> 02:36:10]  still like it still looks like
[02:36:08 -> 02:36:12]  volumetric but it's just laid out
[02:36:10 -> 02:36:14]  linearly so it ends up looking like a
[02:36:12 -> 02:36:17]  line and could interprets it as a line
[02:36:14 -> 02:36:21]  in Hardware
[02:36:17 -> 02:36:23]  um then you might ask okay well how
[02:36:21 -> 02:36:25]  exactly do we calculate num blocks well
[02:36:23 -> 02:36:27]  this is very interesting
[02:36:25 -> 02:36:29]  so we have a bunch of things going on
[02:36:27 -> 02:36:33]  here and this seems a little funny so we
[02:36:29 -> 02:36:35]  have n plus block size minus one and I'm
[02:36:33 -> 02:36:36]  going to illustrate this out here now
[02:36:35 -> 02:36:38]  just to clear up what the heck is this
[02:36:36 -> 02:36:41]  numb blocks things means I actually laid
[02:36:38 -> 02:36:44]  out some calculations for this so block
[02:36:41 -> 02:36:46]  size is the number of threads inside of
[02:36:44 -> 02:36:48]  a block it's the size of the block
[02:36:46 -> 02:36:50]  itself which threads are going to fit
[02:36:48 -> 02:36:53]  into right so so if we have let's just
[02:36:50 -> 02:36:56]  say instead of 10 million elements like
[02:36:53 -> 02:36:58]  we have up here let's say we have 1,24
[02:36:56 -> 02:37:03]  elements right uh if we're trying to fit
[02:36:58 -> 02:37:04]  1,24 elements across uh 256 threads per
[02:37:03 -> 02:37:05]  block that means we're probably going to
[02:37:04 -> 02:37:10]  want four blocks right it'll split it
[02:37:05 -> 02:37:13]  evenly because 256 * 4 is 1024 um and so
[02:37:10 -> 02:37:15]  we have to actually calculate this
[02:37:13 -> 02:37:19]  manually but we have to keep in mind
[02:37:15 -> 02:37:20]  that we are doing uh like we there are
[02:37:19 -> 02:37:22]  more things things we have to keep track
[02:37:20 -> 02:37:26]  of in case say this number ends up being
[02:37:22 -> 02:37:29]  like 1025 right so I actually wrote out
[02:37:26 -> 02:37:32]  a script that does that does this math
[02:37:29 -> 02:37:36]  uh perfectly for us um so let's first
[02:37:32 -> 02:37:39]  look at this so you have this uh 1024
[02:37:36 -> 02:37:42]  plus 256 that's that's the length of the
[02:37:39 -> 02:37:43]  array plus the block size right number
[02:37:42 -> 02:37:46]  of threads per block then you're going
[02:37:43 -> 02:37:49]  to do minus one and whatever that is
[02:37:46 -> 02:37:51]  divide that by the block size uh and
[02:37:49 -> 02:37:53]  then
[02:37:51 -> 02:37:56]  uh the the compiler is automatically
[02:37:53 -> 02:37:59]  going to floor this answer it's going to
[02:37:56 -> 02:38:02]  truncate those those decimal places off
[02:37:59 -> 02:38:03]  so if you get like 4.99 or whatever it's
[02:38:02 -> 02:38:05]  going to take that 0.99 and just
[02:38:03 -> 02:38:07]  truncate it off so you're going to end
[02:38:05 -> 02:38:11]  up with four so if we were to do for
[02:38:07 -> 02:38:16]  example like 1,00
[02:38:11 -> 02:38:21]  um 1024 plus 256 well what's that 1 1,00
[02:38:16 -> 02:38:23]  + 200 is 1200 and then 56 + 24 is 80 so
[02:38:21 -> 02:38:27]  we get
[02:38:23 -> 02:38:33]  1,280 um and if we divide this by
[02:38:27 -> 02:38:33]  256 we end up getting around what's the
[02:38:34 -> 02:38:40]  answer divided 256 we get
[02:38:41 -> 02:38:47]  um we get this number but remember we
[02:38:45 -> 02:38:49]  have the one here which I actually
[02:38:47 -> 02:38:53]  forgot for a second there we have have
[02:38:49 -> 02:38:56]  the one so it technically is uh 79 so
[02:38:53 -> 02:38:57]  you end up with this N9 part and it ends
[02:38:56 -> 02:39:01]  up just being four because you truncate
[02:38:57 -> 02:39:03]  that off however if you end up having
[02:39:01 -> 02:39:05]  this as like
[02:39:03 -> 02:39:08]  1,25 then this number is actually going
[02:39:05 -> 02:39:11]  to end up as 1,280 because you're just
[02:39:08 -> 02:39:14]  adding one back to you know 1279 and you
[02:39:11 -> 02:39:16]  end up with five so in case you end up
[02:39:14 -> 02:39:18]  adding an extra element you want to
[02:39:16 -> 02:39:20]  allocate space and resources for that or
[02:39:18 -> 02:39:22]  else you will not get the answer that
[02:39:20 -> 02:39:23]  you want so that's all this is doing up
[02:39:22 -> 02:39:25]  here and we make sure and this is just
[02:39:23 -> 02:39:30]  like a careful calculation to make sure
[02:39:25 -> 02:39:32]  that everything goes as as we want um
[02:39:30 -> 02:39:35]  and so this is just a a little script
[02:39:32 -> 02:39:37]  that I wrote up to test this but I don't
[02:39:35 -> 02:39:40]  need that
[02:39:37 -> 02:39:40]  anymore
[02:39:42 -> 02:39:50]  um
[02:39:44 -> 02:39:52]  so going further um
[02:39:50 -> 02:39:56]  we essentially in this kernel here let
[02:39:52 -> 02:39:58]  me slide up so in this kernel we have
[02:39:56 -> 02:40:00]  just this this x Dimension laid out
[02:39:58 -> 02:40:02]  right so what you're doing is you have
[02:40:00 -> 02:40:05]  this blocks block idx which is which
[02:40:02 -> 02:40:08]  block it is in that in that line of a
[02:40:05 -> 02:40:12]  grid and then you're multiplying that by
[02:40:08 -> 02:40:14]  the size of the block so how how many
[02:40:12 -> 02:40:17]  threads are there per block times the
[02:40:14 -> 02:40:19]  number of blocks uh and then plus
[02:40:17 -> 02:40:22]  whichever whatever
[02:40:19 -> 02:40:26]  thread we're at right so this gives us
[02:40:22 -> 02:40:29]  the thread um in that in that like line
[02:40:26 -> 02:40:31]  of a grid right and so we end up with
[02:40:29 -> 02:40:34]  whatever place we're at and we use that
[02:40:31 -> 02:40:37]  thread index to then access elements in
[02:40:34 -> 02:40:40]  A and B and C uh and then we just do an
[02:40:37 -> 02:40:42]  an add operation so whichever you know
[02:40:40 -> 02:40:44]  it might be in some cases this might be
[02:40:42 -> 02:40:46]  like you know 2.5 million and in some
[02:40:44 -> 02:40:48]  cases it might be like three uh in which
[02:40:46 -> 02:40:50]  case they're going to be the same number
[02:40:48 -> 02:40:52]  uh and then they're going to add and
[02:40:50 -> 02:40:53]  we're going to get the answer that we
[02:40:52 -> 02:40:56]  expect it just might not happen in the
[02:40:53 -> 02:40:59]  order that like a a loop might right so
[02:40:56 -> 02:41:01]  instead of doing like uh the first the
[02:40:59 -> 02:41:02]  uh the first index and the second index
[02:41:01 -> 02:41:04]  and the third it's going to like scatter
[02:41:02 -> 02:41:05]  and distribute these and it's just going
[02:41:04 -> 02:41:08]  to be fast right so that's what the
[02:41:05 -> 02:41:10]  whole idea is there
[02:41:08 -> 02:41:14]  um and if we go ahead and actually run
[02:41:10 -> 02:41:14]  this script
[02:41:16 -> 02:41:23]  um go 0 0 and then we could just so 0
[02:41:20 -> 02:41:26]  Vector ad and then enter we'll just run
[02:41:23 -> 02:41:29]  this file performing oneup runs uh C
[02:41:26 -> 02:41:33]  Benchmark and CPU Benchmark and GPU so
[02:41:29 -> 02:41:35]  the CPU average time is uh about .14
[02:41:33 -> 02:41:38]  milliseconds which is really fast
[02:41:35 -> 02:41:41]  however the GPU average time is
[02:41:38 -> 02:41:44]  significantly less than that about 143x
[02:41:41 -> 02:41:47]  speed up almost 144 uh and the results
[02:41:44 -> 02:41:49]  match up when we compare them uh index
[02:41:47 -> 02:41:52]  index wise or element wise so so we just
[02:41:49 -> 02:41:55]  verify the results here um we ensure
[02:41:52 -> 02:41:57]  that the the absolute value of the
[02:41:55 -> 02:42:00]  difference between those two is greater
[02:41:57 -> 02:42:02]  than uh 1 * 105 which is just a common
[02:42:00 -> 02:42:04]  verification thing so you'll see that
[02:42:02 -> 02:42:07]  when we're comparing things uh you know
[02:42:04 -> 02:42:09]  more like as we go more into Cuda it's
[02:42:07 -> 02:42:11]  going to be this idea of you Benchmark
[02:42:09 -> 02:42:14]  uh you get like an average time across
[02:42:11 -> 02:42:16]  all the runs and then you make sure that
[02:42:14 -> 02:42:18]  you're getting the correct results by
[02:42:16 -> 02:42:20]  having this tolerance Factor so
[02:42:18 -> 02:42:23]  sometimes this might be like super low
[02:42:20 -> 02:42:25]  or it might be like super high um but
[02:42:23 -> 02:42:28]  that's that's typically how we'll do
[02:42:25 -> 02:42:31]  it then we have the second example of
[02:42:28 -> 02:42:34]  vector addition which is uh very much
[02:42:31 -> 02:42:36]  the same however instead of just having
[02:42:34 -> 02:42:39]  this uh one dimensional like x axis
[02:42:36 -> 02:42:43]  thing where we have one two three four
[02:42:39 -> 02:42:45]  lines um we have a lot more so going
[02:42:43 -> 02:42:47]  back to that example from indexing where
[02:42:45 -> 02:42:49]  we had you know three dimensions um if
[02:42:47 -> 02:42:52]  we actually apply this to Vector Edition
[02:42:49 -> 02:42:54]  we get a noticeable slow down so the
[02:42:52 -> 02:42:56]  first thing you'll notice is that this
[02:42:54 -> 02:42:58]  has way more lines but you're like
[02:42:56 -> 02:42:59]  Elliot certainly this is going to be
[02:42:58 -> 02:43:02]  faster right it uses up the whole Space
[02:42:59 -> 02:43:04]  instead of just uh instead of just a
[02:43:02 -> 02:43:06]  little bit right um well the issue with
[02:43:04 -> 02:43:08]  this is that it Cuda is not really going
[02:43:06 -> 02:43:10]  to struggle with uh scheduling things
[02:43:08 -> 02:43:11]  and making them run fast and and
[02:43:10 -> 02:43:13]  compiling down to something that's going
[02:43:11 -> 02:43:15]  to like really work at speed um it's
[02:43:13 -> 02:43:17]  more so like what are the calculations
[02:43:15 -> 02:43:20]  that you're actually doing in a single
[02:43:17 -> 02:43:21]  uh thread right so this is a this is
[02:43:20 -> 02:43:26]  what's going to happen in a thread
[02:43:21 -> 02:43:28]  notice how we do 1 2 3 4 5 six
[02:43:26 -> 02:43:31]  operations so um three adds three
[02:43:28 -> 02:43:33]  multiplies and three stores so equal
[02:43:31 -> 02:43:35]  sign as well and then this one it's like
[02:43:33 -> 02:43:37]  you have a bunch of these comparisons
[02:43:35 -> 02:43:40]  and and it's just like a bunch of math
[02:43:37 -> 02:43:44]  you it's it's like hard to read right
[02:43:40 -> 02:43:49]  and the point is this does
[02:43:44 -> 02:43:52]  one one multiply one store or one
[02:43:49 -> 02:43:55]  multiply two stores and two ads
[02:43:52 -> 02:43:58]  significantly less than this one so the
[02:43:55 -> 02:44:00]  point is um only use the 3D aspect when
[02:43:58 -> 02:44:02]  you absolutely need to when it is like
[02:44:00 -> 02:44:03]  dependent on your algorithm and you
[02:44:02 -> 02:44:05]  don't need to uh when you when you
[02:44:03 -> 02:44:08]  actually have something that that's like
[02:44:05 -> 02:44:09]  uh spatially 3D then you can use
[02:44:08 -> 02:44:11]  something like this because it might
[02:44:09 -> 02:44:13]  actually work a bit easier and you w't
[02:44:11 -> 02:44:15]  have to do all these calculations to end
[02:44:13 -> 02:44:17]  up laying out this 3D space into like
[02:44:15 -> 02:44:19]  this onedimensional thing um and you
[02:44:17 -> 02:44:21]  have to worry about like things wrapping
[02:44:19 -> 02:44:23]  around and and strides and all this um
[02:44:21 -> 02:44:24]  so that's like that's mainly the
[02:44:23 -> 02:44:27]  bottleneck there and I just really did a
[02:44:24 -> 02:44:29]  comparison between the 3D and the 1D
[02:44:27 -> 02:44:32]  Vector Edition kernel so we can go ahead
[02:44:29 -> 02:44:35]  and actually compile this
[02:44:32 -> 02:44:35]  here
[02:44:36 -> 02:44:43]  um so we notice that they're both a lot
[02:44:39 -> 02:44:47]  faster however um the speed up CPU
[02:44:43 -> 02:44:49]  versus GPU 1D like the GPU 1D is 106
[02:44:47 -> 02:44:52]  times faster but the 3D is only 102
[02:44:49 -> 02:44:57]  times faster so this is actually faster
[02:44:52 -> 02:45:00]  than the um than the GPU 3D um not by a
[02:44:57 -> 02:45:02]  crazy amount but you know by by like 3
[02:45:00 -> 02:45:05]  4% maybe and if you scale up your
[02:45:02 -> 02:45:08]  numbers it might grow but you get the
[02:45:05 -> 02:45:10]  point um there's a lot of unnecessary
[02:45:08 -> 02:45:12]  calculations there um and it's just kind
[02:45:10 -> 02:45:14]  of simpler to go down this route with
[02:45:12 -> 02:45:16]  the 1D
[02:45:14 -> 02:45:18]  kernel now we dive into something a
[02:45:16 -> 02:45:19]  little bit more intuitive
[02:45:18 -> 02:45:21]  algorithmically called matrix
[02:45:19 -> 02:45:23]  multiplication you might have already
[02:45:21 -> 02:45:24]  done this in which case you know this
[02:45:23 -> 02:45:26]  might just be some simple review you
[02:45:24 -> 02:45:27]  might want to skip ahead it's it's up to
[02:45:26 -> 02:45:30]  you really but I'm going to go over this
[02:45:27 -> 02:45:31]  be no matter what because some people
[02:45:30 -> 02:45:34]  may not know and sometimes it's good to
[02:45:31 -> 02:45:37]  get a little refresher on that so we're
[02:45:34 -> 02:45:39]  essentially going to write the naive
[02:45:37 -> 02:45:42]  version the naive version of the matrix
[02:45:39 -> 02:45:45]  multiplication Cuda kernel which is the
[02:45:42 -> 02:45:49]  slowest one but it's the most basic and
[02:45:45 -> 02:45:54]  intuitive to understand um
[02:45:49 -> 02:45:58]  so a matrix looks like this you have
[02:45:54 -> 02:46:00]  rows and you have columns right um let
[02:45:58 -> 02:46:04]  me actually zoom in a little more here
[02:46:00 -> 02:46:07]  so rows and columns um for example a is
[02:46:04 -> 02:46:11]  a 3 by two because it has three rows and
[02:46:07 -> 02:46:13]  two columns right so it's like three
[02:46:11 -> 02:46:16]  high and two long it's like width by
[02:46:13 -> 02:46:18]  height you could say or height by width
[02:46:16 -> 02:46:21]  and then we have B which is a 2x4 so
[02:46:18 -> 02:46:24]  it's two row rows and four uh four
[02:46:21 -> 02:46:27]  columns right uh and the idea is is that
[02:46:24 -> 02:46:30]  as long as these two inner numbers are
[02:46:27 -> 02:46:31]  the same then uh then we then it
[02:46:30 -> 02:46:34]  actually works we're allowed to do that
[02:46:31 -> 02:46:36]  matrix multiplication um and you'll see
[02:46:34 -> 02:46:37]  why in a second here and then these
[02:46:36 -> 02:46:39]  outer Dimensions these three and four
[02:46:37 -> 02:46:44]  would end up being the new size of the
[02:46:39 -> 02:46:46]  new output Matrix C uh so we have you
[02:46:44 -> 02:46:49]  know 1 2 3 4 5 6 and 7 8 9 10 11 12 13
[02:46:46 -> 02:46:52]  14 and what we do here is is it's very
[02:46:49 -> 02:46:55]  is it's very simple you essentially go 7
[02:46:52 -> 02:46:57]  and 11 you you take this you have this
[02:46:55 -> 02:47:00]  uh this B it's like this and then a is
[02:46:57 -> 02:47:05]  like this and so you take the seven and
[02:47:00 -> 02:47:10]  11 in in B and you rotate it and you do
[02:47:05 -> 02:47:13]  a DOT product with uh one and two so you
[02:47:10 -> 02:47:14]  take the seven and and the 11 you flip
[02:47:13 -> 02:47:17]  it over and so the seven is going to
[02:47:14 -> 02:47:19]  multiply with the one and then the 11 is
[02:47:17 -> 02:47:21]  going to multiply with the two right so
[02:47:19 -> 02:47:24]  you're just like it's like sideways uh
[02:47:21 -> 02:47:25]  and then when you multiply one with one
[02:47:24 -> 02:47:27]  with the seven you get seven and two
[02:47:25 -> 02:47:29]  with uh 11 gets 22 and then you would
[02:47:27 -> 02:47:32]  add those together to get
[02:47:29 -> 02:47:33]  29 um and we can see that right here as
[02:47:32 -> 02:47:36]  the first
[02:47:33 -> 02:47:38]  element so notice how it's like the
[02:47:36 -> 02:47:41]  First Column and the first row aligned
[02:47:38 -> 02:47:43]  together and so it's like they're
[02:47:41 -> 02:47:45]  they're like pointing at one spot it's
[02:47:43 -> 02:47:47]  like the first it's like the first row
[02:47:45 -> 02:47:49]  up here instead of down here first row
[02:47:47 -> 02:47:51]  and then the first column column and
[02:47:49 -> 02:47:55]  they meet together and you get this top
[02:47:51 -> 02:47:57]  left corner thing um and that's and
[02:47:55 -> 02:47:59]  that's where we end up with this 29
[02:47:57 -> 02:48:01]  value and then you essentially just do
[02:47:59 -> 02:48:04]  this for the rest of them so you go uh 8
[02:48:01 -> 02:48:07]  and and 12 and then you you flip that
[02:48:04 -> 02:48:09]  flip that sideways and it'll multiply
[02:48:07 -> 02:48:12]  with the one and the two
[02:48:09 -> 02:48:15]  um and then you put that
[02:48:12 -> 02:48:17]  here you have the the second column and
[02:48:15 -> 02:48:20]  the first row so it's going to it's
[02:48:17 -> 02:48:23]  going to meet in the second column and
[02:48:20 -> 02:48:24]  the first row right um and then you just
[02:48:23 -> 02:48:26]  continue doing this for the rest of them
[02:48:24 -> 02:48:29]  until you end up with your final answer
[02:48:26 -> 02:48:33]  so you're essentially just like flipping
[02:48:29 -> 02:48:35]  the column of B onto a row of a and
[02:48:33 -> 02:48:37]  you're doing a do product operation
[02:48:35 -> 02:48:39]  where each uh each of like the like
[02:48:37 -> 02:48:41]  element wise you're going to multiply
[02:48:39 -> 02:48:43]  and then you add all you reduce and you
[02:48:41 -> 02:48:45]  add all of them together and you squash
[02:48:43 -> 02:48:49]  it and then you end up with this final
[02:48:45 -> 02:48:53]  Matrix uh which is of shape uh 3x4 so
[02:48:49 -> 02:48:56]  three uh rows three three rows High by
[02:48:53 -> 02:49:00]  four columns wide and then and then
[02:48:56 -> 02:49:02]  that's how you do a mat mole um so when
[02:49:00 -> 02:49:04]  we go into the I mean typically when
[02:49:02 -> 02:49:05]  you're writing out you know hard to
[02:49:04 -> 02:49:06]  understand algorithms like this when
[02:49:05 -> 02:49:09]  you're trying to fit this all in your
[02:49:06 -> 02:49:11]  head ideally you want to write it on the
[02:49:09 -> 02:49:12]  CPU first if you just jump straight into
[02:49:11 -> 02:49:13]  GPU and try to optimize you're probably
[02:49:12 -> 02:49:15]  going to mess up you're probably not
[02:49:13 -> 02:49:17]  going to get the answers you're looking
[02:49:15 -> 02:49:19]  for and things are going to be weird so
[02:49:17 -> 02:49:21]  you write out the maybe even go back to
[02:49:19 -> 02:49:24]  Python and write this out in Python
[02:49:21 -> 02:49:25]  first so you can visualize it um and
[02:49:24 -> 02:49:28]  make sure that yours matches like P
[02:49:25 -> 02:49:30]  towards or nump or something and then
[02:49:28 -> 02:49:33]  and then you write this out in C and you
[02:49:30 -> 02:49:36]  say okay well how do we do a a m Mo on
[02:49:33 -> 02:49:40]  the CPU here so you have your a and your
[02:49:36 -> 02:49:47]  B and your C Matrix um and then your
[02:49:40 -> 02:49:49]  shapes m k n so m is uh this this how
[02:49:47 -> 02:49:53]  high a is so m in this case was would be
[02:49:49 -> 02:49:56]  three k would be two so two and two and
[02:49:53 -> 02:50:00]  then n would be four right so you end up
[02:49:56 -> 02:50:03]  doing this like
[02:50:00 -> 02:50:06]  um M yeah just just like this
[02:50:03 -> 02:50:08]  autocomplete M * K and then you multiply
[02:50:06 -> 02:50:12]  that with a K byn Matrix and you get an
[02:50:08 -> 02:50:15]  M byn Matrix just space this out so it's
[02:50:12 -> 02:50:15]  easier to look
[02:50:17 -> 02:50:23]  at uh and and that's that so when we
[02:50:20 -> 02:50:25]  look at our our nested for Loops here we
[02:50:23 -> 02:50:27]  can see that we iterate over M so that's
[02:50:25 -> 02:50:31]  the
[02:50:27 -> 02:50:33]  uh that is the height of a right that's
[02:50:31 -> 02:50:35]  the number of rows we have and then
[02:50:33 -> 02:50:37]  we're going to I plus plus that each
[02:50:35 -> 02:50:38]  time and keep in mind when this is laid
[02:50:37 -> 02:50:40]  out in memory it's not actually going to
[02:50:38 -> 02:50:44]  be a matrix it's going to be an array so
[02:50:40 -> 02:50:47]  you're going to have like one 2 3 4 5 6
[02:50:44 -> 02:50:48]  instead of 1 two like as an array and
[02:50:47 -> 02:50:49]  then another array below it and then
[02:50:48 -> 02:50:52]  another it's it's not like that it's
[02:50:49 -> 02:50:55]  just laid out at once so you have to
[02:50:52 -> 02:50:56]  actually manually consider like the
[02:50:55 -> 02:50:58]  wrapping over so you have to actually
[02:50:56 -> 02:51:01]  keep that in mind when you're writing
[02:50:58 -> 02:51:04]  these and that's a tricky part too
[02:51:01 -> 02:51:11]  um so then you have J which is going to
[02:51:04 -> 02:51:15]  iterate over um n which is uh n is uh
[02:51:11 -> 02:51:18]  the number of columns
[02:51:15 -> 02:51:21]  here and then we plus plus that
[02:51:18 -> 02:51:23]  like each iteration we we make this
[02:51:21 -> 02:51:25]  accumulation sum so we're going to
[02:51:23 -> 02:51:29]  accumulate into the sum right because
[02:51:25 -> 02:51:31]  you're going You're essentially uh
[02:51:29 -> 02:51:33]  accumulating things as you're like when
[02:51:31 -> 02:51:35]  we do the add operation and we have all
[02:51:33 -> 02:51:36]  these multiplies and we fuse and add
[02:51:35 -> 02:51:39]  them together that's what this
[02:51:36 -> 02:51:42]  accumulation sum is for uh and so when
[02:51:39 -> 02:51:50]  we iterate through um when we iterate
[02:51:42 -> 02:51:52]  through k um which is K is uh the
[02:51:50 -> 02:51:56]  the the X Dimension you could say in a
[02:51:52 -> 02:51:57]  or the number of columns and then K in B
[02:51:56 -> 02:52:00]  is going to be the height or the number
[02:51:57 -> 02:52:03]  of rows right and and so you iterate
[02:52:00 -> 02:52:05]  through that and when you do your sum
[02:52:03 -> 02:52:07]  you essentially add it and you do um you
[02:52:05 -> 02:52:10]  do essentially this is where the dot
[02:52:07 -> 02:52:13]  product comes in right you do a so
[02:52:10 -> 02:52:17]  that's I where like whatever I is let's
[02:52:13 -> 02:52:19]  say I is like um I is zero right so I is
[02:52:17 -> 02:52:21]  going to be um
[02:52:19 -> 02:52:23]  I is going to be whatever this is right
[02:52:21 -> 02:52:28]  it's going to be the first the the first
[02:52:23 -> 02:52:32]  one times whatever K is and K uh K in
[02:52:28 -> 02:52:36]  this case is going to be well two so
[02:52:32 -> 02:52:39]  when you have zero the zeroth
[02:52:36 -> 02:52:40]  um when when I is zero and K is whatever
[02:52:39 -> 02:52:42]  number it's still going to end up
[02:52:40 -> 02:52:44]  equaling zero and so you have L
[02:52:42 -> 02:52:47]  afterwards and that's going to be
[02:52:44 -> 02:52:50]  whichever spot at what whatever it
[02:52:47 -> 02:52:51]  wherever it is through through K that's
[02:52:50 -> 02:52:55]  where it's going to end up at so like
[02:52:51 -> 02:52:56]  the offset through the row um and it's
[02:52:55 -> 02:53:00]  going to multiply the same thing it's
[02:52:56 -> 02:53:04]  going to do um l so L is where it's at
[02:53:00 -> 02:53:06]  through K which is going to be um the
[02:53:04 -> 02:53:07]  going up and down instead of left right
[02:53:06 -> 02:53:11]  it's going to be up and
[02:53:07 -> 02:53:15]  down and then you have this n term which
[02:53:11 -> 02:53:17]  we could say is uh maybe also uh zero if
[02:53:15 -> 02:53:19]  you're just doing the first one here
[02:53:17 -> 02:53:22]  like the top top left corner and then
[02:53:19 -> 02:53:24]  say j in this case is zero so it's just
[02:53:22 -> 02:53:26]  going to end up hitting the it's going
[02:53:24 -> 02:53:28]  to end up in hitting the same value so
[02:53:26 -> 02:53:30]  you end up just getting the first the
[02:53:28 -> 02:53:32]  first points uh and then you you
[02:53:30 -> 02:53:36]  multiply them together and then you add
[02:53:32 -> 02:53:38]  that um you you you multiply them you
[02:53:36 -> 02:53:41]  multiply the first the one and the the
[02:53:38 -> 02:53:42]  seven together in the first one and then
[02:53:41 -> 02:53:45]  you end up hitting the second one which
[02:53:42 -> 02:53:48]  is the two and the 11 um and that gets
[02:53:45 -> 02:53:52]  summed up together and you're do doing
[02:53:48 -> 02:53:55]  this every single time uh this for Loop
[02:53:52 -> 02:53:56]  this second uh for Loop triggers right
[02:53:55 -> 02:54:01]  so every time this goes through an
[02:53:56 -> 02:54:06]  iteration you're hitting n and n is uh
[02:54:01 -> 02:54:07]  just n is just whichever value uh
[02:54:06 -> 02:54:09]  whichever value is essentially coming
[02:54:07 -> 02:54:10]  next right and so you're just getting
[02:54:09 -> 02:54:12]  this one and this one and then this one
[02:54:10 -> 02:54:14]  and then this one and so on so forth
[02:54:12 -> 02:54:17]  until the end and then you end up just
[02:54:14 -> 02:54:20]  writing this out so you you essentially
[02:54:17 -> 02:54:21]  assign to Value C to whatever that sum
[02:54:20 -> 02:54:25]  is so that you can compute the next dot
[02:54:21 -> 02:54:27]  product so uh this this is like very uh
[02:54:25 -> 02:54:29]  visual I encourage you to I mean if this
[02:54:27 -> 02:54:32]  doesn't completely make sense if you
[02:54:29 -> 02:54:34]  haven't like taken a introductory linear
[02:54:32 -> 02:54:36]  algebra course I completely get it um
[02:54:34 -> 02:54:38]  you might want to just pass us through
[02:54:36 -> 02:54:40]  you know language models or look at some
[02:54:38 -> 02:54:41]  some intuitive videos on the internet
[02:54:40 -> 02:54:45]  and just sort of understand what's going
[02:54:41 -> 02:54:47]  on here try to understand uh what like
[02:54:45 -> 02:54:49]  how things are wrapping around when they
[02:54:47 -> 02:54:51]  when they do like a dried or something
[02:54:49 -> 02:54:53]  um that's that's very important to pay
[02:54:51 -> 02:54:57]  attention to like this like the K when
[02:54:53 -> 02:55:01]  the K is wrapping for example K is uh K
[02:54:57 -> 02:55:02]  is here and K is essentially this this
[02:55:01 -> 02:55:05]  length so it's going to be like
[02:55:02 -> 02:55:08]  whichever whichever whichever uh row you
[02:55:05 -> 02:55:09]  want you want to wrap around that entire
[02:55:08 -> 02:55:11]  row so you want to go to the length of
[02:55:09 -> 02:55:14]  it and wrap and then your offset is
[02:55:11 -> 02:55:16]  going to be that and then same idea here
[02:55:14 -> 02:55:19]  except instead of rows it's going to be
[02:55:16 -> 02:55:21]  like columns column offset right
[02:55:19 -> 02:55:25]  um and that's that's the whole idea
[02:55:21 -> 02:55:27]  there um and then we go into the GPU
[02:55:25 -> 02:55:30]  implementation which is a little bit
[02:55:27 -> 02:55:32]  different but we're essentially using
[02:55:30 -> 02:55:36]  instead of just an i or an ID a single
[02:55:32 -> 02:55:39]  idx term we use a rows and columns so in
[02:55:36 -> 02:55:42]  this grid we have the block ID x.y *
[02:55:39 -> 02:55:46]  block dim doy block idx is you know
[02:55:42 -> 02:55:48]  where the where the block is at in um
[02:55:46 -> 02:55:51]  where the essentially where the block is
[02:55:48 -> 02:55:53]  vertically um and we're just getting
[02:55:51 -> 02:55:56]  essentially the the vertical thread like
[02:55:53 -> 02:55:58]  which thread are we do we want within uh
[02:55:56 -> 02:56:00]  considering like this vertical grid and
[02:55:58 -> 02:56:02]  all the blocks that we have right uh
[02:56:00 -> 02:56:04]  going back to what I said before and we
[02:56:02 -> 02:56:07]  do the same thing with X so we have this
[02:56:04 -> 02:56:10]  we have the thread in the uh vertical
[02:56:07 -> 02:56:13]  and the horizontal
[02:56:10 -> 02:56:14]  Direction uh and then we as we want to
[02:56:13 -> 02:56:17]  this is actually very this is actually
[02:56:14 -> 02:56:19]  very uh this is required you actually
[02:56:17 -> 02:56:21]  have this you need this if statement
[02:56:19 -> 02:56:23]  here because if things go off track or
[02:56:21 -> 02:56:25]  if you have like too many threads then
[02:56:23 -> 02:56:27]  they might go and compute values that
[02:56:25 -> 02:56:29]  you don't want like it might go access
[02:56:27 -> 02:56:30]  other parts in memory it's not
[02:56:29 -> 02:56:32]  restrained right so it's not going to
[02:56:30 -> 02:56:33]  stop when you think it should stop you
[02:56:32 -> 02:56:37]  actually need to put careful restraints
[02:56:33 -> 02:56:41]  on it and say okay well we we want to
[02:56:37 -> 02:56:43]  stop it once the row gets to M because
[02:56:41 -> 02:56:45]  there's no other values outside of that
[02:56:43 -> 02:56:49]  and then same for column as well right
[02:56:45 -> 02:56:55]  so um like when we go up here we
[02:56:49 -> 02:56:57]  have we have M by n right so m is the
[02:56:55 -> 02:57:05]  um m is this
[02:56:57 -> 02:57:09]  part which is the uh where did it go
[02:57:05 -> 02:57:13]  yes m is row so row is y right this this
[02:57:09 -> 02:57:16]  y this height part and then n is is that
[02:57:13 -> 02:57:18]  uh the the width the horizontal part X
[02:57:16 -> 02:57:20]  and so that's that's columns which to X
[02:57:18 -> 02:57:21]  right and so you have to this is just
[02:57:20 -> 02:57:24]  the kind of thing that you have to be
[02:57:21 -> 02:57:26]  careful with um Cuda handles this very
[02:57:24 -> 02:57:29]  well but you just have to include this
[02:57:26 -> 02:57:31]  if statement and then you essentially
[02:57:29 -> 02:57:34]  for each thread um because this is
[02:57:31 -> 02:57:38]  itself in a thread you're going to uh
[02:57:34 -> 02:57:39]  just do a essentially a a DOT product
[02:57:38 -> 02:57:44]  between elements and you're you're going
[02:57:39 -> 02:57:47]  to do like essentially a a row of a row
[02:57:44 -> 02:57:48]  of a and a column of of B and this is
[02:57:47 -> 02:57:50]  going to be done per thread
[02:57:48 -> 02:57:52]  so each different thread is going to
[02:57:50 -> 02:57:55]  have a different maybe a different uh
[02:57:52 -> 02:57:59]  row and a different column of B to to to
[02:57:55 -> 02:58:02]  compute so you have this um you have
[02:57:59 -> 02:58:03]  this K term which is from here um and
[02:58:02 -> 02:58:05]  you're you're cycling through that and
[02:58:03 -> 02:58:08]  you you just essentially apply the same
[02:58:05 -> 02:58:10]  wrapping but instead of worrying about
[02:58:08 -> 02:58:14]  all of these nested for Loops you worry
[02:58:10 -> 02:58:16]  instead about um the rows and columns so
[02:58:14 -> 02:58:18]  these are your actual uh these are the
[02:58:16 -> 02:58:21]  you know the way we index with threads
[02:58:18 -> 02:58:24]  as I was talking about before
[02:58:21 -> 02:58:26]  um but yeah this is uh I'm going to dig
[02:58:24 -> 02:58:28]  more into sort of the in intuition
[02:58:26 -> 02:58:30]  behind this later in the course when we
[02:58:28 -> 02:58:33]  end up optimizing matrix multiplication
[02:58:30 -> 02:58:36]  this uh this is this is called a naive
[02:58:33 -> 02:58:38]  kernel it's it's very it's very limited
[02:58:36 -> 02:58:42]  it doesn't have a ton of optimizations
[02:58:38 -> 02:58:44]  it's not like it's not fast it is like
[02:58:42 -> 02:58:46]  it's like aunds the speed of what
[02:58:44 -> 02:58:48]  state-ofthe-art is it's actually quite
[02:58:46 -> 02:58:50]  slow comparatively um and we're going to
[02:58:48 -> 02:58:52]  optimize this later on and this is going
[02:58:50 -> 02:58:53]  to be the most intuitive thing you will
[02:58:52 -> 02:58:56]  probably learn in this entire course is
[02:58:53 -> 02:58:58]  matrix multiplication in Cuda so don't
[02:58:56 -> 02:59:01]  worry if it this doesn't entirely click
[02:58:58 -> 02:59:03]  right now um just kind of worry about
[02:59:01 -> 02:59:05]  where these threads are how we're
[02:59:03 -> 02:59:07]  getting how we're getting the row and
[02:59:05 -> 02:59:09]  column values and then this wrapping
[02:59:07 -> 02:59:13]  that we have here and then the the
[02:59:09 -> 02:59:18]  offset part right wrapping and
[02:59:13 -> 02:59:19]  offset offset um and yeah that's that
[02:59:18 -> 02:59:23]  that's pretty much all you have to worry
[02:59:19 -> 02:59:25]  about for now um and then we just do you
[02:59:23 -> 02:59:28]  know the same route perform warm-up runs
[02:59:25 -> 02:59:31]  Benchmark it across 20 benchmarks or
[02:59:28 -> 02:59:34]  across 20 runs um Benchmark CPU versus
[02:59:31 -> 02:59:37]  GPU and then return the average time in
[02:59:34 -> 02:59:40]  micros seconds so if I just uh open up a
[02:59:37 -> 02:59:48]  terminal here and go
[02:59:40 -> 02:59:51]  nbcc out to two and we go and run that
[02:59:48 -> 02:59:54]  forming R up
[02:59:51 -> 02:59:55]  runs okay this is very I actually made
[02:59:54 -> 02:59:59]  very large matricies maybe we should
[02:59:55 -> 03:00:01]  shrink these a little bit
[02:59:59 -> 03:00:05]  um we can
[03:00:01 -> 03:00:08]  go let's see maybe
[03:00:05 -> 03:00:08]  256
[03:00:10 -> 03:00:15]  512
[03:00:13 -> 03:00:16]  256 yeah the CPU is not going to like
[03:00:15 -> 03:00:19]  that
[03:00:16 -> 03:00:22]  one uh and so it's benchmarking CPU and
[03:00:19 -> 03:00:25]  so that takes 89,000 micros and this
[03:00:22 -> 03:00:28]  takes 88 microc so we get just like out
[03:00:25 -> 03:00:34]  of the box with these small matricies we
[03:00:28 -> 03:00:38]  get a 1,000x speed up with using Cuda
[03:00:34 -> 03:00:40]  um and that's that so this is uh this is
[03:00:38 -> 03:00:42]  this is uh this is kind of how we test
[03:00:40 -> 03:00:44]  stuff but yeah now we're going to go
[03:00:42 -> 03:00:46]  ahead and jump into like how do you
[03:00:44 -> 03:00:49]  profile these um I know we haven't gone
[03:00:46 -> 03:00:51]  extensively into um like how Cuda
[03:00:49 -> 03:00:56]  actually works under the hood completely
[03:00:51 -> 03:00:57]  there's still more to do but um in a
[03:00:56 -> 03:01:00]  little bit we're going to hit up
[03:00:57 -> 03:01:01]  profiling I would like to cover uh
[03:01:00 -> 03:01:03]  actually some more stuff before we do
[03:01:01 -> 03:01:06]  that let me close this
[03:01:03 -> 03:01:12]  out we pop into the read me here just
[03:01:06 -> 03:01:17]  close this close this um going
[03:01:12 -> 03:01:17]  to just zoom out a little bit
[03:01:18 -> 03:01:24]  sure so again we have these these dim
[03:01:22 -> 03:01:26]  these dim three types um which I was
[03:01:24 -> 03:01:27]  talking about before these should make
[03:01:26 -> 03:01:30]  sense already these these should not be
[03:01:27 -> 03:01:33]  like too hard to grasp
[03:01:30 -> 03:01:36]  um this is what it normally looks like
[03:01:33 -> 03:01:39]  right you put in you put this
[03:01:36 -> 03:01:42]  in you put this in like I said before
[03:01:39 -> 03:01:44]  it's going to simplify to a dim three so
[03:01:42 -> 03:01:47]  this is going to look like a 16 by one
[03:01:44 -> 03:01:49]  by one uh tensor you could say and it's
[03:01:47 -> 03:01:50]  going to add that to the kernel launch
[03:01:49 -> 03:01:52]  configuration this is the kernel launch
[03:01:50 -> 03:01:55]  configuration there's more stuff we can
[03:01:52 -> 03:01:59]  add to it um already covered this stuff
[03:01:55 -> 03:02:02]  already uh and then you have more stuff
[03:01:59 -> 03:02:06]  you can add to it um so we have the grid
[03:02:02 -> 03:02:10]  dim the grid dims uh in you know 1 to 3D
[03:02:06 -> 03:02:13]  block di in 1 to 3D and then this uh NS
[03:02:10 -> 03:02:16]  so this is the number of bytes in shared
[03:02:13 -> 03:02:18]  memory that is allocated per block for
[03:02:16 -> 03:02:22]  this call um so you're going to
[03:02:18 -> 03:02:24]  explicitly allocate memory for a block
[03:02:22 -> 03:02:27]  um in shared memory which is really fast
[03:02:24 -> 03:02:31]  so typically you would omit this uh
[03:02:27 -> 03:02:32]  however if you have a specific uh
[03:02:31 -> 03:02:34]  production like you're trying to deploy
[03:02:32 -> 03:02:36]  a CTIC kernel in production to run
[03:02:34 -> 03:02:38]  something really really fast you might
[03:02:36 -> 03:02:40]  actually want to capitalize off of this
[03:02:38 -> 03:02:41]  because it'll give you more explicit
[03:02:40 -> 03:02:43]  control over what happens and you can
[03:02:41 -> 03:02:45]  measure performance a bit better and
[03:02:43 -> 03:02:46]  you'll get maybe get some some little
[03:02:45 -> 03:02:49]  some little performance gains out of
[03:02:46 -> 03:02:52]  that and then there's this s term which
[03:02:49 -> 03:02:54]  is uh the stream it's in and I'm going
[03:02:52 -> 03:02:55]  to cover streams actually number five so
[03:02:54 -> 03:02:57]  don't worry about this too much but but
[03:02:55 -> 03:03:00]  streams are pretty cool they let you do
[03:02:57 -> 03:03:02]  some some interesting stuff
[03:03:00 -> 03:03:05]  um and then this I didn't talk about
[03:03:02 -> 03:03:08]  this entirely too much Cuda device
[03:03:05 -> 03:03:10]  synchronize and sync threads so Cuda
[03:03:08 -> 03:03:13]  device synchronize ensures all the
[03:03:10 -> 03:03:17]  kernels or all of the uh threads for one
[03:03:13 -> 03:03:19]  problem are all of the like all of the
[03:03:17 -> 03:03:21]  all the different parallel computations
[03:03:19 -> 03:03:23]  for a problem are done before you begin
[03:03:21 -> 03:03:25]  the next so when you when you launch a
[03:03:23 -> 03:03:26]  kernel it's going to have a bunch of
[03:03:25 -> 03:03:30]  blocks in parel and a bunch of threads
[03:03:26 -> 03:03:31]  in parallel run this massive problem um
[03:03:30 -> 03:03:33]  and they might not all finish at the
[03:03:31 -> 03:03:36]  same time like some of them just like do
[03:03:33 -> 03:03:38]  to physics they're they might like not
[03:03:36 -> 03:03:41]  finish at the exact same time and so you
[03:03:38 -> 03:03:43]  have to explicitly synchronize them you
[03:03:41 -> 03:03:47]  have to add this little barrier this
[03:03:43 -> 03:03:48]  this ume essentially preventing a a race
[03:03:47 -> 03:03:52]  condition
[03:03:48 -> 03:03:54]  so if you have a bunch of threads um
[03:03:52 -> 03:03:56]  like for example in this one when you're
[03:03:54 -> 03:03:57]  bit shifting when you're like moving
[03:03:56 -> 03:03:59]  this one over here and then this one
[03:03:57 -> 03:04:01]  over here and then this one over here
[03:03:59 -> 03:04:03]  it's like well ideally you'd want to do
[03:04:01 -> 03:04:05]  this in a certain order and not like
[03:04:03 -> 03:04:08]  store something before it's not supposed
[03:04:05 -> 03:04:11]  to be stored like if um for example if I
[03:04:08 -> 03:04:13]  do this one um and then this one is
[03:04:11 -> 03:04:15]  supposed to happen after but it ends up
[03:04:13 -> 03:04:18]  doing this one first because we didn't
[03:04:15 -> 03:04:20]  synchronize properly um you could you'll
[03:04:18 -> 03:04:22]  end up with the wrong answer right so
[03:04:20 -> 03:04:24]  you have to purposely synchronize the
[03:04:22 -> 03:04:26]  thread so that all of them regardless of
[03:04:24 -> 03:04:27]  like this one might be like way ahead
[03:04:26 -> 03:04:29]  you have to wait for all the other ones
[03:04:27 -> 03:04:31]  to catch up in order for them to hit the
[03:04:29 -> 03:04:33]  same spot so you say okay this one's
[03:04:31 -> 03:04:35]  done but these ones aren't we're going
[03:04:33 -> 03:04:36]  to wait for all these to synchronize up
[03:04:35 -> 03:04:39]  together and then we can continue the
[03:04:36 -> 03:04:41]  next step right that's what uh Cuda
[03:04:39 -> 03:04:44]  device synchronize synchronize will do
[03:04:41 -> 03:04:47]  uh after you typically put this after
[03:04:44 -> 03:04:50]  launching a kernel and then sync threads
[03:04:47 -> 03:04:52]  is put with in a kernel um for threat
[03:04:50 -> 03:04:54]  execution inside of it so one is like
[03:04:52 -> 03:04:56]  out like when you're trying to
[03:04:54 -> 03:04:57]  synchronize the whole grid and then one
[03:04:56 -> 03:05:00]  is like synchronize all the threads
[03:04:57 -> 03:05:04]  within a within like
[03:05:00 -> 03:05:06]  a within a warp so as you might have
[03:05:04 -> 03:05:07]  been able to tell I was a little bit
[03:05:06 -> 03:05:09]  unsure about that last answer so I
[03:05:07 -> 03:05:12]  decided to look it up and sync threads
[03:05:09 -> 03:05:14]  is actually on the level of uh thread
[03:05:12 -> 03:05:18]  blocks instead of warps so you can do
[03:05:14 -> 03:05:21]  syn warps instead of sync threads
[03:05:18 -> 03:05:24]  actually pop back to here and we go at
[03:05:21 -> 03:05:30]  sync the sync threads you can actually
[03:05:24 -> 03:05:30]  do um if you want to do warps you can
[03:05:30 -> 03:05:36]  do stin
[03:05:34 -> 03:05:39]  warps
[03:05:36 -> 03:05:41]  um to sync all of the threads within a
[03:05:39 -> 03:05:43]  warp and then this one will do that it
[03:05:41 -> 03:05:46]  the same thing but a thread block
[03:05:43 -> 03:05:46]  instead
[03:05:51 -> 03:05:58]  all reds within a
[03:05:56 -> 03:06:01]  war
[03:05:58 -> 03:06:02]  um and then this is for an entire thread
[03:06:01 -> 03:06:04]  block so just just a piece of
[03:06:02 -> 03:06:08]  clarification there one other cool thing
[03:06:04 -> 03:06:11]  I came across when uh studying Cuda is
[03:06:08 -> 03:06:12]  how you can actually add in uh explicit
[03:06:11 -> 03:06:16]  flags and you can you can actually
[03:06:12 -> 03:06:18]  convert something like a log to log f
[03:06:16 -> 03:06:19]  using compiler Flags
[03:06:18 -> 03:06:23]  um and I know that there's a little bit
[03:06:19 -> 03:06:26]  to unpack there but if I just go back to
[03:06:23 -> 03:06:28]  uh this compilation here actually no we
[03:06:26 -> 03:06:30]  don't even use we don't even use any of
[03:06:28 -> 03:06:32]  those M functions but if I were to say
[03:06:30 -> 03:06:34]  do like log inside of a kernel um that
[03:06:32 -> 03:06:36]  would go slower than if I were to use
[03:06:34 -> 03:06:39]  log F so log f is like a device
[03:06:36 -> 03:06:42]  operation and log is a host operation so
[03:06:39 -> 03:06:45]  designed to run on CPU on CPU course
[03:06:42 -> 03:06:48]  right um so we can actually do do use
[03:06:45 -> 03:06:51]  fast math as a part of the compiler
[03:06:48 -> 03:06:56]  Flags I can go um
[03:06:51 -> 03:06:56]  use use fast math like
[03:06:57 -> 03:07:01]  this and of course we won't really see
[03:06:59 -> 03:07:06]  any difference
[03:07:01 -> 03:07:08]  but um yeah like same 1006 X same thing
[03:07:06 -> 03:07:10]  um but this will actually convert this
[03:07:08 -> 03:07:13]  to this in case you don't in case you
[03:07:10 -> 03:07:15]  haven't done that uh yet on your own so
[03:07:13 -> 03:07:20]  this actually comes from the Cuda math
[03:07:15 -> 03:07:28]  API reference manual so uh if we look at
[03:07:20 -> 03:07:32]  say like some of the single Precision uh
[03:07:28 -> 03:07:34]  intrinsics yeah so uh single Precision
[03:07:32 -> 03:07:36]  intrinsic functions that are supported
[03:07:34 -> 03:07:39]  only in device
[03:07:36 -> 03:07:44]  code right notice how it has like Co F
[03:07:39 -> 03:07:50]  uh x uh uh exponentiate with base 10 f
[03:07:44 -> 03:07:52]  expf um and then like you know F add um
[03:07:50 -> 03:07:53]  round toward zero right all of these
[03:07:52 -> 03:07:57]  These are these are designed to execute
[03:07:53 -> 03:08:00]  on device um and they have F at the end
[03:07:57 -> 03:08:03]  but if you were to just do like just
[03:08:00 -> 03:08:06]  Coast for example from the math.h librar
[03:08:03 -> 03:08:08]  and C that wouldn't that wouldn't run as
[03:08:06 -> 03:08:09]  fast so this is another little thing you
[03:08:08 -> 03:08:12]  could add to your kernels if you're
[03:08:09 -> 03:08:14]  trying to say do like um if you're
[03:08:12 -> 03:08:16]  trying to do like soft Max or something
[03:08:14 -> 03:08:18]  in a kernel or if you're trying to um
[03:08:16 -> 03:08:20]  maybe do like uh like some Digital
[03:08:18 -> 03:08:24]  Signal processing right you can add
[03:08:20 -> 03:08:28]  these and and get uh some benefits and
[03:08:24 -> 03:08:30]  performance- wise out of those um and
[03:08:28 -> 03:08:32]  same thing here like if you wanted to do
[03:08:30 -> 03:08:35]  a fuse multiply ad um this will like
[03:08:32 -> 03:08:38]  tell the actual uh this will
[03:08:35 -> 03:08:40]  actually like pour this into the
[03:08:38 -> 03:08:43]  instructions where instead of doing like
[03:08:40 -> 03:08:46]  separate uh multiply and add operations
[03:08:43 -> 03:08:47]  you're fusing them together so you can
[03:08:46 -> 03:08:51]  do little tricks like this and just to
[03:08:47 -> 03:08:54]  speed things up performance wise but
[03:08:51 -> 03:08:57]  uh yeah now we can uh now we can
[03:08:54 -> 03:08:59]  actually dive into uh
[03:08:57 -> 03:09:01]  profiling I actually forgot to do the
[03:08:59 -> 03:09:03]  tiled matrix multiplication by hand so I
[03:09:01 -> 03:09:05]  figured I'll just squeeze this in now
[03:09:03 -> 03:09:06]  and and let your mind sit on this for a
[03:09:05 -> 03:09:09]  little bit before we actually start
[03:09:06 -> 03:09:12]  using it and applying it um but before
[03:09:09 -> 03:09:15]  we had this this idea of a matrix
[03:09:12 -> 03:09:17]  multiplication which was um you have
[03:09:15 -> 03:09:20]  like a
[03:09:17 -> 03:09:25]  you have like a matrix
[03:09:20 -> 03:09:28]  a and can you see that maybe not I'm
[03:09:25 -> 03:09:28]  going to move this
[03:09:33 -> 03:09:41]  down switch Mark
[03:09:36 -> 03:09:41]  here a matrix
[03:09:44 -> 03:09:49]  a you with some numbers in it maybe
[03:09:51 -> 03:09:57]  and B and the whole idea here is we do
[03:09:55 -> 03:10:02]  product this with
[03:09:57 -> 03:10:05]  this this with this this with this do
[03:10:02 -> 03:10:09]  the same thing and we bring it down
[03:10:05 -> 03:10:10]  here right all the way till the end um
[03:10:09 -> 03:10:13]  that is one way to do matrix
[03:10:10 -> 03:10:15]  multiplication however you can actually
[03:10:13 -> 03:10:17]  make this more efficient by using
[03:10:15 -> 03:10:19]  something called piling
[03:10:17 -> 03:10:21]  so I'll provide some examples on this
[03:10:19 -> 03:10:24]  later in the course but this is the idea
[03:10:21 -> 03:10:27]  here
[03:10:24 -> 03:10:31]  um you have these uh you have these two
[03:10:27 -> 03:10:33]  matrices A and B so I'm just going to
[03:10:31 -> 03:10:35]  you have to look at this a little bit
[03:10:33 -> 03:10:38]  different
[03:10:35 -> 03:10:41]  but this is what it looks
[03:10:38 -> 03:10:45]  like
[03:10:41 -> 03:10:48]  so we have say
[03:10:45 -> 03:10:52]  um let's just say this is a and this is
[03:10:48 -> 03:10:53]  B okay and then you have this C Matrix
[03:10:52 -> 03:10:55]  and how do you compute like the first
[03:10:53 -> 03:10:57]  element right well you would you would
[03:10:55 -> 03:10:58]  typically take this row and then this
[03:10:57 -> 03:10:59]  column and then you would put that there
[03:10:58 -> 03:11:03]  because that's where they intersect
[03:10:59 -> 03:11:05]  right um but what you can do is you can
[03:11:03 -> 03:11:08]  actually take a chunk you can take a
[03:11:05 -> 03:11:12]  chunk like an actual square or rectangle
[03:11:08 -> 03:11:14]  of a so like maybe this I just put this
[03:11:12 -> 03:11:16]  into like separate
[03:11:14 -> 03:11:18]  pieces say this is like a like you know
[03:11:16 -> 03:11:22]  maybe a a 9
[03:11:18 -> 03:11:26]  by9 right and this is also a 9
[03:11:22 -> 03:11:29]  by so technically each this each of
[03:11:26 -> 03:11:31]  these is technically like a 3X3 tile or
[03:11:29 -> 03:11:35]  a matrix on its own right and so we're
[03:11:31 -> 03:11:35]  just splitting up splitting this up into
[03:11:35 -> 03:11:40]  tiles and so what you can do here
[03:11:42 -> 03:11:51]  is you can as I've lined out here you
[03:11:46 -> 03:11:54]  can you can go one time you could do a
[03:11:51 -> 03:11:57]  Matrix Matrix here times The Matrix
[03:11:54 -> 03:12:01]  there um Like A and B respectively like
[03:11:57 -> 03:12:04]  you do a * b um and then you add that to
[03:12:01 -> 03:12:09]  the Matrix multiply of two and two A and
[03:12:04 -> 03:12:12]  B respectively and then three and
[03:12:09 -> 03:12:14]  three you start with these and then you
[03:12:12 -> 03:12:17]  then you add to these and you add to
[03:12:14 -> 03:12:20]  these so it's like
[03:12:17 -> 03:12:23]  A1 A1 *
[03:12:20 -> 03:12:27]  B1 you multiply those and then you add
[03:12:23 -> 03:12:30]  it to A2 * B2 and then add that to A3 *
[03:12:27 -> 03:12:33]  B3 and then you end up with this with
[03:12:30 -> 03:12:34]  this C1 here and that's the output and
[03:12:33 -> 03:12:37]  this is exactly what I've written out in
[03:12:34 -> 03:12:41]  a sort of cube format is like you've
[03:12:37 -> 03:12:43]  you've laid out some Matrix a right here
[03:12:41 -> 03:12:47]  um which is like a a row and then you've
[03:12:43 -> 03:12:49]  laid out some M Matrix B here um and
[03:12:47 -> 03:12:51]  you're just you're
[03:12:49 -> 03:12:53]  doing this times this and then add to
[03:12:51 -> 03:12:55]  this times this and then add to this
[03:12:53 -> 03:12:58]  times this um and that's and then you
[03:12:55 -> 03:13:00]  just end up with C1 and so what you can
[03:12:58 -> 03:13:03]  do with like the reason why this is so
[03:13:00 -> 03:13:05]  effective is because you can you can
[03:13:03 -> 03:13:07]  actually put these tiles and you can pop
[03:13:05 -> 03:13:09]  them over to a faster memory like like
[03:13:07 -> 03:13:11]  shared memory uh and then they'll end up
[03:13:09 -> 03:13:14]  running like ridiculously fast and you
[03:13:11 -> 03:13:16]  can end up doing these computations like
[03:13:14 -> 03:13:17]  way faster so if you split it into
[03:13:16 -> 03:13:19]  little tiles
[03:13:17 -> 03:13:21]  and let each little like streaming
[03:13:19 -> 03:13:23]  multiprocessor on the on the chip
[03:13:21 -> 03:13:27]  actually take care of the individual uh
[03:13:23 -> 03:13:29]  tile or multiple tiles um then you can
[03:13:27 -> 03:13:33]  actually get a lot more useful uh you
[03:13:29 -> 03:13:35]  get a lot more a lot higher uh compute
[03:13:33 -> 03:13:37]  throughput you could say um but don't
[03:13:35 -> 03:13:39]  worry about this too extensively this is
[03:13:37 -> 03:13:41]  just the intuition behind tiling like
[03:13:39 -> 03:13:42]  the difference between this and the
[03:13:41 -> 03:13:45]  normal version we were doing where we
[03:13:42 -> 03:13:46]  like take a whole row and then we take a
[03:13:45 -> 03:13:47]  whole column and then we dot product
[03:13:46 -> 03:13:51]  them together
[03:13:47 -> 03:13:52]  this is different than that so that I
[03:13:51 -> 03:13:53]  just wanted to put that in your head for
[03:13:52 -> 03:13:57]  later so that it's not a complete
[03:13:53 -> 03:13:57]  surprise when we try to make this
[03:13:58 -> 03:14:04]  faster now we dig into how can we
[03:14:01 -> 03:14:07]  actually profile the performance metrics
[03:14:04 -> 03:14:09]  of our own kernels so how do we optimize
[03:14:07 -> 03:14:12]  these right and we're going to use
[03:14:09 -> 03:14:13]  Nvidia andite compute for this um if
[03:14:12 -> 03:14:15]  you're on Windows you might you might
[03:14:13 -> 03:14:16]  not have this it might look a bit
[03:14:15 -> 03:14:18]  different I haven't tried it on Windows
[03:14:16 -> 03:14:20]  yet but this is what we're going to use
[03:14:18 -> 03:14:22]  on Linux here so this is kind of what it
[03:14:20 -> 03:14:24]  looks like at the end you can see a
[03:14:22 -> 03:14:27]  bunch of details about things um it's
[03:14:24 -> 03:14:29]  very very interesting but we're going to
[03:14:27 -> 03:14:32]  dig into this in a second here just
[03:14:29 -> 03:14:35]  going to close these off and uh we'll go
[03:14:32 -> 03:14:39]  ahead and get started so let me close
[03:14:35 -> 03:14:41]  these here we'll see in this in this uh
[03:14:39 -> 03:14:43]  number five kernels chapter in profiling
[03:14:41 -> 03:14:45]  we have a bunch of files so we're going
[03:14:43 -> 03:14:48]  to start off with this one the mvtx
[03:14:45 -> 03:14:50]  matmo so what is what the heck is mvtx
[03:14:48 -> 03:14:51]  you guys already know what matrix
[03:14:50 -> 03:14:56]  multiplication is I'm not going to go
[03:14:51 -> 03:15:00]  over that nvx is like the the custom
[03:14:56 -> 03:15:02]  profiler for uh Kudo kernels right and
[03:15:00 -> 03:15:03]  what this allows you to do is it's
[03:15:02 -> 03:15:06]  actually quite straightforward if you
[03:15:03 -> 03:15:07]  look at what's happening here like it's
[03:15:06 -> 03:15:09]  it actually makes a lot of sense what's
[03:15:07 -> 03:15:10]  happening so we're able to push this
[03:15:09 -> 03:15:14]  into a range like essentially the
[03:15:10 -> 03:15:16]  timeline um push matrix multiplication
[03:15:14 -> 03:15:18]  push memory allocation right so we're
[03:15:16 -> 03:15:20]  doing the
[03:15:18 -> 03:15:23]  um this is the whole this is the whole
[03:15:20 -> 03:15:26]  matrix multiplication thing from start
[03:15:23 -> 03:15:28]  to finish um we push things into a range
[03:15:26 -> 03:15:34]  so memory allocation and then we pop
[03:15:28 -> 03:15:36]  that we pop that out um we copy pop that
[03:15:34 -> 03:15:38]  out so it's like start and finish uh and
[03:15:36 -> 03:15:41]  then we we do our our dim
[03:15:38 -> 03:15:43]  threes we start kernel execution and
[03:15:41 -> 03:15:44]  then it's going to stop that once we've
[03:15:43 -> 03:15:47]  launched the kernel run it and then
[03:15:44 -> 03:15:49]  synchronize all of our um like our
[03:15:47 -> 03:15:53]  everything in our
[03:15:49 -> 03:15:54]  grid and then copy back to host right uh
[03:15:53 -> 03:15:57]  so this is like very straightforward
[03:15:54 -> 03:15:59]  literally all you need so I mean keep in
[03:15:57 -> 03:16:01]  mind like when we start this one we we
[03:15:59 -> 03:16:02]  have another one afterwards so this one
[03:16:01 -> 03:16:04]  is only going to Target the recent one
[03:16:02 -> 03:16:06]  that was put up right so it's not going
[03:16:04 -> 03:16:08]  to jump back to the first one that was
[03:16:06 -> 03:16:10]  ever pushed in it's going to be like
[03:16:08 -> 03:16:12]  kind of uh like brackets right so you
[03:16:10 -> 03:16:14]  have the uh one layer of brackets on the
[03:16:12 -> 03:16:16]  outside and then one on the inside it's
[03:16:14 -> 03:16:20]  like um it kind of that that's kind of
[03:16:16 -> 03:16:23]  the structure of these of this nbtx tool
[03:16:20 -> 03:16:28]  um so when we go ahead and
[03:16:23 -> 03:16:32]  nbcc um this we want to pass in the uh
[03:16:28 -> 03:16:34]  link NV tools extension that's what mvtx
[03:16:32 -> 03:16:36]  stands for so Nvidia tools extension
[03:16:34 -> 03:16:38]  we're going to compile that we can go
[03:16:36 -> 03:16:41]  ahead and you know run this it'll it'll
[03:16:38 -> 03:16:43]  run as expected good um and then we can
[03:16:41 -> 03:16:47]  actually if we pop back to this read me
[03:16:43 -> 03:16:51]  file um we can do NYS profile
[03:16:47 -> 03:16:56]  uh stats equals true on the um it's not
[03:16:51 -> 03:16:56]  mammal it's 0 but if we go ahead and run
[03:16:57 -> 03:17:03]  this we will notice that there's a bunch
[03:17:00 -> 03:17:06]  of cool stats that pop up now if you're
[03:17:03 -> 03:17:07]  running from a remote machine you could
[03:17:06 -> 03:17:09]  you could use this you could just look
[03:17:07 -> 03:17:13]  at this directly from the terminal
[03:17:09 -> 03:17:14]  however the uh Nvidia ight compute app
[03:17:13 -> 03:17:17]  itself is actually a bit more
[03:17:14 -> 03:17:20]  informative than this so
[03:17:17 -> 03:17:24]  what we can do is is type your Windows
[03:17:20 -> 03:17:27]  key and go windows and then type ncu and
[03:17:24 -> 03:17:29]  then press enter and it should bring up
[03:17:27 -> 03:17:32]  um eni compute and
[03:17:29 -> 03:17:34]  it it it popped up on my second monitor
[03:17:32 -> 03:17:36]  I just brought it over here but this is
[03:17:34 -> 03:17:40]  what it should look like um and what you
[03:17:36 -> 03:17:42]  can do from here is um I'm just going to
[03:17:40 -> 03:17:47]  put this on the second one and then drag
[03:17:42 -> 03:17:49]  the uh report NIS rep file so not the SQ
[03:17:47 -> 03:17:51]  light the SQ light is for for a
[03:17:49 -> 03:17:55]  different thing but we drag this uh into
[03:17:51 -> 03:17:55]  the into the sidebar
[03:17:55 -> 03:17:59]  here now it's in we can see it at the
[03:17:58 -> 03:18:00]  top and now there's a bunch of
[03:17:59 -> 03:18:02]  interesting things in here that we can
[03:18:00 -> 03:18:03]  look at so this this text might be a
[03:18:02 -> 03:18:06]  little small if you're on a phone but
[03:18:03 -> 03:18:09]  just bear with me here so we have bunch
[03:18:06 -> 03:18:10]  of stuff on threads um you know nbtx
[03:18:09 -> 03:18:12]  what is like what is happening
[03:18:10 -> 03:18:16]  sequentially here we can actually zoom
[03:18:12 -> 03:18:17]  in and see um you know the memory copy
[03:18:16 -> 03:18:19]  kernel execution takes about 2
[03:18:17 -> 03:18:21]  milliseconds and we can see everything
[03:18:19 -> 03:18:22]  right so all these are actually pushed
[03:18:21 -> 03:18:25]  into the range and we can see what's
[03:18:22 -> 03:18:27]  happening um and then of course the you
[03:18:25 -> 03:18:28]  know the memory allocation takes a while
[03:18:27 -> 03:18:30]  uh and then the matrix multiplication
[03:18:28 -> 03:18:32]  from start to finish like we highlighted
[03:18:30 -> 03:18:34]  in the code um so that's that's how
[03:18:32 -> 03:18:36]  that's what mvtx does you can push
[03:18:34 -> 03:18:38]  things into a range and you can see how
[03:18:36 -> 03:18:40]  long it actually takes you can see like
[03:18:38 -> 03:18:42]  when it's happening on the timeline and
[03:18:40 -> 03:18:45]  you can look more more in more detail as
[03:18:42 -> 03:18:50]  to like what's happening there right so
[03:18:45 -> 03:18:53]  um anyways if we go to the Cuda Hardware
[03:18:50 -> 03:18:55]  at the top here we can see uh it
[03:18:53 -> 03:18:58]  consists of kernels and memory so
[03:18:55 -> 03:19:01]  there's like um copying so cud M Copy
[03:18:58 -> 03:19:04]  and then there's the Matrix M kernel
[03:19:01 -> 03:19:07]  that we can see here and if we click on
[03:19:04 -> 03:19:10]  this we go show in events view we can
[03:19:07 -> 03:19:11]  click on this down here Zoom to selected
[03:19:10 -> 03:19:15]  on
[03:19:11 -> 03:19:17]  timeline we can rightclick we can go
[03:19:15 -> 03:19:18]  profile kernel
[03:19:17 -> 03:19:20]  and there's a bunch of interesting
[03:19:18 -> 03:19:21]  things here and this might be a little
[03:19:20 -> 03:19:25]  might be a little overwhelming at first
[03:19:21 -> 03:19:27]  but there's common filter metrics PM
[03:19:25 -> 03:19:29]  sampling warp sampling other so we're
[03:19:27 -> 03:19:32]  just going to use PM sampling right now
[03:19:29 -> 03:19:33]  um PM sampling is performance metric
[03:19:32 -> 03:19:36]  sampling so it's going to give us very
[03:19:33 -> 03:19:39]  detailed metrics about things and we'll
[03:19:36 -> 03:19:41]  be able to optimize from that so it's
[03:19:39 -> 03:19:44]  going to use this binary 0 file that we
[03:19:41 -> 03:19:46]  that we made before during compilation
[03:19:44 -> 03:19:47]  um and it's going to bring up this new
[03:19:46 -> 03:19:50]  menu here which is different than the
[03:19:47 -> 03:19:53]  timeline one um so this timeline view
[03:19:50 -> 03:19:54]  and then this is different so in here um
[03:19:53 -> 03:19:56]  you know we can see all of our Kernels
[03:19:54 -> 03:19:58]  at the top so in case we were like maybe
[03:19:56 -> 03:19:59]  profiling two different matrix
[03:19:58 -> 03:20:01]  multiplication kernels they they might
[03:19:59 -> 03:20:03]  both show up here like the the the
[03:20:01 -> 03:20:05]  runtime the lifetime of our program that
[03:20:03 -> 03:20:09]  that's what would pop up here uh and all
[03:20:05 -> 03:20:11]  the kernels inside of that so uh if we
[03:20:09 -> 03:20:12]  go to you know say summary there's
[03:20:11 -> 03:20:13]  there's some interesting stuff here
[03:20:12 -> 03:20:17]  maybe we don't maybe we don't care about
[03:20:13 -> 03:20:20]  this too much there's there's details um
[03:20:17 -> 03:20:21]  so you have uh throughput so overview of
[03:20:20 -> 03:20:25]  throughput for compute and memory
[03:20:21 -> 03:20:27]  resources um PM sampling so uh
[03:20:25 -> 03:20:29]  performance metrics we can bring this
[03:20:27 -> 03:20:34]  down and we can see things like SM
[03:20:29 -> 03:20:36]  throughput uh pipe throughput um a bunch
[03:20:34 -> 03:20:38]  of the a bunch of metrics I don't even
[03:20:36 -> 03:20:40]  understand yet but uh we have things
[03:20:38 -> 03:20:43]  like cach hit rate which is really which
[03:20:40 -> 03:20:46]  is really useful um but if we go to like
[03:20:43 -> 03:20:47]  speed of light throughput for example um
[03:20:46 -> 03:20:49]  this is this this is for the memory
[03:20:47 -> 03:20:54]  resources we have the compute throughput
[03:20:49 -> 03:20:56]  as a percent so that's at about 90 97%
[03:20:54 -> 03:21:00]  and then memory is also at about
[03:20:56 -> 03:21:02]  97% so um you know we we get we get to
[03:21:00 -> 03:21:04]  see cool things like this and and it'll
[03:21:02 -> 03:21:05]  make more sense in a second here we go
[03:21:04 -> 03:21:09]  down
[03:21:05 -> 03:21:11]  to uh memory workload analysis we can
[03:21:09 -> 03:21:15]  see memory throughput in it like very
[03:21:11 -> 03:21:18]  detailed memory uh I guess memory
[03:21:15 -> 03:21:20]  metrics so gigabytes per second how much
[03:21:18 -> 03:21:24]  how much are we able to transfer back
[03:21:20 -> 03:21:27]  and forth right bytes um Dam bytes per
[03:21:24 -> 03:21:31]  second so that GPU vram how how fast are
[03:21:27 -> 03:21:35]  we accessing that um and that that speed
[03:21:31 -> 03:21:38]  is about 41 gabes per second which um
[03:21:35 -> 03:21:42]  which is not super high and then we have
[03:21:38 -> 03:21:44]  like uh L1 hit rates L2 hit rates all
[03:21:42 -> 03:21:45]  this and we can see a memory chart here
[03:21:44 -> 03:21:50]  there's just like a whole bunch of
[03:21:45 -> 03:21:53]  metrics that we get access to um and so
[03:21:50 -> 03:21:56]  if we can we can we can look at we can
[03:21:53 -> 03:21:58]  pay attention to like this number 41 um
[03:21:56 -> 03:22:02]  we'll we'll keep this number in our head
[03:21:58 -> 03:22:03]  for now um there's also a source too so
[03:22:02 -> 03:22:06]  uh you can look at the actual assembly
[03:22:03 -> 03:22:08]  instructions and see um you know how
[03:22:06 -> 03:22:11]  many registers is it taken up uh a bunch
[03:22:08 -> 03:22:15]  of very lowlevel stuff um which I'm not
[03:22:11 -> 03:22:16]  going to dig into right now um but yeah
[03:22:15 -> 03:22:18]  there there there's so many settings
[03:22:16 -> 03:22:23]  that to dig through anyways we're going
[03:22:18 -> 03:22:23]  to keep this number um 41 in our
[03:22:23 -> 03:22:25]  head
[03:22:24 -> 03:22:28]  [Music]
[03:22:25 -> 03:22:30]  now we close this out we'll just put on
[03:22:28 -> 03:22:33]  the side for
[03:22:30 -> 03:22:35]  now we have some other we have some
[03:22:33 -> 03:22:37]  other uh scripts as well so I have a
[03:22:35 -> 03:22:39]  naive mmal so this is the one that we
[03:22:37 -> 03:22:42]  wrote previously this is the exact same
[03:22:39 -> 03:22:44]  just our direct copy and paste uh and
[03:22:42 -> 03:22:45]  then we have a tiled ml which I'm going
[03:22:44 -> 03:22:48]  to cover a little bit later it's a bit
[03:22:45 -> 03:22:52]  more advanced against um
[03:22:48 -> 03:22:54]  but we're going to compare the
[03:22:52 -> 03:22:57]  performance metrics of the naive versus
[03:22:54 -> 03:23:00]  the til ml
[03:22:57 -> 03:23:01]  so if we go ahead and pop into here we
[03:23:00 -> 03:23:07]  go
[03:23:01 -> 03:23:10]  nbcc uh 01 and then 01 and we link uh
[03:23:07 -> 03:23:10]  Envy
[03:23:13 -> 03:23:18]  tools we run successfully and we can go
[03:23:16 -> 03:23:21]  n this profile and then put in 01 right
[03:23:18 -> 03:23:23]  there and I'm going to go ahead and drag
[03:23:21 -> 03:23:26]  this so we pop up another one I'm going
[03:23:23 -> 03:23:27]  to go drag this into an Insight
[03:23:26 -> 03:23:32]  compute
[03:23:27 -> 03:23:35]  and if we check out our uh Cuda Hardware
[03:23:32 -> 03:23:37]  go to kernels Matrix multiply this is
[03:23:35 -> 03:23:40]  the night version remember uh show an
[03:23:37 -> 03:23:42]  events view Zoom to
[03:23:40 -> 03:23:47]  selected
[03:23:42 -> 03:23:49]  profile uh we run the PM sampling again
[03:23:47 -> 03:23:52]  it's going to run that and then we take
[03:23:49 -> 03:23:53]  a look at our new stats um you this is
[03:23:52 -> 03:23:57]  this is the exact same thing without
[03:23:53 -> 03:24:00]  mvtx but just for context
[03:23:57 -> 03:24:03]  details memory workload so we get you
[03:24:00 -> 03:24:05]  know 30 37 it's it's pretty close to
[03:24:03 -> 03:24:10]  what we had before right um maybe a
[03:24:05 -> 03:24:14]  little bit lower um but when we when we
[03:24:10 -> 03:24:14]  compile the til MML
[03:24:24 -> 03:24:29]  it works as expected and say
[03:24:27 -> 03:24:30]  profile we're going to get a number
[03:24:29 -> 03:24:32]  three
[03:24:30 -> 03:24:34]  here I'm going to goad and drag this
[03:24:32 -> 03:24:37]  into Insight
[03:24:34 -> 03:24:40]  compute we open this
[03:24:37 -> 03:24:43]  up pop over to our
[03:24:40 -> 03:24:44]  kernels Matrix multiply optimized show
[03:24:43 -> 03:24:46]  in
[03:24:44 -> 03:24:48]  events assum to select it on timeline so
[03:24:46 -> 03:24:49]  we can see the length of this by the way
[03:24:48 -> 03:24:53]  this is how long it takes it's going to
[03:24:49 -> 03:24:58]  go from you know 430 milliseconds
[03:24:53 -> 03:24:58]  43024 millisecs all the way to
[03:25:05 -> 03:25:08]  [Music]
[03:25:15 -> 03:25:20]  431.073 is significantly higher than it
[03:25:17 -> 03:25:21]  was before so these are the types of
[03:25:20 -> 03:25:23]  things you want to look out for when you
[03:25:21 -> 03:25:26]  see your memory throughput drop after
[03:25:23 -> 03:25:28]  you change something it's like uh maybe
[03:25:26 -> 03:25:30]  we maybe we shouldn't do that um you
[03:25:28 -> 03:25:33]  know from here it went from uh the naive
[03:25:30 -> 03:25:35]  it was at 37 and here it's at 60 right
[03:25:33 -> 03:25:38]  so that's making a a better use of
[03:25:35 -> 03:25:40]  memory um and we'll we'll we'll see more
[03:25:38 -> 03:25:43]  optimizations later on especially in
[03:25:40 -> 03:25:45]  this in this um faster matal chapter as
[03:25:43 -> 03:25:47]  to how we can seriously get this number
[03:25:45 -> 03:25:49]  up
[03:25:47 -> 03:25:51]  um but yeah this is this is how you
[03:25:49 -> 03:25:53]  profile there's a bunch of cool things
[03:25:51 -> 03:25:54]  you want to look out for here um it
[03:25:53 -> 03:25:57]  really depends on which algorithm you're
[03:25:54 -> 03:25:59]  working with with matrix multiplication
[03:25:57 -> 03:26:02]  uh there's some more like there's some
[03:25:59 -> 03:26:05]  more um fine grain optimizations that
[03:26:02 -> 03:26:06]  are just proven to work so we just we
[03:26:05 -> 03:26:08]  can just run those and and kind of
[03:26:06 -> 03:26:10]  compare the difference uh but the you
[03:26:08 -> 03:26:12]  have you have all the resources at your
[03:26:10 -> 03:26:16]  hand here there's tons of things that
[03:26:12 -> 03:26:19]  you can use and learn from so uh yeah
[03:26:16 -> 03:26:22]  this is uh this is how you profile Cuda
[03:26:19 -> 03:26:24]  kernels using Nvidia andite
[03:26:22 -> 03:26:26]  compute I have a readme file here with
[03:26:24 -> 03:26:32]  just pretty much everything we went over
[03:26:26 -> 03:26:32]  so um the NS profile command
[03:26:33 -> 03:26:40]  um you can profile python as well so NS
[03:26:36 -> 03:26:42]  profile and then um you can do you have
[03:26:40 -> 03:26:46]  a like an MLP script in Python you can
[03:26:42 -> 03:26:48]  profile that funny enough and it'll just
[03:26:46 -> 03:26:50]  use the whatever whatever Nvidia
[03:26:48 -> 03:26:52]  libraries is you that python file is
[03:26:50 -> 03:26:55]  using
[03:26:52 -> 03:26:57]  um then we have just you can do this
[03:26:55 -> 03:26:58]  some stuff over the command line like
[03:26:57 -> 03:27:01]  this
[03:26:58 -> 03:27:04]  so uh ncu kernel name you can you can do
[03:27:01 -> 03:27:06]  stuff over the command line um but yeah
[03:27:04 -> 03:27:09]  there's there's a bunch of useful tools
[03:27:06 -> 03:27:12]  here uh so this will this might be
[03:27:09 -> 03:27:13]  updated later on um it's not in like the
[03:27:12 -> 03:27:15]  the best format yet so this might look a
[03:27:13 -> 03:27:17]  bit different when you when you see it
[03:27:15 -> 03:27:19]  but uh these are kind of the the main
[03:27:17 -> 03:27:24]  ideas and then just to just to I guess
[03:27:19 -> 03:27:27]  leave it off on an end note um cupti or
[03:27:24 -> 03:27:30]  Cuda um Cuda profiling tools interface
[03:27:27 -> 03:27:34]  the PTI at the end this is for like
[03:27:30 -> 03:27:35]  creation creating your own uh the
[03:27:34 -> 03:27:38]  creating your own custom profiling and
[03:27:35 -> 03:27:41]  tracing tools that Target specific C
[03:27:38 -> 03:27:43]  applications so you can you can like
[03:27:41 -> 03:27:45]  design your own profiling tools with
[03:27:43 -> 03:27:46]  this um if that's something that catches
[03:27:45 -> 03:27:49]  your interest you might want to look
[03:27:46 -> 03:27:50]  more into it so I'll leave this here uh
[03:27:49 -> 03:27:52]  but that's that's how you profile cutic
[03:27:50 -> 03:27:54]  kernels next up we have this thing
[03:27:52 -> 03:27:56]  called an atomic operation and atomic
[03:27:54 -> 03:27:58]  operations are used in very specific
[03:27:56 -> 03:28:02]  cases so I'm going to try to cover these
[03:27:58 -> 03:28:04]  as best I can by atomic we mean the
[03:28:02 -> 03:28:05]  indivisibility concept in physics where
[03:28:04 -> 03:28:07]  thing cannot be broken down further
[03:28:05 -> 03:28:09]  right so you have an atom it's like oh I
[03:28:07 -> 03:28:10]  mean technically there are quarks and
[03:28:09 -> 03:28:11]  stuff but you don't worry about those
[03:28:10 -> 03:28:14]  it's just like the indivis
[03:28:11 -> 03:28:16]  indivisibility concept of this thing you
[03:28:14 -> 03:28:18]  you cannot cut it in half right there
[03:28:16 -> 03:28:19]  are Parts maybe inside of it that that
[03:28:18 -> 03:28:22]  make it up but you cannot you cannot cut
[03:28:19 -> 03:28:24]  it in half um and that's that's what
[03:28:22 -> 03:28:26]  this Atomic operation is and it operates
[03:28:24 -> 03:28:29]  as a software abstraction for us so the
[03:28:26 -> 03:28:33]  hardware and the Cuda compiler take care
[03:28:29 -> 03:28:35]  of all this for us um essentially an
[03:28:33 -> 03:28:37]  atomic operation ensures that a
[03:28:35 -> 03:28:39]  particular operation on a memory
[03:28:37 -> 03:28:41]  location is completed entirely by one
[03:28:39 -> 03:28:43]  thread before another thread can access
[03:28:41 -> 03:28:45]  or modify the same memory location this
[03:28:43 -> 03:28:46]  prevents raise conditions so remember
[03:28:45 -> 03:28:48]  before when we were talking about how
[03:28:46 -> 03:28:49]  there are like multiple threads that
[03:28:48 -> 03:28:52]  like one might end up being faster and
[03:28:49 -> 03:28:54]  hit the goal before this one and it's
[03:28:52 -> 03:28:56]  they sort of need to like not modify
[03:28:54 -> 03:28:58]  each other's uh they not they need to
[03:28:56 -> 03:29:00]  like not mess with each other's things
[03:28:58 -> 03:29:03]  so that that's what this idea is
[03:29:00 -> 03:29:06]  referring to um
[03:29:03 -> 03:29:07]  so cannot access or modify the same
[03:29:06 -> 03:29:10]  memory location of another thread that's
[03:29:07 -> 03:29:12]  very it's a very key point right um and
[03:29:10 -> 03:29:17]  and we're going to see a very Crystal
[03:29:12 -> 03:29:19]  Clear example of this in a second um
[03:29:17 -> 03:29:22]  we might lose some speed
[03:29:19 -> 03:29:24]  so if we limit the amount of work done
[03:29:22 -> 03:29:25]  on a single piece of memory per unit
[03:29:24 -> 03:29:27]  time through put an atomic operation
[03:29:25 -> 03:29:29]  we're we're going to lose some speed
[03:29:27 -> 03:29:32]  from that right if we're locking things
[03:29:29 -> 03:29:34]  down and limiting how how fast the
[03:29:32 -> 03:29:36]  program can finish by just having
[03:29:34 -> 03:29:38]  everything like not wait for everything
[03:29:36 -> 03:29:42]  else then it just finishes faster right
[03:29:38 -> 03:29:44]  so uh when we use atomics things will
[03:29:42 -> 03:29:46]  slow down but it is guaranteed to be
[03:29:44 -> 03:29:47]  Memory safe um and that that's
[03:29:46 -> 03:29:49]  ultimately what you might care about in
[03:29:47 -> 03:29:52]  some cases it might be better to get the
[03:29:49 -> 03:29:55]  memory safe aspect instead of instead of
[03:29:52 -> 03:29:57]  the performance gain um so there's a
[03:29:55 -> 03:29:58]  bunch of different Atomic operations
[03:29:57 -> 03:30:01]  that we have I'm just going to make this
[03:29:58 -> 03:30:04]  a bit easier to see um you have Atomic
[03:30:01 -> 03:30:07]  ad so essentially what this is you have
[03:30:04 -> 03:30:09]  a you have an in uh a pointer to an INT
[03:30:07 -> 03:30:13]  some some memory address and you have a
[03:30:09 -> 03:30:15]  value and all you do is adds value to
[03:30:13 -> 03:30:17]  the value at address so when we when we
[03:30:15 -> 03:30:19]  pass in like for example say the number
[03:30:17 -> 03:30:21]  four and then we get the memory address
[03:30:19 -> 03:30:24]  to that which is some hex code we put
[03:30:21 -> 03:30:27]  that we put that hex code in here and
[03:30:24 -> 03:30:28]  then we put a Val let's say like two and
[03:30:27 -> 03:30:31]  so what that'll do is it'll say okay
[03:30:28 -> 03:30:33]  well we have the memory address let's um
[03:30:31 -> 03:30:35]  let's get the value for that memory
[03:30:33 -> 03:30:37]  address which is four and then we're
[03:30:35 -> 03:30:38]  going to add the value to that so it's
[03:30:37 -> 03:30:40]  just like this memory address stays the
[03:30:38 -> 03:30:42]  same there's nothing new being created
[03:30:40 -> 03:30:46]  it's just you're taking this value and
[03:30:42 -> 03:30:48]  you're adding it on top um and that's
[03:30:46 -> 03:30:50]  that's kind of the whole philosophy of
[03:30:48 -> 03:30:54]  everything in here so
[03:30:50 -> 03:30:56]  substitution um exchange and and and the
[03:30:54 -> 03:30:59]  return value will always be the old
[03:30:56 -> 03:31:02]  value so when we do like Atomic add and
[03:30:59 -> 03:31:05]  then we say like put int equals Atomic
[03:31:02 -> 03:31:07]  ad or of of of whatever is in here it's
[03:31:05 -> 03:31:09]  going to return the old value of
[03:31:07 -> 03:31:11]  whatever this was so it's going to
[03:31:09 -> 03:31:14]  essentially return um the value at
[03:31:11 -> 03:31:16]  address right um so we can sort of
[03:31:14 -> 03:31:18]  compare and contrast it's it lets us do
[03:31:16 -> 03:31:20]  that um or you could just not like
[03:31:18 -> 03:31:22]  return anything that's fine you could
[03:31:20 -> 03:31:24]  just if you just want to add Val to that
[03:31:22 -> 03:31:28]  to the value then to the address value
[03:31:24 -> 03:31:30]  then you can just do that um but these
[03:31:28 -> 03:31:33]  are all of the uh these are all the
[03:31:30 -> 03:31:36]  operations that come with atomic uh
[03:31:33 -> 03:31:39]  these are all the atomic operations
[03:31:36 -> 03:31:40]  um there's also floating Point Atomic
[03:31:39 -> 03:31:44]  operations
[03:31:40 -> 03:31:46]  um so you you you can think of atomics
[03:31:44 -> 03:31:50]  as like a very fast
[03:31:46 -> 03:31:52]  uh Hardware Mutual exclusion operation
[03:31:50 -> 03:31:53]  um and I'll I'll dig into mutexes in a
[03:31:52 -> 03:31:56]  second here but essentially how this
[03:31:53 -> 03:31:59]  goes is you lock down a memory location
[03:31:56 -> 03:32:02]  um you set old value the The Returned
[03:31:59 -> 03:32:03]  value equal to um like d referencing
[03:32:02 -> 03:32:05]  that memory location so like the hex
[03:32:03 -> 03:32:07]  code and then you get the value for that
[03:32:05 -> 03:32:09]  you set the old you set this old value
[03:32:07 -> 03:32:13]  that you're going to return equal to
[03:32:09 -> 03:32:17]  that um that that D reference value
[03:32:13 -> 03:32:19]  um and then we set
[03:32:17 -> 03:32:22]  the we set the D referenced memory
[03:32:19 -> 03:32:25]  location so that value that goes with
[03:32:22 -> 03:32:27]  that hex code to the old Value Plus the
[03:32:25 -> 03:32:30]  increment which is which is Val right
[03:32:27 -> 03:32:31]  this is int Val um and then we unlock
[03:32:30 -> 03:32:34]  the memory location we return it so it's
[03:32:31 -> 03:32:35]  just like during this part where we're
[03:32:34 -> 03:32:37]  incrementing and we're storing the old
[03:32:35 -> 03:32:39]  value we're going to lock it down so
[03:32:37 -> 03:32:40]  nothing else can interfere with that
[03:32:39 -> 03:32:43]  it's just this has to complete this is
[03:32:40 -> 03:32:45]  priority and that'll that priority will
[03:32:43 -> 03:32:47]  exist through however many core threads
[03:32:45 -> 03:32:49]  we we have right so that way they can't
[03:32:47 -> 03:32:53]  interfere with each other so one has to
[03:32:49 -> 03:32:56]  finish before another one accesses it
[03:32:53 -> 03:33:00]  um and then we just return that right
[03:32:56 -> 03:33:02]  so um in terms of mutual exclusion
[03:33:00 -> 03:33:06]  there's a nice YouTube link on here that
[03:33:02 -> 03:33:08]  I found which was very good um Mutual is
[03:33:06 -> 03:33:11]  is like a like a shared relationship
[03:33:08 -> 03:33:14]  between entities so all of us threads um
[03:33:11 -> 03:33:16]  we're going to the act of keeping
[03:33:14 -> 03:33:19]  something out or preventing ACD
[03:33:16 -> 03:33:21]  so we're going to exclude uh everyone
[03:33:19 -> 03:33:23]  else from accessing each other's thing
[03:33:21 -> 03:33:25]  we're going to let each other finish
[03:33:23 -> 03:33:28]  right that's that's that's Mutual
[03:33:25 -> 03:33:30]  exclusion and this applies to atomics
[03:33:28 -> 03:33:32]  right um so you don't have multiple
[03:33:30 -> 03:33:34]  threads accessing the same thing at
[03:33:32 -> 03:33:37]  once um and there's there's like an
[03:33:34 -> 03:33:39]  intuitive example here of like what this
[03:33:37 -> 03:33:42]  might look um at at a lower level what
[03:33:39 -> 03:33:47]  this is actually doing so uh if we go
[03:33:42 -> 03:33:52]  over to our Atomic ad over here
[03:33:47 -> 03:33:54]  um if I if I nvcc compile this um we'll
[03:33:52 -> 03:33:56]  see like first of all we import whatever
[03:33:54 -> 03:33:59]  we need to the cudar
[03:33:56 -> 03:34:01]  runtime. um we have a number of threads
[03:33:59 -> 03:34:03]  so a th000 threads per block and then a
[03:34:01 -> 03:34:05]  th000 blocks in the grid um these are
[03:34:03 -> 03:34:07]  these are macros that we Define so if
[03:34:05 -> 03:34:09]  there are a th000 blocks with each 1,000
[03:34:07 -> 03:34:11]  threads inside of them then we're going
[03:34:09 -> 03:34:13]  to have a total of a million threads
[03:34:11 -> 03:34:15]  right um and then we have two kernels
[03:34:13 -> 03:34:17]  here so one is going to increment count
[03:34:15 -> 03:34:21]  counter non atomically so it's going to
[03:34:17 -> 03:34:23]  take in a counter uh it's going to store
[03:34:21 -> 03:34:26]  that old value as the D referenced
[03:34:23 -> 03:34:28]  counter CU this is a pointer right um
[03:34:26 -> 03:34:29]  we're going to set the new value to
[03:34:28 -> 03:34:31]  whatever this is whatever that actual
[03:34:29 -> 03:34:34]  integer value is plus one we're just
[03:34:31 -> 03:34:36]  going to increment by one and then we're
[03:34:34 -> 03:34:40]  going to um we're going to update
[03:34:36 -> 03:34:42]  counter right um and then there's an
[03:34:40 -> 03:34:45]  atomic version of this which does the
[03:34:42 -> 03:34:47]  same thing except it locks instead so
[03:34:45 -> 03:34:51]  this part here um this is actually like
[03:34:47 -> 03:34:51]  you're adding you're essentially adding
[03:34:52 -> 03:34:55]  uh not
[03:34:56 -> 03:35:00]  locked and this is
[03:35:01 -> 03:35:05]  not not
[03:35:04 -> 03:35:07]  unlocked right so you're supposed to
[03:35:05 -> 03:35:11]  lock here and then unlock there and
[03:35:07 -> 03:35:16]  return whatever that is
[03:35:11 -> 03:35:18]  so I'm if that's I think that's correct
[03:35:16 -> 03:35:21]  yes
[03:35:18 -> 03:35:23]  so we go down and everything here is is
[03:35:21 -> 03:35:25]  fairly intuitive we have our numb blocks
[03:35:23 -> 03:35:26]  and our num threads and it's the idea is
[03:35:25 -> 03:35:28]  we're going to have a million threads
[03:35:26 -> 03:35:30]  that are each trying to update this same
[03:35:28 -> 03:35:33]  this same counter because we pass this
[03:35:30 -> 03:35:36]  this counter this is a single variable
[03:35:33 -> 03:35:38]  or a single pointer that we pass in um
[03:35:36 -> 03:35:42]  and all of these threads have to have to
[03:35:38 -> 03:35:44]  modify the same thing right so when we
[03:35:42 -> 03:35:46]  actually run
[03:35:44 -> 03:35:49]  this you're you're going to see
[03:35:46 -> 03:35:53]  non-atomic counter value is 41 so this
[03:35:49 -> 03:35:55]  means that all of these threads are
[03:35:53 -> 03:35:56]  attacking the same the same memory
[03:35:55 -> 03:35:59]  address and they're all performing
[03:35:56 -> 03:36:01]  modifications on it at the same time but
[03:35:59 -> 03:36:02]  Atomic it's going to take a little while
[03:36:01 -> 03:36:04]  longer it might take a million
[03:36:02 -> 03:36:05]  operations instead of 41 but it's going
[03:36:04 -> 03:36:08]  to ensure that we get through this
[03:36:05 -> 03:36:10]  properly so it's going to say okay well
[03:36:08 -> 03:36:12]  this thread wants to access it so we
[03:36:10 -> 03:36:14]  need to lock down uh only this thread
[03:36:12 -> 03:36:16]  can access this value and then all the
[03:36:14 -> 03:36:17]  other threads instead of racing to it
[03:36:16 -> 03:36:20]  they're going to just wait because it's
[03:36:17 -> 03:36:22]  an atomic operation right and so this
[03:36:20 -> 03:36:23]  one gets to complete first and then
[03:36:22 -> 03:36:25]  maybe this guy and then this guy and
[03:36:23 -> 03:36:28]  then this guy and they and they all sort
[03:36:25 -> 03:36:30]  of complete um and then you end up with
[03:36:28 -> 03:36:32]  the actual true answer which is uh 1
[03:36:30 -> 03:36:35]  million right because it increments uh
[03:36:32 -> 03:36:38]  it increments from
[03:36:35 -> 03:36:41]  uh increments from from
[03:36:38 -> 03:36:42]  zero so that's that's pretty much
[03:36:41 -> 03:36:46]  atomics
[03:36:42 -> 03:36:47]  um they're pretty cool maybe you can
[03:36:46 -> 03:36:49]  think of a way that you could use them
[03:36:47 -> 03:36:51]  right now I don't know um but that's
[03:36:49 -> 03:36:53]  that's just something that's super
[03:36:51 -> 03:36:55]  important to cover because uh that's
[03:36:53 -> 03:36:56]  that's one of the the risks with uh
[03:36:55 -> 03:36:58]  kernels is that you have a bunch of
[03:36:56 -> 03:37:00]  these a bunch of different threads
[03:36:58 -> 03:37:01]  accessing the same the same thing and
[03:37:00 -> 03:37:04]  making changes that maybe you don't want
[03:37:01 -> 03:37:06]  to it um and not like getting any errors
[03:37:04 -> 03:37:08]  or warnings about it right that's that's
[03:37:06 -> 03:37:10]  a danger that you have so atomics helps
[03:37:08 -> 03:37:13]  secure that and lock that down so now we
[03:37:10 -> 03:37:14]  go into Cuda streams and Cuda streams
[03:37:13 -> 03:37:17]  are one of the most useful things for
[03:37:14 -> 03:37:19]  performance optimization uh in maybe
[03:37:17 -> 03:37:21]  even large systems right so this
[03:37:19 -> 03:37:22]  actually this especially Works in large
[03:37:21 -> 03:37:25]  systems and you're going to see why in a
[03:37:22 -> 03:37:27]  second here um so you can think of the
[03:37:25 -> 03:37:30]  intuition here you can think of streams
[03:37:27 -> 03:37:32]  as River streams where the uh direction
[03:37:30 -> 03:37:35]  of operations flows only forward in time
[03:37:32 -> 03:37:39]  so you have this this timeline and the
[03:37:35 -> 03:37:42]  idea is normally you would copy some
[03:37:39 -> 03:37:44]  data over from host to device and then
[03:37:42 -> 03:37:46]  you would do something with that data
[03:37:44 -> 03:37:48]  like a kernel launch and and then you
[03:37:46 -> 03:37:51]  would copy that back from device to host
[03:37:48 -> 03:37:52]  to do something useful with it um and
[03:37:51 -> 03:37:54]  what you have here is you have these
[03:37:52 -> 03:37:56]  little dependencies where it's like you
[03:37:54 -> 03:37:58]  have to wait for the data to come in
[03:37:56 -> 03:38:00]  before you actually start the kernel
[03:37:58 -> 03:38:02]  launch wouldn't you always want to be
[03:38:00 -> 03:38:03]  running kernels and wouldn't you always
[03:38:02 -> 03:38:05]  want to be doing computation well
[03:38:03 -> 03:38:07]  streams actually solves that issue for
[03:38:05 -> 03:38:09]  us instead of just having one little
[03:38:07 -> 03:38:11]  timeline you can have an extra layer
[03:38:09 -> 03:38:12]  underneath it too and even even even
[03:38:11 -> 03:38:17]  more you can have as many layers as you
[03:38:12 -> 03:38:19]  want and the the whole idea is you can
[03:38:17 -> 03:38:21]  copy some copy something over do a
[03:38:19 -> 03:38:23]  kernel launch and then during that
[03:38:21 -> 03:38:26]  kernel launch like when when that stuff
[03:38:23 -> 03:38:29]  gets copied over you can start copying
[03:38:26 -> 03:38:31]  the next stuff over in a separate stream
[03:38:29 -> 03:38:33]  right so you can you'll be doing some
[03:38:31 -> 03:38:35]  computation while you're copying stuff
[03:38:33 -> 03:38:39]  over and then when this stuff is copied
[03:38:35 -> 03:38:40]  over um you can do the next kernel so
[03:38:39 -> 03:38:43]  it'll it'll look like sort of a
[03:38:40 -> 03:38:48]  staircase and I have an example of this
[03:38:43 -> 03:38:51]  in the uh Nvidia documentation here the
[03:38:48 -> 03:38:53]  Nvidia streams and concurrency
[03:38:51 -> 03:38:57]  slides um and essentially looks like
[03:38:53 -> 03:39:00]  this so you have um this thing called
[03:38:57 -> 03:39:03]  cuda mem copy async which is
[03:39:00 -> 03:39:05]  asynchronous um and normally if you're
[03:39:03 -> 03:39:07]  doing a Serial program which is what
[03:39:05 -> 03:39:08]  we've worked with so far you'd be doing
[03:39:07 -> 03:39:10]  you'd be going in this fashion where
[03:39:08 -> 03:39:13]  you're you're not always doing you'd be
[03:39:10 -> 03:39:15]  like C copy something over do something
[03:39:13 -> 03:39:17]  with it copy it back copy something over
[03:39:15 -> 03:39:19]  do something with back and then in this
[03:39:17 -> 03:39:24]  example you like copy a bunch of stuff
[03:39:19 -> 03:39:26]  over host to device and then you do um
[03:39:24 -> 03:39:28]  maybe
[03:39:26 -> 03:39:32]  uh maybe it's better Illustrated in this
[03:39:28 -> 03:39:35]  example is like you you you copy some
[03:39:32 -> 03:39:38]  stuff over you do you know you do like
[03:39:35 -> 03:39:40]  say like three kernels in a row um and
[03:39:38 -> 03:39:42]  then whilst a kernel is running you're
[03:39:40 -> 03:39:44]  always copying new stuff over so you're
[03:39:42 -> 03:39:45]  not like you're not you're always doing
[03:39:44 -> 03:39:48]  work across all the Stream
[03:39:45 -> 03:39:51]  right uh and this is super useful
[03:39:48 -> 03:39:55]  especially when you have something like
[03:39:51 -> 03:39:57]  um when you have things like training a
[03:39:55 -> 03:39:59]  massive language model right when you're
[03:39:57 -> 03:40:01]  trying to when you have this data loader
[03:39:59 -> 03:40:04]  that is like constantly loading big
[03:40:01 -> 03:40:05]  chunks of text in you don't want to be
[03:40:04 -> 03:40:07]  waiting for that you don't want to just
[03:40:05 -> 03:40:09]  do your your your training forward and
[03:40:07 -> 03:40:11]  backward pass and then and then wait for
[03:40:09 -> 03:40:12]  it again you want like you want it to be
[03:40:11 -> 03:40:14]  loaded in while you're doing your
[03:40:12 -> 03:40:16]  forward and backward pass like you want
[03:40:14 -> 03:40:17]  it to be ready for you so that way you
[03:40:16 -> 03:40:20]  can just start you can just do it again
[03:40:17 -> 03:40:22]  so it's like non-stop forb back or right
[03:40:20 -> 03:40:24]  just you never want to stop doing that
[03:40:22 -> 03:40:25]  and that will that will greatly increase
[03:40:24 -> 03:40:29]  performance and so this is where Cuda
[03:40:25 -> 03:40:31]  streams come in right um so this this
[03:40:29 -> 03:40:33]  whole idea that I'm talking about with
[03:40:31 -> 03:40:35]  like fetching data um before before you
[03:40:33 -> 03:40:36]  actually need it like it's literally
[03:40:35 -> 03:40:39]  called prefetching software abstraction
[03:40:36 -> 03:40:41]  called pre-etching um so you move data
[03:40:39 -> 03:40:43]  around before it as needed and this
[03:40:41 -> 03:40:46]  hides the latency of moving data around
[03:40:43 -> 03:40:49]  like Cuda M Copy right um
[03:40:46 -> 03:40:52]  so we have this we have this kernel
[03:40:49 -> 03:40:55]  launch configuration seen this before we
[03:40:52 -> 03:40:56]  have a grid size we have a block size
[03:40:55 -> 03:40:59]  right and then there were two other
[03:40:56 -> 03:41:02]  things that I talked about which are
[03:40:59 -> 03:41:04]  here now this is the the number of bytes
[03:41:02 -> 03:41:06]  in shared memory right so you're you're
[03:41:04 -> 03:41:08]  doing stuff with shared memory which
[03:41:06 -> 03:41:10]  don't even worry about that right now
[03:41:08 -> 03:41:13]  and then there's this other one s which
[03:41:10 -> 03:41:15]  is the associated stream right so you
[03:41:13 -> 03:41:20]  can actually put a specific current on a
[03:41:15 -> 03:41:21]  stream as we showed in here right um you
[03:41:20 -> 03:41:22]  can have these are all the different
[03:41:21 -> 03:41:25]  streams stream one stream two stream
[03:41:22 -> 03:41:27]  three stream four so you have you you
[03:41:25 -> 03:41:28]  launch this one on stream one and Etc
[03:41:27 -> 03:41:31]  right so that's that's the whole idea
[03:41:28 -> 03:41:34]  there
[03:41:31 -> 03:41:36]  um and this this this is just a super
[03:41:34 -> 03:41:38]  easy way to interface with the streams
[03:41:36 -> 03:41:39]  when we do our Colonel launches so there
[03:41:38 -> 03:41:41]  there's multiple things that come in
[03:41:39 -> 03:41:44]  here when we're dealing with streams so
[03:41:41 -> 03:41:45]  you get this this thing of uh priorities
[03:41:44 -> 03:41:48]  so create streams with different
[03:41:45 -> 03:41:51]  priorities and if we go into
[03:41:48 -> 03:41:53]  [Music]
[03:41:51 -> 03:41:56]  um I don't know if it's in this one
[03:41:53 -> 03:41:59]  maybe this script we if we look at the
[03:41:56 -> 03:42:02]  get priority range here we can see um
[03:41:59 -> 03:42:04]  this takes in a pointer to a least
[03:42:02 -> 03:42:07]  priority int just a variable and then a
[03:42:04 -> 03:42:09]  greatest priority int and so it's like
[03:42:07 -> 03:42:11]  this is the least priority we're going
[03:42:09 -> 03:42:13]  to we're going to plug this in here uh
[03:42:11 -> 03:42:15]  and then we have a greatest priority
[03:42:13 -> 03:42:18]  which which we plug in here right and
[03:42:15 -> 03:42:22]  that's our range and
[03:42:18 -> 03:42:24]  so we can feed these in so that uh Cuda
[03:42:22 -> 03:42:27]  will actually manage which ones get more
[03:42:24 -> 03:42:28]  priority over others so if you want to
[03:42:27 -> 03:42:30]  uh load in a bunch of data first and
[03:42:28 -> 03:42:32]  that's your initial priority you want to
[03:42:30 -> 03:42:34]  like get that part done as fast as
[03:42:32 -> 03:42:36]  possible you can actually prioritize
[03:42:34 -> 03:42:39]  that um with like least and greatest
[03:42:36 -> 03:42:42]  priority right
[03:42:39 -> 03:42:46]  um and then we go uh a little
[03:42:42 -> 03:42:49]  bit a little bit further down
[03:42:46 -> 03:42:54]  and we have some examples here so let me
[03:42:49 -> 03:42:55]  just touch on the basics here so there's
[03:42:54 -> 03:42:58]  some stuff that you may have not seen
[03:42:55 -> 03:43:00]  before which I probably used earlier but
[03:42:58 -> 03:43:02]  I did mention it these are macros uh
[03:43:00 -> 03:43:05]  these are the error checking macros that
[03:43:02 -> 03:43:08]  we have to essentially make sure that
[03:43:05 -> 03:43:11]  operations go through successfully so
[03:43:08 -> 03:43:13]  when when we do like a Cuda Malik we
[03:43:11 -> 03:43:15]  want to make sure that that that that
[03:43:13 -> 03:43:17]  went successfully right and that'll just
[03:43:15 -> 03:43:20]  this will
[03:43:17 -> 03:43:22]  um this will return a Cuda error type
[03:43:20 -> 03:43:24]  meaning either success or error like
[03:43:22 -> 03:43:26]  fail right just indicating whether that
[03:43:24 -> 03:43:29]  went through or not um so that that's
[03:43:26 -> 03:43:31]  what that is and
[03:43:29 -> 03:43:33]  then if we scroll down a little bit more
[03:43:31 -> 03:43:36]  and see where the actual streams are
[03:43:33 -> 03:43:40]  happening um keep in
[03:43:36 -> 03:43:43]  mind up here we we use this Cuda stream
[03:43:40 -> 03:43:45]  type right so it's a Cuda stream type
[03:43:43 -> 03:43:47]  and we Define two streams that stream 1
[03:43:45 -> 03:43:51]  and
[03:43:47 -> 03:43:53]  two we create the Stream So we actually
[03:43:51 -> 03:43:55]  have to have custom uh handlers for this
[03:43:53 -> 03:43:56]  to say like okay you made this you made
[03:43:55 -> 03:43:58]  you defined it it's like now you have to
[03:43:56 -> 03:44:00]  actually create the thing it's a it's a
[03:43:58 -> 03:44:02]  weird context thing Nvidia has but it it
[03:44:00 -> 03:44:04]  it ensures everything is safe um and
[03:44:02 -> 03:44:06]  handled properly by the
[03:44:04 -> 03:44:09]  compiler
[03:44:06 -> 03:44:11]  so we get this other term kudam mam copy
[03:44:09 -> 03:44:13]  async and this essentially just allows
[03:44:11 -> 03:44:15]  us to um have like these asynchronous
[03:44:13 -> 03:44:18]  copies I mean when you have these
[03:44:15 -> 03:44:21]  ordered like if you go camm copy async
[03:44:18 -> 03:44:23]  on stream one and then later on you have
[03:44:21 -> 03:44:25]  like a like like a kernel launch like
[03:44:23 -> 03:44:28]  right underneath it um it'll actually go
[03:44:25 -> 03:44:29]  in that order so it won't just it won't
[03:44:28 -> 03:44:31]  um it won't try to do like the kernel
[03:44:29 -> 03:44:33]  launch before because it's asynchronous
[03:44:31 -> 03:44:35]  um it'll just be asynchronous meaning in
[03:44:33 -> 03:44:39]  the context of streams so you can have
[03:44:35 -> 03:44:41]  things sort of um happening I guess
[03:44:39 -> 03:44:43]  concurrently um but it'll still follow
[03:44:41 -> 03:44:46]  that sequential order within that stream
[03:44:43 -> 03:44:48]  as long as you assign them to the same
[03:44:46 -> 03:44:50]  one um so then we have our thread for
[03:44:48 -> 03:44:53]  block and blocks per grid configuration
[03:44:50 -> 03:44:56]  we launch this on stream one and we have
[03:44:53 -> 03:44:58]  this this B this uh this B uh array on
[03:44:56 -> 03:45:01]  on stream
[03:44:58 -> 03:45:03]  2 and we can do stuff with that as well
[03:45:01 -> 03:45:04]  so and then notice how we have the
[03:45:03 -> 03:45:06]  commment copy inputs the device
[03:45:04 -> 03:45:08]  asynchronously so this is just a little
[03:45:06 -> 03:45:11]  cheat where instead of instead of
[03:45:08 -> 03:45:13]  copying copying uh a from hosted device
[03:45:11 -> 03:45:16]  then copying B to from hosted device uh
[03:45:13 -> 03:45:18]  sequentially you do at the same time so
[03:45:16 -> 03:45:19]  a gets copied and B gets copied at the
[03:45:18 -> 03:45:21]  same time and then you don't have this
[03:45:19 -> 03:45:23]  extra barrier here where like nothing no
[03:45:21 -> 03:45:26]  work is being done you can just get
[03:45:23 -> 03:45:28]  right to uh kernel computation right um
[03:45:26 -> 03:45:30]  so we we see that here we have a stream
[03:45:28 -> 03:45:32]  one and stream two and this is all done
[03:45:30 -> 03:45:36]  on stream one so all this all this
[03:45:32 -> 03:45:39]  memory is going to be shared um
[03:45:36 -> 03:45:41]  and uh we we go down further to this you
[03:45:39 -> 03:45:43]  know we copy back we copy C back
[03:45:41 -> 03:45:44]  asynchronously but there's only one so
[03:45:43 -> 03:45:47]  it doesn't really matter which stream
[03:45:44 -> 03:45:50]  that's on um we just do async because
[03:45:47 -> 03:45:51]  why not and then the stream synchronize
[03:45:50 -> 03:45:54]  so we're going to ensure that all of the
[03:45:51 -> 03:45:56]  streams are then caught up and then we
[03:45:54 -> 03:45:58]  can um and then we can do something with
[03:45:56 -> 03:46:02]  that right so we're going to you know
[03:45:58 -> 03:46:04]  free the device viice a BC and then
[03:46:02 -> 03:46:05]  destroy both of the streams uh and then
[03:46:04 -> 03:46:09]  it's and then it's done so if I go ahead
[03:46:05 -> 03:46:09]  and actually compile and run
[03:46:13 -> 03:46:17]  this test passed and we got everything
[03:46:15 -> 03:46:20]  correctly and we did this vector
[03:46:17 -> 03:46:22]  addition um so it's just kind of what
[03:46:20 -> 03:46:24]  what we all we really did here was just
[03:46:22 -> 03:46:26]  uh this is where the magic happens
[03:46:24 -> 03:46:29]  instead of loading a and then B we load
[03:46:26 -> 03:46:32]  and B at the same time um and then we go
[03:46:29 -> 03:46:34]  into um what's it
[03:46:32 -> 03:46:37]  called
[03:46:34 -> 03:46:39]  Advanced so going to lower this a little
[03:46:37 -> 03:46:41]  bit so you can see so when we go into
[03:46:39 -> 03:46:43]  advanced streams things get a little
[03:46:41 -> 03:46:45]  weirder uh but it's not too crazy so
[03:46:43 -> 03:46:48]  there's a few things that I want to
[03:46:45 -> 03:46:52]  introduce here we have pinned memory so
[03:46:48 -> 03:46:56]  it's essentially saying on on the CPU uh
[03:46:52 -> 03:46:57]  Global Dr it's going to reserve this
[03:46:56 -> 03:47:01]  piece of memory we're going to we're
[03:46:57 -> 03:47:02]  going to pin it using Cuda Malik host um
[03:47:01 -> 03:47:05]  send it just saying you know this is a
[03:47:02 -> 03:47:07]  part of Cuda it's reserved for uh the
[03:47:05 -> 03:47:08]  GPU to use later so we're not going to
[03:47:07 -> 03:47:11]  modify that we're not going to let the
[03:47:08 -> 03:47:12]  operating system or anything play with
[03:47:11 -> 03:47:13]  this we're just going to pin it nothing
[03:47:12 -> 03:47:15]  can touch it and then we're going to
[03:47:13 -> 03:47:16]  drag that over somewhere else for later
[03:47:15 -> 03:47:19]  and we're just going to like reserve
[03:47:16 -> 03:47:20]  that right um so we're going to use this
[03:47:19 -> 03:47:21]  we're going to need this for later so
[03:47:20 -> 03:47:23]  don't play with it is a good way to
[03:47:21 -> 03:47:27]  think about this
[03:47:23 -> 03:47:31]  um events are a critical part of using
[03:47:27 -> 03:47:34]  streams so we can measure kernel
[03:47:31 -> 03:47:37]  execution time given this uh given this
[03:47:34 -> 03:47:39]  example here so we have an event type
[03:47:37 -> 03:47:42]  start and stop so the these don't
[03:47:39 -> 03:47:46]  actually time anything but they are um
[03:47:42 -> 03:47:49]  they're part of uh the input to these uh
[03:47:46 -> 03:47:50]  Event Event record functions so we go
[03:47:49 -> 03:47:52]  and create these with the memory
[03:47:50 -> 03:47:54]  addresses of start and stop and then we
[03:47:52 -> 03:47:57]  can plug in whatever stream for example
[03:47:54 -> 03:47:59]  just stream or stream one or stream two
[03:47:57 -> 03:48:03]  we we do an event
[03:47:59 -> 03:48:05]  record we do uh we launch our kernel on
[03:48:03 -> 03:48:08]  this stream and then we do another event
[03:48:05 -> 03:48:11]  record um in this
[03:48:08 -> 03:48:14]  stream and we can take these values
[03:48:11 -> 03:48:16]  start and stop and they might carry
[03:48:14 -> 03:48:20]  metadata I don't know exactly how Cuda
[03:48:16 -> 03:48:23]  handles this but start and stop we can
[03:48:20 -> 03:48:24]  uh synchronize we can synchronize uh
[03:48:23 -> 03:48:28]  everything and then we can plug it into
[03:48:24 -> 03:48:30]  Cuda event lapse time which takes this
[03:48:28 -> 03:48:31]  milliseconds float uh which is going to
[03:48:30 -> 03:48:33]  be a milliseconds and then you have your
[03:48:31 -> 03:48:35]  start and stop and that's going to tell
[03:48:33 -> 03:48:38]  you how long your kernel took to run
[03:48:35 -> 03:48:41]  right so instead of launching it in the
[03:48:38 -> 03:48:43]  instead of going into the the ncu
[03:48:41 -> 03:48:45]  profiler you can just do this instead if
[03:48:43 -> 03:48:47]  you want to and this will this will not
[03:48:45 -> 03:48:49]  have any computational overhead or it's
[03:48:47 -> 03:48:51]  very minimal so this can be run in like
[03:48:49 -> 03:48:54]  production environments it's not really
[03:48:51 -> 03:48:56]  going to cost you anything um and then
[03:48:54 -> 03:49:01]  you have the synchronization between
[03:48:56 -> 03:49:05]  streams so um essentially these events
[03:49:01 -> 03:49:06]  will synchronize they will be placed in
[03:49:05 -> 03:49:09]  individual streams so instead of the
[03:49:06 -> 03:49:11]  whole device or across all the streams
[03:49:09 -> 03:49:13]  it's just one specific stream right that
[03:49:11 -> 03:49:16]  that's the whole idea with these um and
[03:49:13 -> 03:49:18]  then of course you can overlap uh
[03:49:16 -> 03:49:21]  computation data transfer with uh the
[03:49:18 -> 03:49:23]  prefetching idea which we talked about
[03:49:21 -> 03:49:27]  uh before which I
[03:49:23 -> 03:49:29]  believe pre fetching yes so events are
[03:49:27 -> 03:49:31]  great for that um and then we have
[03:49:29 -> 03:49:32]  callbacks which are used slightly
[03:49:31 -> 03:49:34]  differently you you can essentially set
[03:49:32 -> 03:49:37]  up a pipeline where the completion of
[03:49:34 -> 03:49:39]  one operation on the GPU triggers the
[03:49:37 -> 03:49:42]  start of another on the CPU so this is
[03:49:39 -> 03:49:44]  going to have some more overhead but if
[03:49:42 -> 03:49:45]  you want to log when something happens
[03:49:44 -> 03:49:48]  when you want to log when something
[03:49:45 -> 03:49:50]  happens on your GPU then you can use a
[03:49:48 -> 03:49:53]  call back um so in in this context we
[03:49:50 -> 03:49:55]  have a kernel uh and then you know in
[03:49:53 -> 03:49:58]  like we have we have a like say say like
[03:49:55 -> 03:50:00]  some stream like stream one and then we
[03:49:58 -> 03:50:03]  place this call back right after the
[03:50:00 -> 03:50:05]  kernel so in the timeline it's going to
[03:50:03 -> 03:50:06]  show up as kernel and Cuda stream at
[03:50:05 -> 03:50:09]  callback just as the way they are from
[03:50:06 -> 03:50:12]  like top to bottom in the code and if
[03:50:09 -> 03:50:14]  they're in the same stream um and we put
[03:50:12 -> 03:50:17]  this this call back in and that's the
[03:50:14 -> 03:50:19]  entally going to say when this finishes
[03:50:17 -> 03:50:22]  when when this finishes we're going to
[03:50:19 -> 03:50:25]  call this function
[03:50:22 -> 03:50:29]  um and it's just going to print GPU
[03:50:25 -> 03:50:30]  operation completed right so that's uh
[03:50:29 -> 03:50:31]  that that's like one use case of
[03:50:30 -> 03:50:33]  callbacks you might not use them all the
[03:50:31 -> 03:50:34]  time you're probably going to use events
[03:50:33 -> 03:50:36]  more if you're really trying to get
[03:50:34 -> 03:50:38]  those optimizations out but let's go
[03:50:36 -> 03:50:41]  ahead and look at the uh Advanced
[03:50:38 -> 03:50:44]  section here so
[03:50:41 -> 03:50:46]  um we have we have kernel one which is
[03:50:44 -> 03:50:48]  going to multiply by two and we have
[03:50:46 -> 03:50:50]  kernel 2 which is going to add one all
[03:50:48 -> 03:50:52]  right very simple operations we have our
[03:50:50 -> 03:50:54]  call back here stream call back
[03:50:52 -> 03:50:57]  operation completed just print print out
[03:50:54 -> 03:50:58]  when something happened right um and
[03:50:57 -> 03:51:01]  then just sort of like flowing from top
[03:50:58 -> 03:51:03]  to bottom here we do our our Cuda stream
[03:51:01 -> 03:51:05]  type so we just declare some streams we
[03:51:03 -> 03:51:08]  have our Cuda event type we're just
[03:51:05 -> 03:51:10]  going to initialize an event um we can
[03:51:08 -> 03:51:11]  print out whatever that event is I was
[03:51:10 -> 03:51:14]  testing earlier so this that's why this
[03:51:11 -> 03:51:18]  is still here um we do our Cuda malic
[03:51:14 -> 03:51:21]  host for that pinned memory um we could
[03:51:18 -> 03:51:24]  amalik our um our device
[03:51:21 -> 03:51:27]  data we do our our greatest and least
[03:51:24 -> 03:51:30]  priorities like I was talking about
[03:51:27 -> 03:51:32]  before uh we create we actually create
[03:51:30 -> 03:51:35]  the event itself using this previous
[03:51:32 -> 03:51:40]  event type that we initialized here and
[03:51:35 -> 03:51:43]  then we um we do our you know uh Cuda
[03:51:40 -> 03:51:44]  Cuda M Copy async uh we we launch a
[03:51:43 -> 03:51:46]  kernel and then this is where things
[03:51:44 -> 03:51:49]  start to get a little tricky and I'll do
[03:51:46 -> 03:51:51]  my best to explain so when we do Cuda
[03:51:49 -> 03:51:53]  event record that's going to place a
[03:51:51 -> 03:51:55]  little marker like a little tick right
[03:51:53 -> 03:51:57]  in that stream so we have the stream one
[03:51:55 -> 03:51:59]  here um you know this this is on stream
[03:51:57 -> 03:52:01]  one this on stream one this is on stream
[03:51:59 -> 03:52:03]  one so you're going to do your your copy
[03:52:01 -> 03:52:04]  from hosted device and then you're going
[03:52:03 -> 03:52:07]  to do the kernel and then you're going
[03:52:04 -> 03:52:11]  to put a little tick mark right here um
[03:52:07 -> 03:52:14]  and what that says is
[03:52:11 -> 03:52:16]  um we we might want to do something when
[03:52:14 -> 03:52:18]  when this gets reached so notice how
[03:52:16 -> 03:52:21]  this is right below our kernel so when
[03:52:18 -> 03:52:23]  the kernel is finished when it is done
[03:52:21 -> 03:52:26]  uh this is going to this is this is
[03:52:23 -> 03:52:27]  going to trigger right and then we have
[03:52:26 -> 03:52:29]  this stream weight event as a little
[03:52:27 -> 03:52:32]  dependency so it's essentially going to
[03:52:29 -> 03:52:34]  wait for everything up to uh stream
[03:52:32 -> 03:52:37]  stream one to finish and then it's going
[03:52:34 -> 03:52:39]  to begin on on stream two so stream two
[03:52:37 -> 03:52:43]  has to actually wait for this to happen
[03:52:39 -> 03:52:45]  notice how we pass in uh a stream we
[03:52:43 -> 03:52:46]  which is stream two so stream stream 2
[03:52:45 -> 03:52:49]  will start doing its things like Colonel
[03:52:46 -> 03:52:51]  execution and and the call back um we
[03:52:49 -> 03:52:55]  have to wait
[03:52:51 -> 03:52:57]  for where did it go we have to wait for
[03:52:55 -> 03:52:58]  this event to trigger first and that's
[03:52:57 -> 03:53:01]  essentially all this is is it just waits
[03:52:58 -> 03:53:03]  for it and then it begins on stream to
[03:53:01 -> 03:53:04]  when all of these previous ones are
[03:53:03 -> 03:53:08]  completed
[03:53:04 -> 03:53:10]  right or at least everything else in uh
[03:53:08 -> 03:53:14]  everything else in stream stream one so
[03:53:10 -> 03:53:16]  these two so then stream two comes along
[03:53:14 -> 03:53:19]  after this is done after we've actually
[03:53:16 -> 03:53:22]  done our kernel
[03:53:19 -> 03:53:24]  execution and then stream two is going
[03:53:22 -> 03:53:26]  to run this second kernel so it just
[03:53:24 -> 03:53:29]  it's just kind of ordered in that way so
[03:53:26 -> 03:53:30]  you have the async CM copy kernel one
[03:53:29 -> 03:53:33]  and then it's going to wait for that to
[03:53:30 -> 03:53:35]  finish and then drop down in stream two
[03:53:33 -> 03:53:37]  it's going to start um it's going to
[03:53:35 -> 03:53:41]  start the second kernel execution and
[03:53:37 -> 03:53:43]  then when this is done so when when um
[03:53:41 -> 03:53:44]  once we complete this point this is like
[03:53:43 -> 03:53:46]  another marker in the timeline
[03:53:44 -> 03:53:49]  it's going to wait for all that to
[03:53:46 -> 03:53:50]  complete and then it's going to say okay
[03:53:49 -> 03:53:52]  awesome we can now do a call back and
[03:53:50 -> 03:53:53]  then it's going to go up to this
[03:53:52 -> 03:53:56]  function here and it's going to and it's
[03:53:53 -> 03:53:59]  going to run that um so that's just kind
[03:53:56 -> 03:54:02]  of like stepping through uh one by one
[03:53:59 -> 03:54:05]  what is happening there
[03:54:02 -> 03:54:08]  um and then we'll just copy back to post
[03:54:05 -> 03:54:11]  with Cuda Cuda mem copy async uh and
[03:54:08 -> 03:54:13]  then to finalize we always want to uh
[03:54:11 -> 03:54:14]  synchronize our streams so we have all
[03:54:13 -> 03:54:16]  these streams that are happening we've
[03:54:14 -> 03:54:17]  just added another layer of complexity
[03:54:16 -> 03:54:19]  we need to synchronize those up too
[03:54:17 -> 03:54:20]  right so there's the whole device
[03:54:19 -> 03:54:22]  synchronized which is like you
[03:54:20 -> 03:54:24]  synchronize all the threads in the
[03:54:22 -> 03:54:25]  device and then there's this one which
[03:54:24 -> 03:54:27]  is on the level of stream so you have
[03:54:25 -> 03:54:29]  like maybe stream one 2 3 four and you
[03:54:27 -> 03:54:32]  like wait for all of them to like finish
[03:54:29 -> 03:54:33]  um before you before you continue right
[03:54:32 -> 03:54:37]  you wait for all them to catch up by
[03:54:33 -> 03:54:38]  adding a little barrier blocking um and
[03:54:37 -> 03:54:40]  and that's what's happening here and
[03:54:38 -> 03:54:41]  then we just destroy all of these we
[03:54:40 -> 03:54:43]  just essentially remove all these
[03:54:41 -> 03:54:45]  contexts and then we're good to go so
[03:54:43 -> 03:54:47]  that's uh that's how stream work that's
[03:54:45 -> 03:54:51]  how this Advanced thing works under the
[03:54:47 -> 03:54:51]  hood um if I go ahead and run
[03:54:56 -> 03:55:01]  this so we notice how when we when we
[03:54:58 -> 03:55:03]  are printing out uh where did it go when
[03:55:01 -> 03:55:05]  we're printing out the event the event
[03:55:03 -> 03:55:07]  is just a pointer so it's like a it's
[03:55:05 -> 03:55:11]  just like a memory address thing and
[03:55:07 -> 03:55:14]  then we got our our operation completed
[03:55:11 -> 03:55:16]  so that's when we do this with when we
[03:55:14 -> 03:55:19]  do this call back and then we end up
[03:55:16 -> 03:55:20]  with the test passed afterwards so uh
[03:55:19 -> 03:55:22]  you know just referencing back to that
[03:55:20 -> 03:55:24]  Nvidia diagram with all the all the
[03:55:22 -> 03:55:26]  different streams like that that's
[03:55:24 -> 03:55:29]  essentially what you care about
[03:55:26 -> 03:55:30]  right so I hope the last part wasn't too
[03:55:29 -> 03:55:33]  conceptually hard for you that that's
[03:55:30 -> 03:55:35]  typically where uh people will sort of
[03:55:33 -> 03:55:38]  break down and and question a lot of
[03:55:35 -> 03:55:40]  things it it was probably hard but
[03:55:38 -> 03:55:41]  anyways I'm glad you made it through
[03:55:40 -> 03:55:43]  feel free to rewatch some parts that is
[03:55:41 -> 03:55:46]  one of the most challenging parts of the
[03:55:43 -> 03:55:48]  course there's lot to unpack it's very
[03:55:46 -> 03:55:50]  spatially intuitive but this part is
[03:55:48 -> 03:55:53]  supposed to not be very spatially
[03:55:50 -> 03:55:55]  intuitive it is supposed to be just like
[03:55:53 -> 03:55:56]  textbook examples uh this is how you
[03:55:55 -> 03:55:59]  navigate things it's it's not supposed
[03:55:56 -> 03:56:03]  to be very hard mathematically spatially
[03:55:59 -> 03:56:07]  anything so this is this chapter is on
[03:56:03 -> 03:56:10]  the Cuda API so this is chapter six Cuda
[03:56:07 -> 03:56:13]  apis um we have a few to go through
[03:56:10 -> 03:56:16]  kublos CNN Etc but I want you to
[03:56:13 -> 03:56:20]  navigate over to docs. nvidia.com
[03:56:16 -> 03:56:23]  Cuda so here we have a lot of resources
[03:56:20 -> 03:56:25]  we have a lot of cool things to look at
[03:56:23 -> 03:56:28]  um and I just kind of want to point this
[03:56:25 -> 03:56:30]  out not that it's specific to uh the cud
[03:56:28 -> 03:56:32]  API section but because there's a lot of
[03:56:30 -> 03:56:34]  there's a lot of useful things here so
[03:56:32 -> 03:56:36]  you have your installation guides for
[03:56:34 -> 03:56:38]  like Windows and Linux right like this
[03:56:36 -> 03:56:40]  is just like everything you need to get
[03:56:38 -> 03:56:42]  started um programming guide best
[03:56:40 -> 03:56:44]  practices all the different you know
[03:56:42 -> 03:56:47]  Maxwell Pascal VTA Turing m here Hopper
[03:56:44 -> 03:56:50]  Ada Maxwell um all these different
[03:56:47 -> 03:56:52]  compatibility guides and tuning guides
[03:56:50 -> 03:56:54]  uh for different architectures and then
[03:56:52 -> 03:56:56]  you have like your PTX which is the
[03:56:54 -> 03:56:59]  assembly instructions for Cuda that's
[03:56:56 -> 03:57:01]  what it compiles down to uh and then
[03:56:59 -> 03:57:05]  just like API references miscellaneous
[03:57:01 -> 03:57:07]  stuff um and tools like nbcc uh GDB for
[03:57:05 -> 03:57:09]  Cuda so when we covered GDB earlier in
[03:57:07 -> 03:57:11]  the C+ plus review section this is the
[03:57:09 -> 03:57:13]  equivalent for Cuda so when you're
[03:57:11 -> 03:57:15]  debugging Cuda programs you'd use that
[03:57:13 -> 03:57:18]  uh and then there's like Insite compute
[03:57:15 -> 03:57:21]  which we used earlier um and that's yeah
[03:57:18 -> 03:57:23]  just a lot of very informative tools
[03:57:21 -> 03:57:26]  here what we mostly care about is the C
[03:57:23 -> 03:57:29]  API references so in here we have
[03:57:26 -> 03:57:30]  runtime API driver API math API you can
[03:57:29 -> 03:57:33]  go through those if you want but mainly
[03:57:30 -> 03:57:36]  what I'm going to cover is kublos and
[03:57:33 -> 03:57:39]  cdnn which is over here if you go to
[03:57:36 -> 03:57:42]  docs. video.com deeplearning CNN you can
[03:57:39 -> 03:57:45]  find this and these are the main ones
[03:57:42 -> 03:57:49]  which I expect to cover in this section
[03:57:45 -> 03:57:52]  so you can think of these uh like kublos
[03:57:49 -> 03:57:54]  and CNN as they're not you're not
[03:57:52 -> 03:57:55]  actually writing you're not writing
[03:57:54 -> 03:57:58]  things out
[03:57:55 -> 03:58:01]  manually you're not writing out your own
[03:57:58 -> 03:58:04]  kernels it's the the whole idea here is
[03:58:01 -> 03:58:06]  you have like this this black box
[03:58:04 -> 03:58:08]  function that you call or like a it's
[03:58:06 -> 03:58:12]  binded to a shared object file like an
[03:58:08 -> 03:58:14]  so and it's opaque so they use these
[03:58:12 -> 03:58:16]  these this word called an opaque struct
[03:58:14 -> 03:58:18]  type and what that is is is you're just
[03:58:16 -> 03:58:21]  calling something that is compiled down
[03:58:18 -> 03:58:23]  to to run on the hardware you do not get
[03:58:21 -> 03:58:25]  to see it because it's you know in
[03:58:23 -> 03:58:28]  encoded in some binary format that you
[03:58:25 -> 03:58:30]  can't really read as a human and so you
[03:58:28 -> 03:58:32]  have to refer to these opaque struct
[03:58:30 -> 03:58:34]  types to be able to call those these are
[03:58:32 -> 03:58:37]  highly optimized so like the
[03:58:34 -> 03:58:39]  state-of-the-art algorithms in the world
[03:58:37 -> 03:58:40]  uh for running you know deep learning
[03:58:39 -> 03:58:43]  algorithms that these are the fastest
[03:58:40 -> 03:58:44]  ones um sometimes you might get
[03:58:43 -> 03:58:46]  something faster depending on the use
[03:58:44 -> 03:58:48]  case but we'll generally assume that
[03:58:46 -> 03:58:52]  these that the Cuda API provides the
[03:58:48 -> 03:58:55]  fastest functions uh generally speaking
[03:58:52 -> 03:58:57]  so when you're trying to figure out how
[03:58:55 -> 03:58:59]  to get the fastest possible inference to
[03:58:57 -> 03:59:05]  work on your GPU
[03:58:59 -> 03:59:08]  cluster uh you might want to uh you know
[03:59:05 -> 03:59:11]  use something like Cuda API um and then
[03:59:08 -> 03:59:13]  going through uh you know just going
[03:59:11 -> 03:59:14]  through and researching and figuring out
[03:59:13 -> 03:59:18]  how to get it done by going to like
[03:59:14 -> 03:59:20]  Google search perplexity Chad gbt um you
[03:59:18 -> 03:59:22]  know maybe anthropic
[03:59:20 -> 03:59:25]  models and then keyword searching in the
[03:59:22 -> 03:59:28]  Nvidia docs like just a crlf like that
[03:59:25 -> 03:59:31]  um but the the C API is going to give
[03:59:28 -> 03:59:32]  you the fastest stuff right um You may
[03:59:31 -> 03:59:35]  have seen this before how we did this
[03:59:32 -> 03:59:37]  these like error checks um and these
[03:59:35 -> 03:59:41]  essentially just just say like when you
[03:59:37 -> 03:59:43]  call a function and say like kublos um
[03:59:41 -> 03:59:45]  you're you're going to check if if that
[03:59:43 -> 03:59:46]  return an error or not and if it does
[03:59:45 -> 03:59:48]  you're going to print the error and the
[03:59:46 -> 03:59:50]  line it was at right so these are just
[03:59:48 -> 03:59:51]  custom ways of of printing out errors
[03:59:50 -> 03:59:54]  and when things don't go according to
[03:59:51 -> 03:59:56]  plan so I have these both for um kblast
[03:59:54 -> 03:59:59]  and qnn so it just checks the function
[03:59:56 -> 04:00:02]  make sure it went through properly
[03:59:59 -> 04:00:07]  um
[04:00:02 -> 04:00:10]  now kuas is short for Cuda so the CU is
[04:00:07 -> 04:00:12]  for Cuda basic linear algebra sub
[04:00:10 -> 04:00:15]  routines or subsystems I can't remember
[04:00:12 -> 04:00:16]  which one but it's it's for it's for
[04:00:15 -> 04:00:19]  linear algebra stuff like matrix
[04:00:16 -> 04:00:22]  multiplication right and sgem which is a
[04:00:19 -> 04:00:23]  short for single Precision General
[04:00:22 -> 04:00:25]  matrix
[04:00:23 -> 04:00:27]  multiplication
[04:00:25 -> 04:00:29]  um and that's that that's like pretty
[04:00:27 -> 04:00:30]  much what this whole like GRE me file is
[04:00:29 -> 04:00:33]  about I'm kind of like reciting it as we
[04:00:30 -> 04:00:34]  go down but um you know there there are
[04:00:33 -> 04:00:37]  resources on this like proper error
[04:00:34 -> 04:00:39]  checking Library samples um if we were
[04:00:37 -> 04:00:41]  to go to this there's like a library
[04:00:39 -> 04:00:44]  samples where you can test out each of
[04:00:41 -> 04:00:46]  these um
[04:00:44 -> 04:00:48]  but the the whole idea with kublos and
[04:00:46 -> 04:00:51]  and how it's important to deep learning
[04:00:48 -> 04:00:53]  is in in something like the transformer
[04:00:51 -> 04:00:54]  or an MLP in the Transformer itself
[04:00:53 -> 04:00:56]  you're going to use this algorithm
[04:00:54 -> 04:00:58]  called matrix multiplication and when
[04:00:56 -> 04:00:59]  you want the MLP to run really fast or
[04:00:58 -> 04:01:01]  you want this language model to have
[04:00:59 -> 04:01:04]  really really fast inference time you
[04:01:01 -> 04:01:06]  want the algorithms to not really have
[04:01:04 -> 04:01:08]  bottlenecks right you want them to run
[04:01:06 -> 04:01:11]  as fast as possible on the hardware and
[04:01:08 -> 04:01:14]  so using the sub routines in kuas you
[04:01:11 -> 04:01:15]  can actually get that um there are other
[04:01:14 -> 04:01:17]  ways where you can like combine and mix
[04:01:15 -> 04:01:20]  things together but that's more advanced
[04:01:17 -> 04:01:22]  for now we're just assume that the
[04:01:20 -> 04:01:27]  fastest algorithms exist in kublos and
[04:01:22 -> 04:01:28]  CNN for deep learning purposes um so
[04:01:27 -> 04:01:30]  we're going to go ahead and start here
[04:01:28 -> 04:01:33]  with uh
[04:01:30 -> 04:01:33]  kublos
[04:01:34 -> 04:01:42]  now basic linear algebra sub programs
[04:01:39 -> 04:01:46]  um for accelerating AI high performance
[04:01:42 -> 04:01:49]  applications like I said before
[04:01:46 -> 04:01:52]  um industry standard blast apis and Gem
[04:01:49 -> 04:01:53]  API so General matrix multiplication
[04:01:52 -> 04:01:55]  with support for fusions highly
[04:01:53 -> 04:01:56]  optimized for NVIDIA gpus I'll dig into
[04:01:55 -> 04:01:59]  fusions in a second here don't worry
[04:01:56 -> 04:02:02]  about that um but what what I've
[04:01:59 -> 04:02:04]  essentially done with each of these is
[04:02:02 -> 04:02:06]  I've laid them out into testing so
[04:02:04 -> 04:02:08]  before I go into actually like uh
[04:02:06 -> 04:02:10]  printing out what the results are and
[04:02:08 -> 04:02:12]  and how well these work um and what the
[04:02:10 -> 04:02:14]  differences are it's important to cover
[04:02:12 -> 04:02:18]  what they actually do what is the
[04:02:14 -> 04:02:23]  difference between these so kubalas
[04:02:18 -> 04:02:25]  itself is just the super high it's it's
[04:02:23 -> 04:02:27]  it's essentially the easiest one to to
[04:02:25 -> 04:02:28]  use and get working it's just like the
[04:02:27 -> 04:02:31]  the standard that you typically start
[04:02:28 -> 04:02:34]  with um and it's going to support uh
[04:02:31 -> 04:02:35]  your basic you know uh single Precision
[04:02:34 -> 04:02:40]  so
[04:02:35 -> 04:02:44]  fp32 um and uh fp16 matrix
[04:02:40 -> 04:02:47]  multiplication right um kuas LT is a
[04:02:44 -> 04:02:50]  lightweight extension of kuas that
[04:02:47 -> 04:02:51]  provides a more flexible API primarily
[04:02:50 -> 04:02:54]  aimed at improving performance for
[04:02:51 -> 04:02:58]  specific workloads
[04:02:54 -> 04:03:02]  um except this is more oriented around
[04:02:58 -> 04:03:03]  larger matrices so kuas LT is optimized
[04:03:02 -> 04:03:06]  a little differently and it can be
[04:03:03 -> 04:03:10]  faster than uh just regular kuas in
[04:03:06 -> 04:03:12]  cases so when you have when you have
[04:03:10 -> 04:03:14]  something that's a lightweight ideally
[04:03:12 -> 04:03:15]  it's going to be lower precision right
[04:03:14 -> 04:03:17]  you can think of the L as like
[04:03:15 -> 04:03:19]  lightweight or lower Precision whatever
[04:03:17 -> 04:03:21]  you want and and essentially what this
[04:03:19 -> 04:03:23]  means is is it's the same as kublos
[04:03:21 -> 04:03:27]  except uh when you use lower Precision
[04:03:23 -> 04:03:28]  like fp16 fp8 and 8 um they're they're
[04:03:27 -> 04:03:32]  going to run way way faster and that's
[04:03:28 -> 04:03:35]  what LT has designed for so same idea
[04:03:32 -> 04:03:37]  just lower Precision bigger matrices um
[04:03:35 -> 04:03:40]  different kind of
[04:03:37 -> 04:03:42]  workloads and then you have XT which is
[04:03:40 -> 04:03:44]  I don't actually recommend this because
[04:03:42 -> 04:03:47]  it's ridiculously slow
[04:03:44 -> 04:03:50]  um but you can you can interconnect
[04:03:47 -> 04:03:52]  multiple gpus and CPUs together to solve
[04:03:50 -> 04:03:54]  a problem so if you have a massive
[04:03:52 -> 04:03:56]  Matrix uh you have a giant matrix
[04:03:54 -> 04:03:59]  multiplication to do you can actually
[04:03:56 -> 04:04:01]  split this across the CPU and GPU and
[04:03:59 -> 04:04:05]  they will talk to each other and get
[04:04:01 -> 04:04:09]  things done um however the the memory
[04:04:05 -> 04:04:10]  bandwidth bottlenecks um really limit
[04:04:09 -> 04:04:12]  the
[04:04:10 -> 04:04:15]  compute because you don't just have this
[04:04:12 -> 04:04:16]  super fast like uh this High memory
[04:04:15 -> 04:04:18]  bandwidth on the GPU that you can just
[04:04:16 -> 04:04:20]  go back and forth like it the memory
[04:04:18 -> 04:04:23]  bandwidth between the CPU and GPU is
[04:04:20 -> 04:04:25]  really low that's why it takes so long
[04:04:23 -> 04:04:27]  to copy things over so you have to worry
[04:04:25 -> 04:04:30]  about that uh your your solving speed
[04:04:27 -> 04:04:32]  actually gets slowed down a lot um and
[04:04:30 -> 04:04:35]  this is one of the the holdbacks with XT
[04:04:32 -> 04:04:37]  um but you can run multiple gpus it's
[04:04:35 -> 04:04:40]  designed to be thread safe ideal for
[04:04:37 -> 04:04:43]  large scale computations from you know
[04:04:40 -> 04:04:46]  in distributed workloads um large scale
[04:04:43 -> 04:04:48]  algebra that exceeds GPU memory so if
[04:04:46 -> 04:04:50]  you have like uh if you have giant
[04:04:48 -> 04:04:54]  matrices that it's like
[04:04:50 -> 04:04:56]  16,384 by 16,384 and you multiply that
[04:04:54 -> 04:05:00]  by itself uh that might not all fit on
[04:04:56 -> 04:05:06]  like an 8 GB card you know if I do I go
[04:05:00 -> 04:05:08]  Python and I do 16 384 squared that's uh
[04:05:06 -> 04:05:12]  what 8 m that's like
[04:05:08 -> 04:05:14]  268 that's like 268 Million numbers
[04:05:12 -> 04:05:15]  that's a lot of numbers um and if you
[04:05:14 -> 04:05:19]  have three of these if you have an a b
[04:05:15 -> 04:05:23]  and a c all allocated then that's like
[04:05:19 -> 04:05:25]  700 that's like 800 Million numbers and
[04:05:23 -> 04:05:28]  then if you you know kick this up to
[04:05:25 -> 04:05:32]  like uh fp32 you multiply by the number
[04:05:28 -> 04:05:34]  of um you multiply by four that's the
[04:05:32 -> 04:05:37]  that's the size of the that's the size
[04:05:34 -> 04:05:40]  of a float so four four bytes and you
[04:05:37 -> 04:05:43]  get ridiculous numbers like 3.2 GB of
[04:05:40 -> 04:05:45]  space so if you have a 2 GB card or it's
[04:05:43 -> 04:05:47]  like an embedded system it's not going
[04:05:45 -> 04:05:50]  to fit all that um or if you were to
[04:05:47 -> 04:05:52]  bump this up to like say
[04:05:50 -> 04:05:54]  100,000 that that's not going to fit
[04:05:52 -> 04:05:56]  right like these type of numbers are so
[04:05:54 -> 04:05:58]  massive that you just need to use
[04:05:56 -> 04:06:00]  external CPU dram in order to actually
[04:05:58 -> 04:06:02]  store them right um so you can there's
[04:06:00 -> 04:06:07]  obviously other ways of optimizing that
[04:06:02 -> 04:06:12]  but XT allows you to do this um so I did
[04:06:07 -> 04:06:15]  a run of these um where the size was 16
[04:06:12 -> 04:06:17]  384 and this all actually fit on my card
[04:06:15 -> 04:06:23]  because um you
[04:06:17 -> 04:06:26]  know I have uh 8 GB of gpv RAM which is
[04:06:23 -> 04:06:30]  low but high in some cases
[04:06:26 -> 04:06:33]  um kublos versus kublos XT so I did five
[04:06:30 -> 04:06:35]  runs each it's about 0.59 seconds on
[04:06:33 -> 04:06:39]  average and then this took about 3.5
[04:06:35 -> 04:06:41]  seconds on average so the results ended
[04:06:39 -> 04:06:44]  up matching so it was they were like
[04:06:41 -> 04:06:46]  pretty much identical uh and then yeah
[04:06:44 -> 04:06:48]  it's it's it's just like insane how much
[04:06:46 -> 04:06:52]  of a speed up that gets when you just
[04:06:48 -> 04:06:53]  stick with using the GPU right and so
[04:06:52 -> 04:06:55]  that's like one of the examples why you
[04:06:53 -> 04:06:59]  want to be careful when you use things
[04:06:55 -> 04:07:00]  like kuas XT um kuas DX is we're not
[04:06:59 -> 04:07:02]  going to be doing that we're not going
[04:07:00 -> 04:07:05]  to be using that in this course um but
[04:07:02 -> 04:07:08]  you can look more into it here with this
[04:07:05 -> 04:07:09]  documentation um now cutless is
[04:07:08 -> 04:07:11]  something I'll you know I'll cover in
[04:07:09 -> 04:07:14]  the in the extra section A little bit
[04:07:11 -> 04:07:16]  more but um
[04:07:14 -> 04:07:20]  kublos and its variance run on the
[04:07:16 -> 04:07:24]  host whatever comes with kublos DX isn't
[04:07:20 -> 04:07:27]  well super documented or optimized um
[04:07:24 -> 04:07:29]  and when we're trying to do things like
[04:07:27 -> 04:07:31]  matrix multiplication in a Transformer
[04:07:29 -> 04:07:33]  we may not want to rely on something
[04:07:31 -> 04:07:35]  where the where the operations are
[04:07:33 -> 04:07:37]  scheduled from the host when it when we
[04:07:35 -> 04:07:39]  want to call like a Koss operation the
[04:07:37 -> 04:07:41]  CPU tells it to do that right whereas on
[04:07:39 -> 04:07:43]  GPU it's just like a kernel that's
[04:07:41 -> 04:07:44]  launched and all the operations are are
[04:07:43 -> 04:07:46]  done so they're a little bit different
[04:07:44 -> 04:07:50]  in that regard
[04:07:46 -> 04:07:52]  um when we do like matrix multiplication
[04:07:50 -> 04:07:54]  along with like a r activation and then
[04:07:52 -> 04:07:55]  another matrix multiplication and then
[04:07:54 -> 04:07:58]  another value and then like a
[04:07:55 -> 04:08:00]  convolutional layer it's like you you
[04:07:58 -> 04:08:02]  typically don't have that in a single
[04:08:00 -> 04:08:04]  operation you fuse those together and
[04:08:02 -> 04:08:05]  that's this idea of fusion which we
[04:08:04 -> 04:08:09]  talked about
[04:08:05 -> 04:08:11]  before um and when when you when you
[04:08:09 -> 04:08:13]  have something like cutless which is a
[04:08:11 -> 04:08:17]  template library that is able to use
[04:08:13 -> 04:08:22]  things together um you know you can uh
[04:08:17 -> 04:08:25]  you can get a lot higher performance
[04:08:22 -> 04:08:27]  so for example um like flash attention
[04:08:25 -> 04:08:30]  it doesn't actually use cutless but it's
[04:08:27 -> 04:08:33]  an example of what fused Cuda kernels
[04:08:30 -> 04:08:35]  look like so flash attention was a paper
[04:08:33 -> 04:08:37]  that came out you know I think two three
[04:08:35 -> 04:08:41]  years ago something if we just pull this
[04:08:37 -> 04:08:43]  up it's uh you know it's essentially for
[04:08:41 -> 04:08:46]  the attention mechanism in Transformers
[04:08:43 -> 04:08:48]  and it has a whole thing and the idea is
[04:08:46 -> 04:08:50]  um you have this this this this
[04:08:48 -> 04:08:53]  attention layer in the GPT it's like
[04:08:50 -> 04:08:55]  matal drop out soft Max mask and another
[04:08:53 -> 04:08:57]  mmal and if you feed these together with
[04:08:55 -> 04:08:59]  custom handwritten highly optimized
[04:08:57 -> 04:09:01]  kernels you can make this really really
[04:08:59 -> 04:09:04]  fast and you can speed this up by like 5
[04:09:01 -> 04:09:07]  to 10x on certain hardware and that's
[04:09:04 -> 04:09:08]  the idea of fusion so cutless will allow
[04:09:07 -> 04:09:12]  you to develop you know faster matrix
[04:09:08 -> 04:09:14]  multiplication algorithms um and then
[04:09:12 -> 04:09:16]  you would take something like fusion and
[04:09:14 -> 04:09:17]  you would combine what you've maybe
[04:09:16 -> 04:09:19]  written in cutless and you would you
[04:09:17 -> 04:09:20]  would just combine everything so that
[04:09:19 -> 04:09:22]  you don't have to rely on whatever
[04:09:20 -> 04:09:24]  Nvidia provides and you can just do your
[04:09:22 -> 04:09:26]  own thing right that's that's kind of
[04:09:24 -> 04:09:29]  the reason why we do things like fusion
[04:09:26 -> 04:09:32]  and uh and and we use cutless so
[04:09:29 -> 04:09:35]  template uh template linear algebra sub
[04:09:32 -> 04:09:36]  routines um but don't worry too much
[04:09:35 -> 04:09:38]  about cutless we're not going to go over
[04:09:36 -> 04:09:41]  cut list in this course uh there's a lot
[04:09:38 -> 04:09:43]  to cover there uh but anyways that's
[04:09:41 -> 04:09:44]  like the whole idea of that's the whole
[04:09:43 -> 04:09:48]  idea of
[04:09:44 -> 04:09:50]  kuas now we dig into these I'm going to
[04:09:48 -> 04:09:53]  sort of go through this as best I can to
[04:09:50 -> 04:09:57]  illustrate what's happening so we need
[04:09:53 -> 04:09:58]  to import this uh kublos V2 right so
[04:09:57 -> 04:10:01]  this is this is a new thing we're going
[04:09:58 -> 04:10:04]  to add we're going to add F
[04:10:01 -> 04:10:06]  fp16 uh inclusions and then just some
[04:10:04 -> 04:10:09]  Matrix sizes right as macros then we're
[04:10:06 -> 04:10:11]  going to include these these macers that
[04:10:09 -> 04:10:13]  check for errors so we wrap those around
[04:10:11 -> 04:10:15]  like for example here when we when we do
[04:10:13 -> 04:10:17]  a Cuda Malik it's it's running on Cuda
[04:10:15 -> 04:10:19]  so we have to make sure that doesn't
[04:10:17 -> 04:10:20]  return an error uh we just do this
[04:10:19 -> 04:10:23]  consistently for everything to make sure
[04:10:20 -> 04:10:26]  that nothing uh just breaks everything
[04:10:23 -> 04:10:29]  goes according to plan right um and so
[04:10:26 -> 04:10:31]  in this example
[04:10:29 -> 04:10:33]  um I'm actually going to go back to a
[04:10:31 -> 04:10:36]  section there was a there was a point I
[04:10:33 -> 04:10:36]  missed
[04:10:37 -> 04:10:43]  um as we saw in the script I make these
[04:10:41 -> 04:10:45]  arrays
[04:10:43 -> 04:10:46]  and notice how these are small right
[04:10:45 -> 04:10:48]  there's a few th there there's a few
[04:10:46 -> 04:10:50]  other things like this that you want to
[04:10:48 -> 04:10:52]  watch out for when you're comparing
[04:10:50 -> 04:10:55]  things because what we're doing here is
[04:10:52 -> 04:10:58]  we're essentially comparing the speeds
[04:10:55 -> 04:10:59]  and uh how similar things are together
[04:10:58 -> 04:11:02]  we want to we want to compare like the
[04:10:59 -> 04:11:05]  fp32 with the fp16 see how they perform
[04:11:02 -> 04:11:06]  make sure they match up in the end um
[04:11:05 -> 04:11:10]  and so there's a lot of things you have
[04:11:06 -> 04:11:13]  to watch out for so doing warm-up runs
[04:11:10 -> 04:11:14]  is good because uh Cuda might encour
[04:11:13 -> 04:11:17]  some additional overhead when you do
[04:11:14 -> 04:11:21]  like the first few runs so you want to
[04:11:17 -> 04:11:22]  get those done with um and then continue
[04:11:21 -> 04:11:24]  with the Benchmark runs so make sure
[04:11:22 -> 04:11:26]  that the overhead like just gets like
[04:11:24 -> 04:11:28]  removed you want it to like just sort
[04:11:26 -> 04:11:30]  that out on its own for like a few runs
[04:11:28 -> 04:11:31]  and then you do like a 100 Benchmark
[04:11:30 -> 04:11:33]  runs and you take the average time of
[04:11:31 -> 04:11:35]  those and that's way you get an accurate
[04:11:33 -> 04:11:38]  measurement of how long it takes to
[04:11:35 -> 04:11:40]  execute a function or a kernel right um
[04:11:38 -> 04:11:42]  without doing any marup runs you might
[04:11:40 -> 04:11:44]  see like the first thing take like 50
[04:11:42 -> 04:11:46]  milliseconds and then the next one take
[04:11:44 -> 04:11:47]  2 milliseconds and it's like whoa what
[04:11:46 -> 04:11:49]  happened there well that was the
[04:11:47 -> 04:11:51]  overhead that you needed that that kuo
[04:11:49 -> 04:11:53]  actually took over and required so
[04:11:51 -> 04:11:58]  that's why you do the the warm-up
[04:11:53 -> 04:12:00]  runs um so warm-up and Benchmark runs
[04:11:58 -> 04:12:06]  you want to verify all of your results
[04:12:00 -> 04:12:06]  so uh in in here where was
[04:12:07 -> 04:12:11]  it maybe we didn't maybe we didn't
[04:12:09 -> 04:12:14]  compare results in here but we do
[04:12:11 -> 04:12:15]  somewhere else um
[04:12:14 -> 04:12:18]  you want to verify everything so that it
[04:12:15 -> 04:12:20]  all matches up and then the last one is
[04:12:18 -> 04:12:22]  when you're testing things from scratch
[04:12:20 -> 04:12:25]  instead of like randomly initializing
[04:12:22 -> 04:12:27]  massive matrices instead of having like
[04:12:25 -> 04:12:29]  thousand by thousand and just like
[04:12:27 -> 04:12:31]  random distribution you want to populate
[04:12:29 -> 04:12:32]  it with values that you can actually
[04:12:31 -> 04:12:34]  calculate on your own like something
[04:12:32 -> 04:12:36]  that you could take to your whiteboard
[04:12:34 -> 04:12:38]  or do it in your head right uh so when
[04:12:36 -> 04:12:40]  you have something that's laid out like
[04:12:38 -> 04:12:42]  this you can go ahead and write out okay
[04:12:40 -> 04:12:44]  well why did it not work right then it's
[04:12:42 -> 04:12:46]  more more easy to break down the problem
[04:12:44 -> 04:12:48]  and understand what went wrong there as
[04:12:46 -> 04:12:51]  opposed to taking apart a
[04:12:48 -> 04:12:54]  non-reproducible 1,00 by 1000 Matrix you
[04:12:51 -> 04:12:56]  just can't do that um but anyways going
[04:12:54 -> 04:13:02]  back to the point here did we initialize
[04:12:56 -> 04:13:03]  some matrices so a 3x4 and then a 4X two
[04:13:02 -> 04:13:07]  so they should have 12
[04:13:03 -> 04:13:09]  elements right we scroll back up this
[04:13:07 -> 04:13:13]  goes up to 12 and then this goes up to
[04:13:09 -> 04:13:13]  eight that's a 2x4
[04:13:13 -> 04:13:21]  we initialize uh these on the CPU so C
[04:13:18 -> 04:13:22]  on the CPU the single Precision CU loss
[04:13:21 -> 04:13:28]  output on the
[04:13:22 -> 04:13:31]  CPU and then uh the the CQ loss output
[04:13:28 -> 04:13:34]  uh for the for the half
[04:13:31 -> 04:13:36]  Precision uh and then we do a CPU mapal
[04:13:34 -> 04:13:39]  just to just to test the results just to
[04:13:36 -> 04:13:41]  populate um because again the CPU might
[04:13:39 -> 04:13:42]  be the easiest one to actually write out
[04:13:41 -> 04:13:44]  for us if we're transferring this from
[04:13:42 -> 04:13:47]  python our numpy we just write out the
[04:13:44 -> 04:13:50]  CPU everything will be super easy to uh
[04:13:47 -> 04:13:53]  compare back to it um so we just we just
[04:13:50 -> 04:13:56]  do that first we have this kublos handle
[04:13:53 -> 04:13:58]  thing which I'm going to go in into in a
[04:13:56 -> 04:14:00]  second here um how like all of these are
[04:13:58 -> 04:14:03]  how these how all of these work
[04:14:00 -> 04:14:05]  together but you have this kublos handle
[04:14:03 -> 04:14:06]  um which just gives like a kublos
[04:14:05 -> 04:14:08]  context on like what you're doing you
[04:14:06 -> 04:14:11]  just have to initialize this for safety
[04:14:08 -> 04:14:15]  we create that with a separate function
[04:14:11 -> 04:14:16]  um we do all of our our malx and then we
[04:14:15 -> 04:14:18]  have this this new function here which
[04:14:16 -> 04:14:22]  you haven't seen before this is called
[04:14:18 -> 04:14:25]  This is called sjem so single Precision
[04:14:22 -> 04:14:30]  General General with the GE matrix
[04:14:25 -> 04:14:33]  multiplication and inside of here if I
[04:14:30 -> 04:14:34]  control click on this in vs code we can
[04:14:33 -> 04:14:36]  see where this come from we can see
[04:14:34 -> 04:14:38]  where this comes from and I can right
[04:14:36 -> 04:14:40]  click on this and see where the root of
[04:14:38 -> 04:14:42]  this actually is
[04:14:40 -> 04:14:47]  right so we look at in order what all
[04:14:42 -> 04:14:49]  these are so we have the the handle the
[04:14:47 -> 04:14:50]  operation these operations I'm actually
[04:14:49 -> 04:14:52]  going to pull these up in a second so
[04:14:50 -> 04:14:55]  this makes more sense we have the shape
[04:14:52 -> 04:14:58]  so M and K the alpha term which will
[04:14:55 -> 04:15:00]  make sense in a second we have a the
[04:14:58 -> 04:15:03]  leading dimension of a so in this case
[04:15:00 -> 04:15:06]  it'll it it would be M
[04:15:03 -> 04:15:09]  um B the leading dimension of B which in
[04:15:06 -> 04:15:13]  this case would be n because it's an MN
[04:15:09 -> 04:15:14]  time n by K and so the first one going
[04:15:13 -> 04:15:17]  to have leading M and then the second
[04:15:14 -> 04:15:20]  one's going to have leading uh or sorry
[04:15:17 -> 04:15:22]  leading K I mean leading k um B is going
[04:15:20 -> 04:15:24]  to have a leading K the leading
[04:15:22 -> 04:15:27]  Dimension is going to be a k and then we
[04:15:24 -> 04:15:29]  have this beta and a c which we uh
[04:15:27 -> 04:15:30]  element wise multiply by and then
[04:15:29 -> 04:15:32]  leading dimension of C so there's a lot
[04:15:30 -> 04:15:35]  of things you have to add to this and it
[04:15:32 -> 04:15:38]  gets it gets quite bloated fast um so I
[04:15:35 -> 04:15:39]  guess to just sort of illustrate what
[04:15:38 -> 04:15:43]  all of those mean there's a lot to
[04:15:39 -> 04:15:46]  unpack here if we scroll down to
[04:15:43 -> 04:15:49]  um
[04:15:46 -> 04:15:49]  sjem
[04:15:54 -> 04:16:03]  sjem kuas sjem awesome so this is a
[04:15:59 -> 04:16:04]  super important part um this is exactly
[04:16:03 -> 04:16:07]  what we just
[04:16:04 -> 04:16:09]  saw and look at this so this is this is
[04:16:07 -> 04:16:12]  what the Matrix Matrix multiplication
[04:16:09 -> 04:16:15]  looks like so we have a c and we do
[04:16:12 -> 04:16:16]  alpha times an OP whatever operation
[04:16:15 -> 04:16:20]  which is going to be like maybe a
[04:16:16 -> 04:16:22]  transpose on a um Matrix multiply that
[04:16:20 -> 04:16:25]  with some operation on B which might be
[04:16:22 -> 04:16:31]  a transpose and then plus uh the beta
[04:16:25 -> 04:16:31]  term which beta is a
[04:16:33 -> 04:16:39]  um beta is just going to be a constant
[04:16:36 -> 04:16:43]  float number so like in this case you
[04:16:39 -> 04:16:46]  might want to have a as 1.0 and B as 0.0
[04:16:43 -> 04:16:49]  just so that you're only doing um a * b
[04:16:46 -> 04:16:52]  equals c right um but it does gener it
[04:16:49 -> 04:16:53]  does provide this this abstraction for
[04:16:52 -> 04:16:55]  you so that you can do more with it if
[04:16:53 -> 04:16:59]  you want to like add something on later
[04:16:55 -> 04:17:01]  like maybe a like maybe a bias right
[04:16:59 -> 04:17:04]  um and then we have all these other
[04:17:01 -> 04:17:08]  operations so kublos op n kublos op T
[04:17:04 -> 04:17:11]  and kublos op C so
[04:17:08 -> 04:17:13]  um we want to we want to worry about the
[04:17:11 -> 04:17:15]  n and the T here so the n is
[04:17:13 -> 04:17:18]  like no
[04:17:15 -> 04:17:21]  operations the t is a
[04:17:18 -> 04:17:21]  transpose
[04:17:23 -> 04:17:30]  um and then you have this column major
[04:17:26 -> 04:17:31]  format so column major throws a lot of
[04:17:30 -> 04:17:34]  this off so to illustrate this
[04:17:31 -> 04:17:38]  difference of column versus row major as
[04:17:34 -> 04:17:40]  we saw in uh as we saw here so matrices
[04:17:38 -> 04:17:44]  are stored in column major format with
[04:17:40 -> 04:17:48]  these um with these Dimensions m m by K
[04:17:44 -> 04:17:50]  and then K byn and c m byn um column
[04:17:48 -> 04:17:53]  Major versus row major is very important
[04:17:50 -> 04:17:55]  so it's actually a little harder because
[04:17:53 -> 04:18:00]  it's column major but we'll make do
[04:17:55 -> 04:18:00]  anyways column major
[04:18:01 -> 04:18:07]  Matrix we'll just have
[04:18:05 -> 04:18:09]  a well actually we make it row major
[04:18:07 -> 04:18:14]  first that's a good
[04:18:09 -> 04:18:14]  idea R major a equals
[04:18:22 -> 04:18:27]  uh and then call
[04:18:24 -> 04:18:27]  major
[04:18:33 -> 04:18:38]  a and then the memory layout
[04:18:45 -> 04:18:50]  so notice the difference here so we have
[04:18:48 -> 04:18:54]  a we have a
[04:18:50 -> 04:18:58]  uh a 2x4 Matrix right it goes 1 2 3 4 5
[04:18:54 -> 04:18:59]  6 7 8 then here we have 1 5 2 6 3 7 4 8
[04:18:58 -> 04:19:03]  right so the whole idea here is we
[04:18:59 -> 04:19:03]  essentially transposed it
[04:19:04 -> 04:19:09]  we the way this row goes from left to
[04:19:07 -> 04:19:13]  right it is now going from top to
[04:19:09 -> 04:19:14]  bottom and now we have to deal with this
[04:19:13 -> 04:19:16]  so if we have something that's like I
[04:19:14 -> 04:19:19]  mean typically when you're going to feed
[04:19:16 -> 04:19:22]  a matrix into a matrix multiplication
[04:19:19 -> 04:19:25]  you expect it to be in row major and
[04:19:22 -> 04:19:28]  kublos kublos sjem expects it to be in
[04:19:25 -> 04:19:30]  column major so there's a way to get
[04:19:28 -> 04:19:31]  around this and notice how this not by
[04:19:30 -> 04:19:33]  the way notice how this is laid out in
[04:19:31 -> 04:19:36]  memory this is like a very simple way of
[04:19:33 -> 04:19:37]  looking at it and then this is like okay
[04:19:36 -> 04:19:39]  that's I guess so but a little
[04:19:37 -> 04:19:45]  interesting because of you know it's
[04:19:39 -> 04:19:47]  just like 1526 1526 and that order but
[04:19:45 -> 04:19:50]  uh there's an interesting article I
[04:19:47 -> 04:19:52]  found on how to uh how to deal with this
[04:19:50 -> 04:19:56]  so this is why you see our dimensions
[04:19:52 -> 04:19:58]  are a little messed up um but on this
[04:19:56 -> 04:20:01]  pay attention to your shaping from stack
[04:19:58 -> 04:20:03]  Overflow I found an answer to this and
[04:20:01 -> 04:20:06]  how to make it actually work to your
[04:20:03 -> 04:20:06]  favor
[04:20:07 -> 04:20:14]  so where is
[04:20:10 -> 04:20:17]  it this guy pretty much said
[04:20:14 -> 04:20:19]  um kuas interprets matrices as column
[04:20:17 -> 04:20:21]  ordered so when you execute this you are
[04:20:19 -> 04:20:24]  correctly transposing uh cuz you're
[04:20:21 -> 04:20:26]  doing an opt which is going to transpose
[04:20:24 -> 04:20:29]  a and then an opt is going to transpose
[04:20:26 -> 04:20:32]  uh B you're correctly transposing each
[04:20:29 -> 04:20:33]  input um but KU stumps will result in
[04:20:32 -> 04:20:35]  column major order so you want it to
[04:20:33 -> 04:20:37]  come back in row major order so that you
[04:20:35 -> 04:20:39]  can use it for something else so what
[04:20:37 -> 04:20:43]  you end up having to do to avoid this
[04:20:39 -> 04:20:46]  whole column major mess is Trick KU into
[04:20:43 -> 04:20:48]  Computing differently um and so the way
[04:20:46 -> 04:20:52]  that you the way that you call this is
[04:20:48 -> 04:20:54]  you say handle op n OPN and you go n MK
[04:20:52 -> 04:20:59]  instead of
[04:20:54 -> 04:21:02]  mkn uh and then you go um Alpha and then
[04:20:59 -> 04:21:04]  your your B Matrix instead of a um
[04:21:02 -> 04:21:06]  because normally you would do a here but
[04:21:04 -> 04:21:10]  instead you do
[04:21:06 -> 04:21:12]  B um and then you do you're leading a
[04:21:10 -> 04:21:15]  dimension so remember n n is the leading
[04:21:12 -> 04:21:17]  a dimension now and then you have um
[04:21:15 -> 04:21:20]  then you have your a and the leading
[04:21:17 -> 04:21:22]  dimension for that is going to be um
[04:21:20 -> 04:21:24]  it's going to be K I know it's confusing
[04:21:22 -> 04:21:27]  but just bear with me and then you have
[04:21:24 -> 04:21:30]  the beta which is just like that that
[04:21:27 -> 04:21:31]  number you're going to multiply C by and
[04:21:30 -> 04:21:34]  then the leading dimension for C is
[04:21:31 -> 04:21:35]  going to be n so there's a lot there's a
[04:21:34 -> 04:21:38]  lot of different there's a lot of weird
[04:21:35 -> 04:21:39]  changes there but essentially that's
[04:21:38 -> 04:21:42]  going to that's going to avoid the
[04:21:39 -> 04:21:47]  column major issue so when we do this in
[04:21:42 -> 04:21:51]  DS code you can see that I call uh Kos
[04:21:47 -> 04:21:55]  sjem with that same idea so n n and then
[04:21:51 -> 04:21:59]  go n MK When we put our when we put
[04:21:55 -> 04:22:02]  device B Matrix we put n device a matrix
[04:21:59 -> 04:22:04]  K beta device C Matrix then n again so
[04:22:02 -> 04:22:06]  just exactly like copy and paste from
[04:22:04 -> 04:22:09]  that um and this
[04:22:06 -> 04:22:09]  works
[04:22:10 -> 04:22:17]  so this this this concept applies to H
[04:22:14 -> 04:22:20]  gem as well which is what we have below
[04:22:17 -> 04:22:22]  so I've done a single precision and a
[04:22:20 -> 04:22:24]  half precision as well um and there's
[04:22:22 -> 04:22:26]  like a little casting operation that you
[04:22:24 -> 04:22:30]  do here it's like a flow to half
[04:22:26 -> 04:22:32]  function which um we we initialize these
[04:22:30 -> 04:22:34]  as as half Precision matrices and then
[04:22:32 -> 04:22:36]  we we do a float to have conversion and
[04:22:34 -> 04:22:38]  store the result in those based on our
[04:22:36 -> 04:22:40]  index
[04:22:38 -> 04:22:43]  um camm
[04:22:40 -> 04:22:46]  copy High Precision data to the device
[04:22:43 -> 04:22:49]  so the host right this is a host sorry a
[04:22:46 -> 04:22:53]  half
[04:22:49 -> 04:22:56]  um and then we just copy back we we do
[04:22:53 -> 04:23:00]  the hgem which is the half Precision uh
[04:22:56 -> 04:23:03]  version of this and then we copy back
[04:23:00 -> 04:23:04]  um and then we we can just print out the
[04:23:03 -> 04:23:08]  results to make sure everything matches
[04:23:04 -> 04:23:11]  up so when I go
[04:23:08 -> 04:23:13]  nbcc and we have to add this little L
[04:23:11 -> 04:23:14]  like link this is for link and then we
[04:23:13 -> 04:23:17]  put kublos at the end of it that's what
[04:23:14 -> 04:23:18]  that means cuz cuos doesn't come just
[04:23:17 -> 04:23:20]  right out of the B you have to actually
[04:23:18 -> 04:23:22]  manually link it because it has to has
[04:23:20 -> 04:23:25]  to do that part separately we just go
[04:23:22 -> 04:23:29]  ahead and run this we can see our
[04:23:25 -> 04:23:33]  matrices so when we go up this is uh row
[04:23:29 -> 04:23:36]  major right 1 2 3 4 1 2 3 4 5 6 7 8 9 10
[04:23:33 -> 04:23:39]  11 12 and same one same idea with this
[04:23:36 -> 04:23:42]  one it's you know it's a 4X two so it's
[04:23:39 -> 04:23:44]  going to be four high and then two two
[04:23:42 -> 04:23:44]  wide
[04:23:46 -> 04:23:54]  um so the two the two uh width is one
[04:23:50 -> 04:23:56]  two and then 1 2 3 4 5 6 7 8 right
[04:23:54 -> 04:23:59]  everything is lined up as we want it to
[04:23:56 -> 04:24:02]  um and when we do a a row major matrix
[04:23:59 -> 04:24:03]  multiplication on the CPU we end up with
[04:24:02 -> 04:24:05]  what we're supposed to get so these
[04:24:03 -> 04:24:07]  inner Dimensions cancel out and we end
[04:24:05 -> 04:24:09]  up with the three and two like we were
[04:24:07 -> 04:24:11]  we were practicing before right and we
[04:24:09 -> 04:24:12]  end up with the three and two here these
[04:24:11 -> 04:24:16]  results these are these are our
[04:24:12 -> 04:24:19]  verification results 50 60 114 140 Etc
[04:24:16 -> 04:24:23]  we do the kuas sjem result we get the
[04:24:19 -> 04:24:26]  same shape um same
[04:24:23 -> 04:24:28]  numbers and then the HD result after we
[04:24:26 -> 04:24:31]  cast back to single
[04:24:28 -> 04:24:36]  Precision 3x two boom boom boom boom
[04:24:31 -> 04:24:38]  boom boom right so that is just a very
[04:24:36 -> 04:24:40]  uh clear concise example on how on on
[04:24:38 -> 04:24:42]  like the differences between kublos sjem
[04:24:40 -> 04:24:45]  and H gem there all all the differences
[04:24:42 -> 04:24:49]  just half single versus half precision
[04:24:45 -> 04:24:51]  and the key point of uh changing how you
[04:24:49 -> 04:24:53]  uh set this up this is the most
[04:24:51 -> 04:24:56]  important part of this entire script is
[04:24:53 -> 04:24:59]  just what it what it is what it does and
[04:24:56 -> 04:25:01]  then everything in between uh that we
[04:24:59 -> 04:25:03]  need to pay attention to in order to
[04:25:01 -> 04:25:07]  keep everything in row
[04:25:03 -> 04:25:11]  majure so now we move on to kuas LT
[04:25:07 -> 04:25:13]  so if we just pop over to um right here
[04:25:11 -> 04:25:16]  Kos LT
[04:25:13 -> 04:25:18]  so kuas LT is the lightweight version
[04:25:16 -> 04:25:21]  and it's designed to hold much bigger
[04:25:18 -> 04:25:22]  matrices and so I figured little
[04:25:21 -> 04:25:25]  something to do with that out the hard
[04:25:22 -> 04:25:29]  way and I was playing around um you
[04:25:25 -> 04:25:32]  cannot have matricies without multiples
[04:25:29 -> 04:25:34]  of four so if you have like a 3x4 um
[04:25:32 -> 04:25:36]  like three is not a multiple of four or
[04:25:34 -> 04:25:38]  a 2x4 both of them aren't multiples of
[04:25:36 -> 04:25:40]  four uh those that does not work that
[04:25:38 -> 04:25:41]  will not that will not return
[04:25:40 -> 04:25:45]  successfully but if you do have like a
[04:25:41 -> 04:25:48]  4X 4 or 4X 8 or like a 12 by 16 or
[04:25:45 -> 04:25:51]  something like that'll work fine so if
[04:25:48 -> 04:25:54]  we like right click on this just like I
[04:25:51 -> 04:25:55]  found this uh in the kublos
[04:25:54 -> 04:25:58]  documentation
[04:25:55 -> 04:26:03]  um where did where did it
[04:25:58 -> 04:26:03]  go let's search this
[04:26:04 -> 04:26:10]  up oh maybe I should copy this part um
[04:26:08 -> 04:26:14]  but yeah so if you plug if you plug that
[04:26:10 -> 04:26:18]  string into here you'll see um in the
[04:26:14 -> 04:26:21]  kuas LT matmo um it's going to be scroll
[04:26:18 -> 04:26:23]  down data ordering so four by a line
[04:26:21 -> 04:26:26]  leading dimens leading Dimensions must
[04:26:23 -> 04:26:28]  be multiples of four um Dimensions M
[04:26:26 -> 04:26:30]  andk must be multiples of four right so
[04:26:28 -> 04:26:32]  it's just universally a good idea when
[04:26:30 -> 04:26:36]  you're working with big majores don't do
[04:26:32 -> 04:26:40]  like 4,091 do 496 it's just kind of like
[04:26:36 -> 04:26:42]  makes logical sense to do that so uh
[04:26:40 -> 04:26:45]  that's that's all I kind of I wanted to
[04:26:42 -> 04:26:46]  say for that is to watch out there um
[04:26:45 -> 04:26:49]  but let's go ahead and just dive into
[04:26:46 -> 04:26:54]  like what the actual LT MCO is doing so
[04:26:49 -> 04:26:56]  we have this new include header or or um
[04:26:54 -> 04:26:59]  this new include that we have to do so
[04:26:56 -> 04:27:01]  Koss lt. and then we have the regular
[04:26:59 -> 04:27:04]  macros for checking Cuda and checking uh
[04:27:01 -> 04:27:07]  kuas for errors we have the CPU imple
[04:27:04 -> 04:27:08]  implementation to compare against um we
[04:27:07 -> 04:27:09]  have the print Matrix so that's just
[04:27:08 -> 04:27:11]  going to like conveniently print things
[04:27:09 -> 04:27:13]  for us that we can look at them and make
[04:27:11 -> 04:27:15]  sure everything lines up
[04:27:13 -> 04:27:18]  um and then notice how I make the sizes
[04:27:15 -> 04:27:20]  four four and four right so nothing less
[04:27:18 -> 04:27:23]  than that just Square Matrix Square
[04:27:20 -> 04:27:25]  matrices um I do also make sure to make
[04:27:23 -> 04:27:28]  these uh these like different elements
[04:27:25 -> 04:27:31]  so if these were a clone this was like
[04:27:28 -> 04:27:32]  one 1 to 16 and then 1 to 16 like you
[04:27:31 -> 04:27:34]  might not get the results you want so
[04:27:32 -> 04:27:36]  you want to make them a little bit
[04:27:34 -> 04:27:38]  unique so that you don't run into like
[04:27:36 -> 04:27:41]  any weird cases where you think you have
[04:27:38 -> 04:27:43]  the right answer but you don't so I
[04:27:41 -> 04:27:45]  changed this three right here to a four
[04:27:43 -> 04:27:49]  so it's four and then four and then
[04:27:45 -> 04:27:50]  instead of 13 to 16 I did 17 to 20 um
[04:27:49 -> 04:27:54]  just to mix it up a
[04:27:50 -> 04:27:57]  bit now the actual magic I mean we have
[04:27:54 -> 04:27:59]  a lot of stuff happening here where we
[04:27:57 -> 04:28:02]  um where we essentially where we
[04:27:59 -> 04:28:05]  essentially uh make like a like a fp32
[04:28:02 -> 04:28:07]  matrix we cam malic and we we populate
[04:28:05 -> 04:28:11]  those and we also have a half so this is
[04:28:07 -> 04:28:14]  just the half of a float so it's fp16 is
[04:28:11 -> 04:28:18]  what this is um and then we just
[04:28:14 -> 04:28:18]  populate those with
[04:28:18 -> 04:28:26]  uh we we we we populate these where are
[04:28:22 -> 04:28:29]  we populating it I think
[04:28:26 -> 04:28:30]  that's oh no we already populated them
[04:28:29 -> 04:28:33]  up here I'm being
[04:28:30 -> 04:28:35]  silly but we go down after we've like
[04:28:33 -> 04:28:39]  CMM copied everything this is all on the
[04:28:35 -> 04:28:41]  device now um we have this kublos LT
[04:28:39 -> 04:28:43]  handle so this is this just the handle
[04:28:41 -> 04:28:46]  that we need to create the context we go
[04:28:43 -> 04:28:48]  and create that um and then we have this
[04:28:46 -> 04:28:51]  new term called a kuas LT Matrix layout
[04:28:48 -> 04:28:52]  type um so this is just there's a few
[04:28:51 -> 04:28:54]  types we need to ensure that this goes
[04:28:52 -> 04:28:57]  properly and this is one of them so
[04:28:54 -> 04:28:58]  essentially just the shapes um and the
[04:28:57 -> 04:29:00]  data type that we're going to use so
[04:28:58 -> 04:29:03]  Cuda data type
[04:29:00 -> 04:29:06]  uh and then this this kind of follows
[04:29:03 -> 04:29:10]  the same idea as the whole column major
[04:29:06 -> 04:29:13]  thing so if we I don't know where
[04:29:10 -> 04:29:16]  exactly this was in the box but if we go
[04:29:13 -> 04:29:20]  to uh
[04:29:16 -> 04:29:23]  kuas what was it it was The Matrix kuas
[04:29:20 -> 04:29:24]  LT Matrix layout create if we just enter
[04:29:23 -> 04:29:28]  this
[04:29:24 -> 04:29:31]  in we see it's down here and if we take
[04:29:28 -> 04:29:33]  a look at the rows and columns number
[04:29:31 -> 04:29:35]  and rows and Columns of the Matrix and
[04:29:33 -> 04:29:37]  then leading Dimension leading dimension
[04:29:35 -> 04:29:39]  of the Matrix in column major layout so
[04:29:37 -> 04:29:41]  this is the same idea um and I'm going
[04:29:39 -> 04:29:42]  to show you how to like get through that
[04:29:41 -> 04:29:44]  it's it's a little bit EAS easier
[04:29:42 -> 04:29:47]  intuitively to sort of see how that how
[04:29:44 -> 04:29:50]  that pans out but yes uh we do need to
[04:29:47 -> 04:29:54]  abide by that column major rule there so
[04:29:50 -> 04:29:57]  popping back here um for for fp32 we
[04:29:54 -> 04:29:59]  just use real um and then 32f so if we
[04:29:57 -> 04:30:01]  actually look at this specific type like
[04:29:59 -> 04:30:02]  where does this come from the Cuda data
[04:30:01 -> 04:30:05]  type you can actually see a list of
[04:30:02 -> 04:30:09]  these so we have anywhere from like um
[04:30:05 -> 04:30:11]  like real num so R is real C is complex
[04:30:09 -> 04:30:14]  and then this number is the Precision so
[04:30:11 -> 04:30:16]  16 is half and then 32 is single and
[04:30:14 -> 04:30:18]  then 64 is full and then you have like
[04:30:16 -> 04:30:21]  the other you know smaller types that
[04:30:18 -> 04:30:23]  you can use uh and then like just normal
[04:30:21 -> 04:30:27]  f is uh essentially what what that
[04:30:23 -> 04:30:29]  difference is is like normal fp16 will
[04:30:27 -> 04:30:31]  have a like it'll have a sign bit that's
[04:30:29 -> 04:30:32]  either positive or negative and then
[04:30:31 -> 04:30:35]  it'll have a certain number of exponent
[04:30:32 -> 04:30:37]  bits so how how big is like the integer
[04:30:35 -> 04:30:40]  half like before before the decimal
[04:30:37 -> 04:30:42]  place and then the mantisa bits which is
[04:30:40 -> 04:30:46]  uh how precise can it be in the decimal
[04:30:42 -> 04:30:48]  places right so fp16 is going to be more
[04:30:46 -> 04:30:50]  precise in decimals and then bf16 or
[04:30:48 -> 04:30:52]  brain float 16 the reason it's called
[04:30:50 -> 04:30:55]  brain flow is because it came from
[04:30:52 -> 04:30:57]  Google brain um that that is going to
[04:30:55 -> 04:31:00]  have less mantisa and more
[04:30:57 -> 04:31:03]  exponent exponent bits so that's how
[04:31:00 -> 04:31:04]  that kind of naming scheme goes uh but
[04:31:03 -> 04:31:08]  all we need to worry about in this case
[04:31:04 -> 04:31:11]  is the um the real 32bit floating point
[04:31:08 -> 04:31:13]  and then the uh real 16bit floating
[04:31:11 -> 04:31:16]  point Point
[04:31:13 -> 04:31:19]  um so we can see that we use those here
[04:31:16 -> 04:31:21]  uh just for the 32-bit we use that and
[04:31:19 -> 04:31:24]  then when we're ordering this we want to
[04:31:21 -> 04:31:28]  do um so the first Matrix a is going to
[04:31:24 -> 04:31:30]  be of shape n m m by K so we want to
[04:31:28 -> 04:31:32]  flip that so it's going to be K by m and
[04:31:30 -> 04:31:34]  then because we've flipped it uh we need
[04:31:32 -> 04:31:39]  to put the leading Dimension here right
[04:31:34 -> 04:31:42]  so uh leading Dimension is at the end um
[04:31:39 -> 04:31:44]  and that's that's that so
[04:31:42 -> 04:31:47]  Matrix B same idea we have it's normally
[04:31:44 -> 04:31:50]  K byn so we flip that and then put the
[04:31:47 -> 04:31:53]  put the new n as the leading Dimension
[04:31:50 -> 04:31:56]  and then same idea here as well um then
[04:31:53 -> 04:31:59]  we go to the fp6 which literally just
[04:31:56 -> 04:32:00]  uses real but we replace the 32 with 16
[04:31:59 -> 04:32:03]  and we do the same exact thing with
[04:32:00 -> 04:32:06]  shapes um and then we go down to the uh
[04:32:03 -> 04:32:08]  the map description type which we just
[04:32:06 -> 04:32:11]  have to create we just have to create
[04:32:08 -> 04:32:13]  that and essentially we we pass in this
[04:32:11 -> 04:32:16]  this typ type that we Define the memory
[04:32:13 -> 04:32:19]  address to that the compute type that
[04:32:16 -> 04:32:21]  we're doing and then the data type so we
[04:32:19 -> 04:32:23]  look at this it's going to be the map M
[04:32:21 -> 04:32:26]  description with the kublos LT map M
[04:32:23 -> 04:32:28]  description type like we had here and
[04:32:26 -> 04:32:30]  then the kublos compute type which I'll
[04:32:28 -> 04:32:31]  show you in a second here and then the
[04:32:30 -> 04:32:35]  data type which we were doing before
[04:32:31 -> 04:32:36]  which is just going to be um fp32 so we
[04:32:35 -> 04:32:38]  go to this compute
[04:32:36 -> 04:32:40]  type we actually see there's a few of
[04:32:38 -> 04:32:43]  these uh you just do like control click
[04:32:40 -> 04:32:46]  to look at those um but there's a few
[04:32:43 -> 04:32:48]  here so we have like kuas compute um 16
[04:32:46 -> 04:32:49]  F which is which is what we're going to
[04:32:48 -> 04:32:52]  want for the the next one but we're
[04:32:49 -> 04:32:56]  using this one right now um you can do
[04:32:52 -> 04:32:57]  like fast you can do like fast so it'll
[04:32:56 -> 04:33:00]  it'll like change I can't remember
[04:32:57 -> 04:33:02]  exactly how it changes those inside but
[04:33:00 -> 04:33:06]  it's something in the realm of like
[04:33:02 -> 04:33:08]  accumulating and this and that um but
[04:33:06 -> 04:33:10]  yeah so these can be uh you it kind of
[04:33:08 -> 04:33:12]  just depends on like what you're doing
[04:33:10 -> 04:33:13]  if you're sticking with brain float then
[04:33:12 -> 04:33:17]  you can you could pick that type you
[04:33:13 -> 04:33:22]  could do like Fast um up to you but um
[04:33:17 -> 04:33:22]  yeah that's that's the whole idea there
[04:33:22 -> 04:33:28]  so if we pop back to this one you notice
[04:33:25 -> 04:33:30]  we're just using the compute 16 float
[04:33:28 -> 04:33:34]  and then the the data type of 16 uh
[04:33:30 -> 04:33:37]  16bit float as well um and then we're
[04:33:34 -> 04:33:39]  going to we're going to set an attribute
[04:33:37 -> 04:33:40]  and the parts of this attribute that
[04:33:39 -> 04:33:43]  we're going to need are the M
[04:33:40 -> 04:33:46]  description type which we finded earlier
[04:33:43 -> 04:33:50]  um we're going to need a uh description
[04:33:46 -> 04:33:54]  attribute which we uh which is if you
[04:33:50 -> 04:33:58]  can literally go to these um and these
[04:33:54 -> 04:33:59]  are these are essentially just the um I
[04:33:58 -> 04:34:01]  can't remember exactly what this is but
[04:33:59 -> 04:34:03]  it's probably like some transpose like
[04:34:01 -> 04:34:05]  transpose a and then transpose b or
[04:34:03 -> 04:34:07]  whatever that is um that that's that's
[04:34:05 -> 04:34:10]  what I'd assume this is I haven't looked
[04:34:07 -> 04:34:13]  into this in depth but um and then this
[04:34:10 -> 04:34:15]  one is just going to be the kuas
[04:34:13 -> 04:34:18]  operation so yeah so essentially just
[04:34:15 -> 04:34:21]  like transposing or not and then the the
[04:34:18 -> 04:34:24]  size of uh this CU loss operation type
[04:34:21 -> 04:34:26]  so uh that's just like this essentially
[04:34:24 -> 04:34:28]  so we're taking this this transpose
[04:34:26 -> 04:34:30]  operations um and that we're just
[04:34:28 -> 04:34:33]  setting what that that op is so when it
[04:34:30 -> 04:34:35]  does like op and then in Brackets a like
[04:34:33 -> 04:34:38]  that's what this is
[04:34:35 -> 04:34:42]  um and then we go ahead and do the kuas
[04:34:38 -> 04:34:46]  LTM which uh itself you know takes in a
[04:34:42 -> 04:34:52]  handle um the map uh M description
[04:34:46 -> 04:34:57]  type um an alpha a matrix layout B
[04:34:52 -> 04:35:02]  Matrix layout for B beta C uh C Matrix
[04:34:57 -> 04:35:02]  layout uh D and and Etc right
[04:35:02 -> 04:35:06]  um You can pretty much just directly
[04:35:04 -> 04:35:07]  paste these like there's a lot of these
[04:35:06 -> 04:35:08]  that we don't actually need that we can
[04:35:07 -> 04:35:10]  just set to null and aren't really
[04:35:08 -> 04:35:12]  required so don't worry too much about
[04:35:10 -> 04:35:14]  that you just kind of want things to be
[04:35:12 -> 04:35:16]  in the right order and so similar to how
[04:35:14 -> 04:35:21]  we did for regular kuas you're going to
[04:35:16 -> 04:35:22]  do the B Matrix first and then uh and
[04:35:21 -> 04:35:24]  then you're going to do the a matrix
[04:35:22 -> 04:35:26]  after
[04:35:24 -> 04:35:28]  right so that's that's kind of how that
[04:35:26 -> 04:35:30]  goes then we just do the check KU blast
[04:35:28 -> 04:35:32]  to make sure everything goes properly um
[04:35:30 -> 04:35:33]  and then and then there's a there's an
[04:35:32 -> 04:35:35]  important part that I kind of like
[04:35:33 -> 04:35:39]  messed up here and it was a little silly
[04:35:35 -> 04:35:43]  but I was trying to do this uh this uh
[04:35:39 -> 04:35:47]  16 bit floating Point kuas LT ml with uh
[04:35:43 -> 04:35:49]  with the regular Alpha types so um you
[04:35:47 -> 04:35:51]  know notice how this is like this that
[04:35:49 -> 04:35:53]  this is void it doesn't specifically say
[04:35:51 -> 04:35:56]  float so I looked I looked at that and I
[04:35:53 -> 04:35:58]  was like hm maybe maybe like we
[04:35:56 -> 04:36:00]  shouldn't use a 32-bit number and
[04:35:58 -> 04:36:02]  multiply that by a 16bit floating Point
[04:36:00 -> 04:36:06]  number maybe that that's not how it goes
[04:36:02 -> 04:36:07]  so I was like okay um I and at the time
[04:36:06 -> 04:36:09]  I was getting a bunch of zero output so
[04:36:07 -> 04:36:11]  I figured this might be a good fix and
[04:36:09 -> 04:36:14]  it ended up working so uh you pretty
[04:36:11 -> 04:36:16]  much just have to uh typ cast this so
[04:36:14 -> 04:36:18]  float to half um and then you just set
[04:36:16 -> 04:36:19]  you know your Alpha is going to be one
[04:36:18 -> 04:36:20]  your beta is going to be zero you just
[04:36:19 -> 04:36:22]  want to do a map that that's all you
[04:36:20 -> 04:36:24]  care about so that's how you're going to
[04:36:22 -> 04:36:25]  set them and then just have Alpha half
[04:36:24 -> 04:36:28]  and beta half and just set these
[04:36:25 -> 04:36:30]  accordingly um and everything will work
[04:36:28 -> 04:36:34]  according to plan
[04:36:30 -> 04:36:38]  so that's how it goes um once we're once
[04:36:34 -> 04:36:41]  we're done those we copy the uh results
[04:36:38 -> 04:36:42]  back to host uh we're not doing any any
[04:36:41 -> 04:36:43]  like benchmark here we don't worry about
[04:36:42 -> 04:36:47]  that we just want to make sure we have
[04:36:43 -> 04:36:52]  the correct results um for both the kuas
[04:36:47 -> 04:36:53]  LT uh single and the half Precision um
[04:36:52 -> 04:36:57]  and so we just we essentially copy these
[04:36:53 -> 04:37:00]  back we do a CPU ml to uh you know
[04:36:57 -> 04:37:03]  essentially get a ver a verification uh
[04:37:00 -> 04:37:06]  output so that we can compare it to
[04:37:03 -> 04:37:07]  um and then we we do the actual
[04:37:06 -> 04:37:10]  comparison itself so we use a standard
[04:37:07 -> 04:37:11]  Library absolute value we go this minus
[04:37:10 -> 04:37:14]  that it's going to get give us some
[04:37:11 -> 04:37:16]  value that's like um you know hopefully
[04:37:14 -> 04:37:20]  less than 1 * 10
[04:37:16 -> 04:37:23]  ^5 um and if that's false or or sorry if
[04:37:20 -> 04:37:25]  if this is bigger than that number if if
[04:37:23 -> 04:37:28]  it's bigger than what it tolers
[04:37:25 -> 04:37:29]  tolerates then um they it says they
[04:37:28 -> 04:37:32]  don't match and we end up returning uh
[04:37:29 -> 04:37:34]  they they don't match um so if I go
[04:37:32 -> 04:37:37]  ahead and
[04:37:34 -> 04:37:40]  actually
[04:37:37 -> 04:37:44]  uh run this we have to do link Coss and
[04:37:40 -> 04:37:46]  Link C LT cuz remember at the top here
[04:37:44 -> 04:37:50]  we did kuas lt. so we have to include
[04:37:46 -> 04:37:54]  that and then if we just uh run this we
[04:37:50 -> 04:37:58]  can see that uh we get a matrix a so row
[04:37:54 -> 04:38:00]  major Matrix B um four four and then 17
[04:37:58 -> 04:38:05]  through 20 as we as we wanted and then
[04:38:00 -> 04:38:07]  we get a c output so 106 664 106 664 106
[04:38:05 -> 04:38:09]  664 for all of these different
[04:38:07 -> 04:38:12]  precisions and we can see that these
[04:38:09 -> 04:38:14]  match with intolerance awesome so that's
[04:38:12 -> 04:38:15]  just like comparing them and making sure
[04:38:14 -> 04:38:17]  that they work the way we want them to
[04:38:15 -> 04:38:19]  so we can like carry that and Port it to
[04:38:17 -> 04:38:22]  something else
[04:38:19 -> 04:38:23]  but then for actually comparing it for
[04:38:22 -> 04:38:26]  measuring performance we have a
[04:38:23 -> 04:38:29]  different script here so I essentially
[04:38:26 -> 04:38:30]  did the same thing um don't like worry
[04:38:29 -> 04:38:33]  about a bunch of what's happening in
[04:38:30 -> 04:38:36]  here this is this is just a uh this is
[04:38:33 -> 04:38:40]  just a uh a benchmarking script that I
[04:38:36 -> 04:38:44]  wrote for for comparing very large
[04:38:40 -> 04:38:47]  matrices so we do a 496 by 1024 Times
[04:38:44 -> 04:38:49]  a24 by 496 so the inner Dimensions
[04:38:47 -> 04:38:54]  cancel out the the K is cancel out and
[04:38:49 -> 04:38:58]  then we end up with a 496 by 496 Matrix
[04:38:54 -> 04:38:59]  um and so I pretty much do a naive
[04:38:58 -> 04:39:02]  Matrix multiply because these are very
[04:38:59 -> 04:39:04]  big the CPU will take forever to do
[04:39:02 -> 04:39:06]  these so I I wrote a naive Matrix
[04:39:04 -> 04:39:08]  multiply that still gives you know a
[04:39:06 -> 04:39:12]  verifiably true answer in row major
[04:39:08 -> 04:39:15]  order and then you know we have our our
[04:39:12 -> 04:39:18]  our normal distribution uh random number
[04:39:15 -> 04:39:22]  generator here that you know ensures
[04:39:18 -> 04:39:24]  everything is is kind of just goes as we
[04:39:22 -> 04:39:28]  want some essentially like when you do
[04:39:24 -> 04:39:30]  torch. randn it's going to just do that
[04:39:28 -> 04:39:33]  um it's going to make it nor like Rand n
[04:39:30 -> 04:39:35]  is for normally uh randomly normally
[04:39:33 -> 04:39:38]  distributed um that's that's what this
[04:39:35 -> 04:39:39]  is doing and then verify results same
[04:39:38 -> 04:39:41]  idea does it does the relative error
[04:39:39 -> 04:39:45]  match with intolerance
[04:39:41 -> 04:39:47]  um we do a timing so with it with our uh
[04:39:45 -> 04:39:49]  with our our previous streams so we
[04:39:47 -> 04:39:52]  don't actually like put the stream in
[04:39:49 -> 04:39:54]  but we just want to uh record the time
[04:39:52 -> 04:39:57]  and do this elapse time thing uh and
[04:39:54 -> 04:39:59]  then just return that to measure it um
[04:39:57 -> 04:40:02]  on the actual device itself uh and then
[04:39:59 -> 04:40:04]  we Benchmark and we just have a bunch of
[04:40:02 -> 04:40:05]  other stuff filled in in between here
[04:40:04 -> 04:40:08]  the same as what you saw in the in the
[04:40:05 -> 04:40:10]  previous script um and then we we just
[04:40:08 -> 04:40:12]  end up printing out the average time
[04:40:10 -> 04:40:15]  afterwards
[04:40:12 -> 04:40:16]  so um it'll also return the max error we
[04:40:15 -> 04:40:19]  get as well consider there is some error
[04:40:16 -> 04:40:23]  with fp16 so we should know about that
[04:40:19 -> 04:40:26]  too um but if I I just print this out
[04:40:23 -> 04:40:26]  here two
[04:40:29 -> 04:40:36]  compare we print this out notice
[04:40:33 -> 04:40:39]  so okay so KU loss results match the
[04:40:36 -> 04:40:41]  naive kernel with intolerance results
[04:40:39 -> 04:40:43]  match results match and results match
[04:40:41 -> 04:40:44]  awesome so everything is lined up um I
[04:40:43 -> 04:40:46]  did give it some additional tolerance
[04:40:44 -> 04:40:48]  here just because of some of the error
[04:40:46 -> 04:40:51]  but here uh we notice that this error is
[04:40:48 -> 04:40:53]  not super significant um it it's not
[04:40:51 -> 04:40:55]  this is the maximum error so most of it
[04:40:53 -> 04:40:57]  is going to be very insignificant
[04:40:55 -> 04:40:59]  compared to this this is just like a
[04:40:57 -> 04:41:03]  side edge case which might pop up a few
[04:40:59 -> 04:41:05]  times uh very like not often at all so
[04:41:03 -> 04:41:08]  when we actually look at the times we
[04:41:05 -> 04:41:11]  can see normal CU loss gives us fp32
[04:41:08 -> 04:41:14]  average time of 2.5 milliseconds uh kuas
[04:41:11 -> 04:41:16]  LT gives us an an average of 2 point
[04:41:14 -> 04:41:21]  about 2.8 milliseconds which isn't
[04:41:16 -> 04:41:25]  amazing uh and then kuas LT FP 16 gives
[04:41:21 -> 04:41:27]  us 63 milliseconds and then LT gives us
[04:41:25 -> 04:41:32]  um about four
[04:41:27 -> 04:41:33]  uh 46 milliseconds which is really fast
[04:41:32 -> 04:41:35]  compared to this naive kernel that we've
[04:41:33 -> 04:41:37]  written before so this took 28
[04:41:35 -> 04:41:39]  milliseconds to right this is about the
[04:41:37 -> 04:41:42]  time that it takes me to Ping the Google
[04:41:39 -> 04:41:46]  servers about 28 Mill seconds for it to
[04:41:42 -> 04:41:48]  run the complete naive kernel and the LT
[04:41:46 -> 04:41:52]  took Point about5 milliseconds that is
[04:41:48 -> 04:41:54]  insanely fast um like look at the if you
[04:41:52 -> 04:41:57]  just look back at at at how we were we
[04:41:54 -> 04:42:00]  were doing a matrix multiplication of
[04:41:57 -> 04:42:04]  like taking the column and do producting
[04:42:00 -> 04:42:06]  it with a row like you have a it's like
[04:42:04 -> 04:42:09]  size uh
[04:42:06 -> 04:42:11]  1,24 and it does that and it it does for
[04:42:09 -> 04:42:15]  every single combination it's it and it
[04:42:11 -> 04:42:18]  does all of that in uh half a
[04:42:15 -> 04:42:21]  millisecond so that's that's about 5
[04:42:18 -> 04:42:25]  10,000 of a second uh but yeah anyways
[04:42:21 -> 04:42:27]  that's that is a kublos LT at this point
[04:42:25 -> 04:42:30]  you might be a little upset or
[04:42:27 -> 04:42:32]  frustrated about why I'm just uh showing
[04:42:30 -> 04:42:34]  you code and then reviewing it and not
[04:42:32 -> 04:42:35]  like writing it from scratch like
[04:42:34 -> 04:42:37]  starting from the very top like let's
[04:42:35 -> 04:42:39]  define these and then write this and
[04:42:37 -> 04:42:41]  then write this the point is like all of
[04:42:39 -> 04:42:43]  this it is a lot there's a lot of lines
[04:42:41 -> 04:42:46]  I'm not going to type this all manually
[04:42:43 -> 04:42:48]  the course is uh long enough already as
[04:42:46 -> 04:42:51]  it is I'm not going to make it 10 times
[04:42:48 -> 04:42:54]  longer by writing everything uh by hand
[04:42:51 -> 04:42:58]  um but yeah you you you kind of get the
[04:42:54 -> 04:43:00]  point you I identify the main the main
[04:42:58 -> 04:43:02]  uh material that you need to learn the
[04:43:00 -> 04:43:03]  most important stuff and I highlight it
[04:43:02 -> 04:43:05]  um but I don't need to highlight
[04:43:03 -> 04:43:08]  everything like writing check KU loss is
[04:43:05 -> 04:43:11]  just redundant writing
[04:43:08 -> 04:43:12]  um I don't know like writing the writing
[04:43:11 -> 04:43:15]  this stuff it's just redundant like you
[04:43:12 -> 04:43:17]  already know what that is um so that's
[04:43:15 -> 04:43:18]  that's kind of why I'm sticking away
[04:43:17 -> 04:43:20]  from that side and just trying to kind
[04:43:18 -> 04:43:22]  of trying to make like Fast progress
[04:43:20 -> 04:43:24]  here um we might write out some things
[04:43:22 -> 04:43:25]  later on just just to kind of help you
[04:43:24 -> 04:43:27]  understand it when it gets more
[04:43:25 -> 04:43:29]  intuitive and and you're actually like
[04:43:27 -> 04:43:32]  building stuff but right now uh we don't
[04:43:29 -> 04:43:34]  this is not very conceptually hard so
[04:43:32 -> 04:43:37]  we're just kind of flying through it but
[04:43:34 -> 04:43:39]  this next part is on kublos XT so it's
[04:43:37 -> 04:43:41]  essentially the same thing as what we've
[04:43:39 -> 04:43:43]  just done except it's a bit different we
[04:43:41 -> 04:43:46]  still do the handle um we still create
[04:43:43 -> 04:43:47]  it we do this new thing called device
[04:43:46 -> 04:43:49]  select which remember when I talked
[04:43:47 -> 04:43:51]  about um how you can have multiple gpus
[04:43:49 -> 04:43:55]  and and do a computation across multiple
[04:43:51 -> 04:43:58]  gpus in the host that's what this is so
[04:43:55 -> 04:44:01]  we we essentially just do this little
[04:43:58 -> 04:44:03]  hack um and we just device select
[04:44:01 -> 04:44:05]  whatever the main GPU device is um and
[04:44:03 -> 04:44:06]  that's that's how we do things across so
[04:44:05 -> 04:44:09]  this is this is just a little hack when
[04:44:06 -> 04:44:12]  you have one one GPU so that you do
[04:44:09 -> 04:44:14]  this um and then we do this XTS gem
[04:44:12 -> 04:44:16]  which is the same exact thing as we've
[04:44:14 -> 04:44:17]  been doing before except you have these
[04:44:16 -> 04:44:21]  things that are on the host and then
[04:44:17 -> 04:44:22]  they're managed by um this this sjem
[04:44:21 -> 04:44:24]  function so you don't have to actually
[04:44:22 -> 04:44:26]  move anything to device you just you
[04:44:24 -> 04:44:28]  don't even I I don't have a single camm
[04:44:26 -> 04:44:31]  copy in here
[04:44:28 -> 04:44:33]  um you just pass in your matrices on the
[04:44:31 -> 04:44:35]  host and it'll manage all of that memory
[04:44:33 -> 04:44:37]  back and forth but at some performance
[04:44:35 -> 04:44:41]  cost so you'll see that in a second when
[04:44:37 -> 04:44:41]  I compile this um
[04:44:44 -> 04:44:48]  we don't actually need to link LT but
[04:44:45 -> 04:44:48]  that's
[04:44:48 -> 04:44:52]  fine um and so you can see maximum
[04:44:50 -> 04:44:57]  difference between CPU and GPU results
[04:44:52 -> 04:44:57]  right um and then if we go to
[04:44:58 -> 04:45:03]  say and then we just link KU
[04:45:06 -> 04:45:10]  Bloss we'll give that a second here I
[04:45:08 -> 04:45:14]  did I did the exact same thing as as our
[04:45:10 -> 04:45:17]  Koss LT but for XT instead and made very
[04:45:14 -> 04:45:21]  big matrices so 16 384 you notice the
[04:45:17 -> 04:45:23]  kuo Run does uh but you know like I like
[04:45:21 -> 04:45:26]  I think I highlighted this before but 0.
[04:45:23 -> 04:45:28]  59 seconds or 6 seconds on average and
[04:45:26 -> 04:45:32]  then the kuas LT is going to be way
[04:45:28 -> 04:45:35]  longer than that so uh we'll we'll just
[04:45:32 -> 04:45:37]  give this a second here to finish um but
[04:45:35 -> 04:45:40]  yeah you you you get my point it takes a
[04:45:37 -> 04:45:43]  while you don't want to run this maybe
[04:45:40 -> 04:45:45]  you don't want to run this in
[04:45:43 -> 04:45:47]  production um so we get the average time
[04:45:45 -> 04:45:49]  everything matches with in toolerance
[04:45:47 -> 04:45:54]  and we're good um but yeah that's that's
[04:45:49 -> 04:45:54]  kublos XT for you um go and delete
[04:45:55 -> 04:46:01]  these I hope you enjoy that section on
[04:45:57 -> 04:46:03]  kuas or Cuda basic linear algebra
[04:46:01 -> 04:46:07]  subprograms uh that was that was quite a
[04:46:03 -> 04:46:09]  bit hey uh we got a second part on CNN
[04:46:07 -> 04:46:13]  so when you do when you do um for
[04:46:09 -> 04:46:17]  example like P install torch like
[04:46:13 -> 04:46:20]  this uh and you see like uh where is it
[04:46:17 -> 04:46:21]  kublos qnn right that's this is where
[04:46:20 -> 04:46:22]  this is where it's kind of coming from
[04:46:21 -> 04:46:24]  right and you have all the other things
[04:46:22 -> 04:46:26]  like q fft and C random number
[04:46:24 -> 04:46:29]  generators and CP solver and CP sparse
[04:46:26 -> 04:46:31]  and Collective Communications across
[04:46:29 -> 04:46:33]  multiple nodes profilers Triton which
[04:46:31 -> 04:46:35]  we'll go into later right I mean this is
[04:46:33 -> 04:46:38]  this is why I'm covering this stuff so
[04:46:35 -> 04:46:40]  that you can uh so you can kind of work
[04:46:38 -> 04:46:42]  with it and you understand how Pi torch
[04:46:40 -> 04:46:45]  Works under the Hood right that's one of
[04:46:42 -> 04:46:48]  the main reasons I'm putting this out so
[04:46:45 -> 04:46:50]  when you go into CNN um there's there's
[04:46:48 -> 04:46:51]  actually a lot more to unpack here as
[04:46:50 -> 04:46:54]  compared to kuas but it's not
[04:46:51 -> 04:46:56]  conceptually hard um some of it is a
[04:46:54 -> 04:47:01]  little bit intuitive but not really um
[04:46:56 -> 04:47:03]  so CNN is not entirely for matrix
[04:47:01 -> 04:47:05]  multiplication it does matrix
[04:47:03 -> 04:47:07]  multiplication in some operations to
[04:47:05 -> 04:47:08]  really speed things up uh but it's not a
[04:47:07 -> 04:47:10]  performance bottleneck you don't
[04:47:08 -> 04:47:12]  actually explicitly do matrix
[04:47:10 -> 04:47:14]  multiplication in CNN that's not a thing
[04:47:12 -> 04:47:17]  that's what kblast handles right um so
[04:47:14 -> 04:47:18]  in CNN you're going to deal with things
[04:47:17 -> 04:47:22]  like
[04:47:18 -> 04:47:25]  convolutions um pooling layers soft Max
[04:47:22 -> 04:47:27]  um Dropout right batch
[04:47:25 -> 04:47:30]  normalization uh tensor Transformations
[04:47:27 -> 04:47:33]  like reshaping and concatenation layer
[04:47:30 -> 04:47:35]  Norm all this right so all these other
[04:47:33 -> 04:47:37]  deep learning operations other than
[04:47:35 -> 04:47:39]  matrix multiplication is what qnn is
[04:47:37 -> 04:47:41]  going to cover a lot of um a lot of the
[04:47:39 -> 04:47:44]  a lot of the common most Comm L used
[04:47:41 -> 04:47:45]  ones um so this is this is kind of why
[04:47:44 -> 04:47:47]  I'm bringing you to the docs here is
[04:47:45 -> 04:47:50]  because this there's this is a super
[04:47:47 -> 04:47:52]  direct interface with everything um so
[04:47:50 -> 04:47:55]  they actually have their own thing Doc
[04:47:52 -> 04:47:57]  Nvidia deeplearning CNN and you go there
[04:47:55 -> 04:47:59]  and it brings you to this page so
[04:47:57 -> 04:48:00]  there's multiple things here we have
[04:47:59 -> 04:48:02]  like a getting started or installation
[04:48:00 -> 04:48:04]  guide which we don't care about and then
[04:48:02 -> 04:48:06]  the other ones the important ones which
[04:48:04 -> 04:48:08]  is backend API and developer guide so
[04:48:06 -> 04:48:11]  we're going to be looking at these two
[04:48:08 -> 04:48:14]  today and doing some examples of C DNN
[04:48:11 -> 04:48:18]  operations and comparing them so if we
[04:48:14 -> 04:48:20]  go to like backend API overview um we
[04:48:18 -> 04:48:23]  can see it's like
[04:48:20 -> 04:48:26]  um Cuda so it supports the Cuda streams
[04:48:23 -> 04:48:27]  that we were talking about before um it
[04:48:26 -> 04:48:30]  has multiple things that it's kind of
[04:48:27 -> 04:48:31]  like linked to so there's like I I don't
[04:48:30 -> 04:48:33]  I wouldn't pay attention to that
[04:48:31 -> 04:48:35]  entirely but um you have these you have
[04:48:33 -> 04:48:40]  essentially have these three parts so
[04:48:35 -> 04:48:46]  you have qnn graph qnn Ops qnn CNN and
[04:48:40 -> 04:48:49]  adver serial so graph is going to it
[04:48:46 -> 04:48:51]  it's not uh it's not it doesn't support
[04:48:49 -> 04:48:53]  like graph operations where you're like
[04:48:51 -> 04:48:55]  dealing with graphs it's more so how do
[04:48:53 -> 04:48:57]  you combine operations together in the
[04:48:55 -> 04:48:59]  form of a graph so when you're doing
[04:48:57 -> 04:49:00]  like a like a convolution layer and then
[04:48:59 -> 04:49:03]  you're adding a bias and then you're
[04:49:00 -> 04:49:04]  doing a Max pool after it like that
[04:49:03 -> 04:49:06]  that's going to look like a graph right
[04:49:04 -> 04:49:09]  you're going to have these nodes
[04:49:06 -> 04:49:11]  essentially where it's like a node is an
[04:49:09 -> 04:49:14]  operation and an edge is a a tensor
[04:49:11 -> 04:49:17]  right or a matrix and so it' be like
[04:49:14 -> 04:49:19]  convolution 2D and then there's going to
[04:49:17 -> 04:49:21]  be an edge which is the which is the
[04:49:19 -> 04:49:23]  data flowing from from the output of
[04:49:21 -> 04:49:25]  here to the input of the next node and
[04:49:23 -> 04:49:27]  that next node might be like the bias
[04:49:25 -> 04:49:30]  that it adds right uh and then and then
[04:49:27 -> 04:49:33]  it's going to flow out of the bias into
[04:49:30 -> 04:49:35]  uh like a Max pool layer or average pool
[04:49:33 -> 04:49:36]  layer and then it's going to go from
[04:49:35 -> 04:49:38]  there and so instead of doing these
[04:49:36 -> 04:49:40]  separately where you do a separate
[04:49:38 -> 04:49:42]  function call for uh convolution bias so
[04:49:40 -> 04:49:44]  you do like a manual bias kernel and
[04:49:42 -> 04:49:47]  then a manual Max pool kernel you just
[04:49:44 -> 04:49:49]  fuse these all into one and you keep
[04:49:47 -> 04:49:52]  track of all of your all of your uh data
[04:49:49 -> 04:49:54]  in between and it does that for you so q
[04:49:52 -> 04:49:56]  that that's what the whole qnn graph
[04:49:54 -> 04:49:58]  thing is um it supports you know both
[04:49:56 -> 04:50:00]  forward and backward path so when you're
[04:49:58 -> 04:50:03]  going through calculating the prediction
[04:50:00 -> 04:50:05]  predictions uh and when you're uh
[04:50:03 -> 04:50:07]  modifying all the gradients and and back
[04:50:05 -> 04:50:09]  propagating through it supports both of
[04:50:07 -> 04:50:11]  those right so it's designed for it's
[04:50:09 -> 04:50:12]  designed for kind of
[04:50:11 -> 04:50:14]  uh just putting something in place
[04:50:12 -> 04:50:16]  instead of having to write like all the
[04:50:14 -> 04:50:20]  kernels from scratch is just kind of
[04:50:16 -> 04:50:23]  makes your life easier that way um so
[04:50:20 -> 04:50:26]  there are multiple things within a CNN
[04:50:23 -> 04:50:30]  graph so we have these pre-compiled
[04:50:26 -> 04:50:32]  engines runtime compiled engines um I
[04:50:30 -> 04:50:36]  have this pulled up on my second monitor
[04:50:32 -> 04:50:36]  here which I'll probably just bring over
[04:50:39 -> 04:50:43]  um where did it
[04:50:44 -> 04:50:48]  go yes so the pre-compiled single
[04:50:47 -> 04:50:50]  operation engines I'm going to make this
[04:50:48 -> 04:50:52]  more readable the pre-compiled single
[04:50:50 -> 04:50:54]  operation engines pre-compiled and
[04:50:52 -> 04:50:56]  optimized for a single specific
[04:50:54 -> 04:50:58]  operation like a convolution because
[04:50:56 -> 04:51:00]  they're pre-compiled they offer very
[04:50:58 -> 04:51:02]  efficient execution and are inflexible
[04:51:00 -> 04:51:04]  in terms of operations they can perform
[04:51:02 -> 04:51:06]  right they're comp compiled down to
[04:51:04 -> 04:51:08]  machine code they only do one specific
[04:51:06 -> 04:51:10]  function on something but it goes very
[04:51:08 -> 04:51:13]  fast because of all the optimizations in
[04:51:10 -> 04:51:15]  b area that it has right um so like for
[04:51:13 -> 04:51:16]  example a major multiplication engine
[04:51:15 -> 04:51:18]  that is pre-compiled and optimized
[04:51:16 -> 04:51:20]  specifically for that operation right
[04:51:18 -> 04:51:23]  like similar to convolutions um and then
[04:51:20 -> 04:51:26]  there's generic runtime Fusion
[04:51:23 -> 04:51:29]  engines designed to dynamically fuse
[04:51:26 -> 04:51:31]  mult multiple operations at runtime so
[04:51:29 -> 04:51:32]  offer more flexibility compared to
[04:51:31 -> 04:51:34]  pre-compiled because they're generic and
[04:51:32 -> 04:51:36]  they can adapt right uh these are things
[04:51:34 -> 04:51:40]  that would that would happen during the
[04:51:36 -> 04:51:42]  compilation um not might not be as as
[04:51:40 -> 04:51:44]  high performance optimized but they're
[04:51:42 -> 04:51:46]  they are generic and they can and they
[04:51:44 -> 04:51:48]  can they act as like a generic fuser of
[04:51:46 -> 04:51:50]  operations together so you will get
[04:51:48 -> 04:51:52]  those performance benefits but they're
[04:51:50 -> 04:51:54]  not going to be as high so you still get
[04:51:52 -> 04:51:55]  them but they're not going to be um
[04:51:54 -> 04:51:57]  they're not going to be as high as
[04:51:55 -> 04:51:59]  something customly written for that
[04:51:57 -> 04:52:02]  algorithm then you have a specialized
[04:51:59 -> 04:52:04]  runtime Fusion engine similar to generic
[04:52:02 -> 04:52:06]  runtime Fusion engines uh but they're
[04:52:04 -> 04:52:08]  typically uh specifically optimized for
[04:52:06 -> 04:52:12]  certain patterns or combinations of
[04:52:08 -> 04:52:14]  operations right so offering runtime
[04:52:12 -> 04:52:18]  flexibility and leveraging optimizations
[04:52:14 -> 04:52:20]  for particular use cases or operation
[04:52:18 -> 04:52:23]  sequences and then for example an engine
[04:52:20 -> 04:52:24]  optimized for fusing convolution layers
[04:52:23 -> 04:52:26]  followed by activation functions in
[04:52:24 -> 04:52:28]  neural networks like similar how similar
[04:52:26 -> 04:52:29]  to how I was talking about before you
[04:52:28 -> 04:52:32]  have like convolution and then a bias
[04:52:29 -> 04:52:34]  and then uh convolution bias and then a
[04:52:32 -> 04:52:37]  Max pull layer uh similar similar to
[04:52:34 -> 04:52:39]  that right um It'll recognize your code
[04:52:37 -> 04:52:42]  architecture uh and it'll find the fuse
[04:52:39 -> 04:52:44]  patterns where you would get a speed up
[04:52:42 -> 04:52:47]  so um it's it's going to just it's going
[04:52:44 -> 04:52:49]  to be um it's going to be smart right
[04:52:47 -> 04:52:50]  it's going to try to be smart when it's
[04:52:49 -> 04:52:52]  when it's compil this down and seeing
[04:52:50 -> 04:52:54]  where you could actually get a speed up
[04:52:52 -> 04:52:57]  from
[04:52:54 -> 04:52:59]  um and then you have the specialized
[04:52:57 -> 04:53:01]  pre-compiled so pre-compiled for
[04:52:59 -> 04:53:03]  specific
[04:53:01 -> 04:53:05]  sequences they offer the same high
[04:53:03 -> 04:53:07]  performance as pre-compiled single
[04:53:05 -> 04:53:09]  operations so these ones that were
[04:53:07 -> 04:53:11]  really fast uh but can handle sequences
[04:53:09 -> 04:53:13]  of operations rather than just single
[04:53:11 -> 04:53:16]  ones so these are
[04:53:13 -> 04:53:18]  actually these are actually amazing if
[04:53:16 -> 04:53:19]  you are trying to do a lot of layers
[04:53:18 -> 04:53:22]  like if you have a whole like say a
[04:53:19 -> 04:53:23]  Transformer Block in a neural network uh
[04:53:22 -> 04:53:25]  and you want to do that entire attention
[04:53:23 -> 04:53:27]  block this is an example of what that
[04:53:25 -> 04:53:29]  would be so you have a lot of different
[04:53:27 -> 04:53:31]  operations that you're doing in there
[04:53:29 -> 04:53:35]  but if you just have this wrapper that
[04:53:31 -> 04:53:36]  says multi-ad attention you call that uh
[04:53:35 -> 04:53:38]  you get everything you put everything in
[04:53:36 -> 04:53:40]  that you need and you get everything out
[04:53:38 -> 04:53:42]  that's useful and then you can continue
[04:53:40 -> 04:53:44]  on and it's going to be highly optimized
[04:53:42 -> 04:53:46]  pre-compiled into binary specifically
[04:53:44 -> 04:53:48]  for that right so this is kind of how
[04:53:46 -> 04:53:49]  qnn is structured and this is these are
[04:53:48 -> 04:53:50]  the things you're going to you know want
[04:53:49 -> 04:53:52]  to pay attention to when you're trying
[04:53:50 -> 04:53:54]  to optimize when you're looking out for
[04:53:52 -> 04:53:57]  how you can take advantage of underlying
[04:53:54 -> 04:54:01]  qnn features
[04:53:57 -> 04:54:04]  um so uh you know there's an example of
[04:54:01 -> 04:54:07]  like runtime Fusion right here um and
[04:54:04 -> 04:54:09]  then if we go back to the graph API if
[04:54:07 -> 04:54:11]  we go back I know it's bright you'll be
[04:54:09 -> 04:54:14]  fine um
[04:54:11 -> 04:54:16]  we pop over to this this graph API I
[04:54:14 -> 04:54:21]  think that's where it was yeah graph API
[04:54:16 -> 04:54:23]  with operation Fusion so
[04:54:21 -> 04:54:27]  um convolution
[04:54:23 -> 04:54:30]  forward pointwise bias pointwise value
[04:54:27 -> 04:54:33]  right um that's that's kind of the whole
[04:54:30 -> 04:54:35]  idea and if you wanted to fuse these
[04:54:33 -> 04:54:37]  together you would do like uh you'd have
[04:54:35 -> 04:54:40]  like essentially some organization you
[04:54:37 -> 04:54:41]  have three tensors that you input um and
[04:54:40 -> 04:54:44]  and that would be like their variable
[04:54:41 -> 04:54:46]  names uh and then like these two would
[04:54:44 -> 04:54:49]  go through here like your um like your X
[04:54:46 -> 04:54:52]  and then your your W your weight kernel
[04:54:49 -> 04:54:53]  the convolution filter itself is your W
[04:54:52 -> 04:54:56]  and that would output something with
[04:54:53 -> 04:54:58]  this arrow and then this one this bias
[04:54:56 -> 04:54:59]  would come in um and it would
[04:54:58 -> 04:55:01]  essentially add to the output of that
[04:54:59 -> 04:55:03]  convolution and then you would do a
[04:55:01 -> 04:55:06]  pointwise rue which is it's just
[04:55:03 -> 04:55:10]  a it literally just goes one by one
[04:55:06 -> 04:55:12]  through through each element um
[04:55:10 -> 04:55:14]  and then you get your output so that
[04:55:12 -> 04:55:16]  that's the whole idea of a graph is like
[04:55:14 -> 04:55:18]  you have these which is your data flow
[04:55:16 -> 04:55:20]  the edges is your is your actual data
[04:55:18 -> 04:55:21]  and where it's flowing to and then the
[04:55:20 -> 04:55:24]  points the nodes themselves are the
[04:55:21 -> 04:55:27]  operations so um continuing to go
[04:55:24 -> 04:55:31]  through this um you know inputs
[04:55:27 -> 04:55:34]  convolution backward so Alpha Beta Dy uh
[04:55:31 -> 04:55:35]  W and DX um and then you would end up
[04:55:34 -> 04:55:38]  getting
[04:55:35 -> 04:55:40]  um yeah you you you kind of get the
[04:55:38 -> 04:55:42]  point you put whatever in is required
[04:55:40 -> 04:55:45]  for an operation and you get whatever
[04:55:42 -> 04:55:46]  out is is useful right um especially
[04:55:45 -> 04:55:48]  important you pay attention to that in
[04:55:46 -> 04:55:49]  the backward pass of things because
[04:55:48 -> 04:55:52]  there's going to be more data you'll
[04:55:49 -> 04:55:54]  have to take care of
[04:55:52 -> 04:55:57]  um but yeah these these kind of work the
[04:55:54 -> 04:56:00]  same way all around um normalization you
[04:55:57 -> 04:56:01]  have like your mean Epsilon and variance
[04:56:00 -> 04:56:05]  um then your
[04:56:01 -> 04:56:08]  scale continuing to go forward um same
[04:56:05 -> 04:56:09]  ideas generic runtime Fusion engines you
[04:56:08 -> 04:56:12]  can kind of just scroll through this and
[04:56:09 -> 04:56:15]  get the idea about how everything is
[04:56:12 -> 04:56:17]  architected
[04:56:15 -> 04:56:19]  um there's there's quite a bit here I
[04:56:17 -> 04:56:21]  don't expect that you'll read all of
[04:56:19 -> 04:56:24]  this um
[04:56:21 -> 04:56:27]  but yeah that's that's pretty much how
[04:56:24 -> 04:56:31]  the whole uh Fusion engine thing works I
[04:56:27 -> 04:56:33]  know it's a kind of a a silly term but
[04:56:31 -> 04:56:35]  it it is you're effectively fusing
[04:56:33 -> 04:56:38]  operations together and you could say
[04:56:35 -> 04:56:39]  that acts as like an engine right so if
[04:56:38 -> 04:56:41]  we go back to this VSS code here that
[04:56:39 -> 04:56:44]  I've opened the read me file you can
[04:56:41 -> 04:56:46]  actually find these in here so just you
[04:56:44 -> 04:56:49]  know a bunch of um bunch of stuff but
[04:56:46 -> 04:56:53]  like the graph API very important um
[04:56:49 -> 04:56:55]  Matt Mo um convolution forward backward
[04:56:53 -> 04:56:57]  backward data
[04:56:55 -> 04:56:59]  pointwise
[04:56:57 -> 04:57:02]  um yeah just like pretty much some of
[04:56:59 -> 04:57:04]  the images copy and pasted and then this
[04:57:02 -> 04:57:07]  one was this one was actually
[04:57:04 -> 04:57:09]  support uh that this one is support for
[04:57:07 -> 04:57:12]  the different Compu capabilities right
[04:57:09 -> 04:57:15]  so if you have like for example if if I
[04:57:12 -> 04:57:16]  did um can't remember what it was it was
[04:57:15 -> 04:57:18]  like device query remember that when we
[04:57:16 -> 04:57:19]  printed out the computer capability and
[04:57:18 -> 04:57:25]  mine was 8 uh
[04:57:19 -> 04:57:28]  8.6 so um I would actually not get the
[04:57:25 -> 04:57:29]  uh convolution backward filter fusions I
[04:57:28 -> 04:57:32]  would not get this because it's only
[04:57:29 -> 04:57:33]  supported on 9.0 and up right so pay
[04:57:32 -> 04:57:35]  attention to things like that when
[04:57:33 -> 04:57:37]  you're trying to fuse things together
[04:57:35 -> 04:57:39]  for like research production purposes
[04:57:37 -> 04:57:41]  you want to pay attention to what is
[04:57:39 -> 04:57:43]  supported on your Ware or whatever
[04:57:41 -> 04:57:45]  Hardware you're working on um so that
[04:57:43 -> 04:57:47]  you don't like try to do something and
[04:57:45 -> 04:57:49]  have it not work and waste time so it's
[04:57:47 -> 04:57:51]  good to just like double check with this
[04:57:49 -> 04:57:53]  stuff and see this is all in the CNN
[04:57:51 -> 04:57:56]  docs
[04:57:53 -> 04:57:57]  but there's still a few more sections we
[04:57:56 -> 04:58:00]  have to cover so I'm going to dig into
[04:57:57 -> 04:58:03]  those um we have the Ops API which I'll
[04:58:00 -> 04:58:04]  dig into next is is very simple um if we
[04:58:03 -> 04:58:09]  just go
[04:58:04 -> 04:58:11]  to here go to Ops um essentially you
[04:58:09 -> 04:58:14]  have these these same opaque stru types
[04:58:11 -> 04:58:16]  as you did with kuas um except they do
[04:58:14 -> 04:58:19]  different operations so you can do like
[04:58:16 -> 04:58:22]  um like you can like create tensors
[04:58:19 -> 04:58:24]  pooling um filter Dropout loss
[04:58:22 -> 04:58:26]  activation all of this so the actual
[04:58:24 -> 04:58:28]  functions here might be hard to read
[04:58:26 -> 04:58:32]  it's very bright
[04:58:28 -> 04:58:33]  um but activation backward activation
[04:58:32 -> 04:58:34]  forward you just have like a massive
[04:58:33 -> 04:58:37]  list of stuff I'm not going to go
[04:58:34 -> 04:58:38]  through these one by one um but but you
[04:58:37 -> 04:58:40]  get the point these are all the
[04:58:38 -> 04:58:42]  operations that are supported with kunan
[04:58:40 -> 04:58:44]  to CNN and we're going to test we're
[04:58:42 -> 04:58:47]  going to test some of them
[04:58:44 -> 04:58:49]  out so you know you get a bunch of like
[04:58:47 -> 04:58:51]  uh descriptions about each what if each
[04:58:49 -> 04:58:52]  of these do so it you know in case
[04:58:51 -> 04:58:53]  you're wondering about something or
[04:58:52 -> 04:58:55]  you're not getting an output as you'd
[04:58:53 -> 04:58:57]  expect um you would generally refer to
[04:58:55 -> 04:58:59]  these doc so you could whatever type
[04:58:57 -> 04:59:00]  you're working with or whatever function
[04:58:59 -> 04:59:03]  you're trying to call like let's say
[04:59:00 -> 04:59:06]  you're you know working with this or
[04:59:03 -> 04:59:07]  maybe you're doing let's see maybe
[04:59:06 -> 04:59:12]  something simple like
[04:59:07 -> 04:59:15]  uh like U activate backward right so you
[04:59:12 -> 04:59:17]  have this you copy it f and you you can
[04:59:15 -> 04:59:19]  find these all across so you have the
[04:59:17 -> 04:59:20]  qnn activation backward and it has all
[04:59:19 -> 04:59:24]  the different types in here that you
[04:59:20 -> 04:59:25]  would use um and then you have the this
[04:59:24 -> 04:59:26]  one this original one that we
[04:59:25 -> 04:59:27]  highlighted before so that's how you
[04:59:26 -> 04:59:29]  navigate these you just search for
[04:59:27 -> 04:59:32]  whatever is wrong and then kind of just
[04:59:29 -> 04:59:33]  like look at that and see any of the any
[04:59:32 -> 04:59:36]  of the notes on it and see if you miss
[04:59:33 -> 04:59:40]  something um that's that's kind of how
[04:59:36 -> 04:59:43]  uh you're supposed to approach these um
[04:59:40 -> 04:59:45]  and then going into uh CNN API so this
[04:59:43 -> 04:59:48]  is where stuff might get a little bit
[04:59:45 -> 04:59:49]  interesting um it's not like any graph
[04:59:48 -> 04:59:54]  Fusion stuff you're doing it's just like
[04:59:49 -> 04:59:56]  raw algorithms um so you'll have um you
[04:59:54 -> 04:59:58]  know convolution backward bias like all
[04:59:56 -> 05:00:01]  just all the different convolution stuff
[04:59:58 -> 05:00:04]  um there is I mean there there is fused
[05:00:01 -> 05:00:08]  Ops um so that's where like some of this
[05:00:04 -> 05:00:09]  would come in but um yeah this is this
[05:00:08 -> 05:00:11]  is where all the convolution stuff is
[05:00:09 -> 05:00:14]  going to be for like image processing
[05:00:11 -> 05:00:15]  and you name it right so we're going to
[05:00:14 -> 05:00:17]  actually use convolutions in a second
[05:00:15 -> 05:00:18]  here so I'm going to I'm going to save
[05:00:17 -> 05:00:21]  this but it's you you approach it the
[05:00:18 -> 05:00:23]  similar way as you would with with Ops
[05:00:21 -> 05:00:25]  operations and then you have the uh
[05:00:23 -> 05:00:30]  adversarial
[05:00:25 -> 05:00:33]  API which is um same idea but you know
[05:00:30 -> 05:00:37]  other functions so like you know rnn's
[05:00:33 -> 05:00:39]  um C uh CTC loss multi-head
[05:00:37 -> 05:00:42]  attention uh we'll do yeah see
[05:00:39 -> 05:00:46]  multi-head attention weight so if I go
[05:00:42 -> 05:00:49]  um multi head attention multi-ad
[05:00:46 -> 05:00:55]  attention forward awesome how do we use
[05:00:49 -> 05:00:55]  this right there's there's multi head
[05:00:57 -> 05:01:03]  attention forward there's 22 of these in
[05:01:01 -> 05:01:07]  here 21
[05:01:03 -> 05:01:08]  now so you kind of get the point we can
[05:01:07 -> 05:01:12]  scroll through
[05:01:08 -> 05:01:12]  these there's a lot
[05:01:13 -> 05:01:19]  jeez um so this is how you would do a a
[05:01:17 -> 05:01:21]  multi-head attention block forward and
[05:01:19 -> 05:01:23]  the forward pass there's a lot of stuff
[05:01:21 -> 05:01:25]  in here but that's that that that that's
[05:01:23 -> 05:01:27]  kind of how this goes right um just the
[05:01:25 -> 05:01:29]  adversarial like extra the other section
[05:01:27 -> 05:01:33]  miscellaneous whatever you want to call
[05:01:29 -> 05:01:36]  it um but now we can actually go into
[05:01:33 -> 05:01:39]  some of the uh
[05:01:36 -> 05:01:42]  comparisons to understand uh how to
[05:01:39 -> 05:01:43]  actually use CNN in a Cuda script now we
[05:01:42 -> 05:01:46]  can actually go into some of the code
[05:01:43 -> 05:01:48]  and examples behind CNN and how it works
[05:01:46 -> 05:01:50]  under the hood well not not how it works
[05:01:48 -> 05:01:52]  under the hood but how we can use things
[05:01:50 -> 05:01:55]  like P torch under the hood to make
[05:01:52 -> 05:01:58]  operations really fast so in this
[05:01:55 -> 05:02:00]  example um you know we we do the Cuda
[05:01:58 -> 05:02:03]  runtime and the cnn. H I'm just going to
[05:02:00 -> 05:02:04]  do the 10 function for example um and in
[05:02:03 -> 05:02:08]  case you haven't seen the 10 function
[05:02:04 -> 05:02:08]  yet um
[05:02:10 -> 05:02:14]  we go to Google
[05:02:11 -> 05:02:16]  Images uh it literally just looks like
[05:02:14 -> 05:02:19]  like this 10 H
[05:02:16 -> 05:02:22]  um or like this maybe this is a better
[05:02:19 -> 05:02:24]  one it's like between -1 and one it's
[05:02:22 -> 05:02:26]  just a little activation function that
[05:02:24 -> 05:02:30]  you do it's like a nice smooth S curve
[05:02:26 -> 05:02:31]  and uh yeah so that's that's all we're
[05:02:30 -> 05:02:33]  really doing here we don't actually need
[05:02:31 -> 05:02:35]  to like do the type in the formula it's
[05:02:33 -> 05:02:37]  already done for us um I've actually
[05:02:35 -> 05:02:40]  written out the 10h kernel here and
[05:02:37 -> 05:02:46]  operation is literally just 10 HF for P
[05:02:40 -> 05:02:49]  function on device um so very simple um
[05:02:46 -> 05:02:56]  we we want to have a tensor with the
[05:02:49 -> 05:02:58]  shape um what's it called n by n by C by
[05:02:56 -> 05:03:01]  height by width that's the format we
[05:02:58 -> 05:03:04]  want to use so it's going to be like n
[05:03:01 -> 05:03:07]  is batch size channels is uh Channel C
[05:03:04 -> 05:03:09]  is channels height and then width right
[05:03:07 -> 05:03:11]  so the whole idea of like channels is if
[05:03:09 -> 05:03:12]  you have uh like like an image for
[05:03:11 -> 05:03:14]  example like the image you're video
[05:03:12 -> 05:03:16]  you're watching me on right now this is
[05:03:14 -> 05:03:19]  going to have three channels it's going
[05:03:16 -> 05:03:21]  to have RGB right so this is if uh say
[05:03:19 -> 05:03:23]  you've you've done some convolutions and
[05:03:21 -> 05:03:26]  now your now your channel Dimension is
[05:03:23 -> 05:03:29]  very big so instead of three you've got
[05:03:26 -> 05:03:30]  like 32 elements per pixel that you have
[05:03:29 -> 05:03:32]  to keep track of and and do operations
[05:03:30 -> 05:03:35]  on right so we're making a big tensor
[05:03:32 -> 05:03:38]  here big tensor um if I actually go into
[05:03:35 -> 05:03:41]  python new 256 we going to go well four
[05:03:38 -> 05:03:47]  times cuz four is the number of of bytes
[05:03:41 -> 05:03:51]  it's going to occupy 4 * 256 * 32 * 2 *
[05:03:47 -> 05:03:54]  224 uh squared we end up getting this
[05:03:51 -> 05:03:57]  number and if we divide this number by
[05:03:54 -> 05:04:01]  uh 1 million it's
[05:03:57 -> 05:04:02]  about 1.6 gabt that's how big this
[05:04:01 -> 05:04:04]  tensor is and then we're going to do a
[05:04:02 -> 05:04:06]  10h on
[05:04:04 -> 05:04:08]  that so Cuda DN is going to handle this
[05:04:06 -> 05:04:10]  and we're going to compare that to the
[05:04:08 -> 05:04:14]  naive kernel um
[05:04:10 -> 05:04:17]  and yeah so just stepping through this
[05:04:14 -> 05:04:19]  same Cuda Malik initialized data Cuda M
[05:04:17 -> 05:04:21]  Copy we're going to create some events
[05:04:19 -> 05:04:23]  that we can Time stuff on the GPU we're
[05:04:21 -> 05:04:26]  going to do our our warmup and and
[05:04:23 -> 05:04:29]  Benchmark runs um we're going to do some
[05:04:26 -> 05:04:33]  warmup runs for the knif kernel
[05:04:29 -> 05:04:34]  um we're going to do benchmarks for it
[05:04:33 -> 05:04:35]  we're going to set up qnn and this is
[05:04:34 -> 05:04:37]  where we're actually going to learn a
[05:04:35 -> 05:04:39]  little bit um and then like benchmarks
[05:04:37 -> 05:04:42]  for CNN of course this is where it m
[05:04:39 -> 05:04:45]  takes place so we have this qnn handle
[05:04:42 -> 05:04:47]  type we create
[05:04:45 -> 05:04:50]  that we have this tensor descriptor type
[05:04:47 -> 05:04:52]  so you actually need to um it's like the
[05:04:50 -> 05:04:54]  when you did like a matrix a descriptor
[05:04:52 -> 05:04:56]  or Matrix B descriptor it's the same
[05:04:54 -> 05:04:58]  idea but we do that for tensors because
[05:04:56 -> 05:04:59]  it's you know it's more in the Deep
[05:04:58 -> 05:05:01]  learning context there's more deep
[05:04:59 -> 05:05:04]  learning operations in Cuda CP DNN since
[05:05:01 -> 05:05:08]  it's deep neural network C deep neural
[05:05:04 -> 05:05:10]  net right um so we have create Tor
[05:05:08 -> 05:05:11]  descriptor which is is going to just
[05:05:10 -> 05:05:12]  take the memory address of this and
[05:05:11 -> 05:05:15]  actually create the tensor descriptor
[05:05:12 -> 05:05:17]  based on the type um we're going to set
[05:05:15 -> 05:05:20]  the tensor descriptor so it's going to
[05:05:17 -> 05:05:23]  be um this type which we which we did
[05:05:20 -> 05:05:28]  already the tensor format the data type
[05:05:23 -> 05:05:30]  and then each of those uh NCH HW format
[05:05:28 -> 05:05:32]  right so we do this we do this format
[05:05:30 -> 05:05:37]  and you can you can look this up and
[05:05:32 -> 05:05:41]  there's um it's right here tensor format
[05:05:37 -> 05:05:46]  so we can do NCH HW we can do NW C or n
[05:05:41 -> 05:05:49]  nhwc um and Etc right so that's we just
[05:05:46 -> 05:05:52]  we just pick this one and then this data
[05:05:49 -> 05:05:53]  float we look at this um we can have
[05:05:52 -> 05:05:57]  like a bunch of different ones fast
[05:05:53 -> 05:05:59]  float for fp8 um fp8 so we'd have one
[05:05:57 -> 05:06:02]  sign bit and then five exponent bits and
[05:05:59 -> 05:06:05]  then two mantisa bits right so it's
[05:06:02 -> 05:06:07]  eight total right um and then fp8 you
[05:06:05 -> 05:06:08]  know more more mantisa bits and then
[05:06:07 -> 05:06:10]  Boolean we have all these different
[05:06:08 -> 05:06:13]  types but we want the float there's no
[05:06:10 -> 05:06:17]  like float 32 here we just want a normal
[05:06:13 -> 05:06:20]  float no no half no bf16 none of that um
[05:06:17 -> 05:06:23]  just something basic to to to
[05:06:20 -> 05:06:24]  use and then we create not only the
[05:06:23 -> 05:06:26]  tensor descriptor because we have the
[05:06:24 -> 05:06:28]  tensor itself and then we have the
[05:06:26 -> 05:06:30]  activation descriptor which is about
[05:06:28 -> 05:06:32]  what the activation is going to do and
[05:06:30 -> 05:06:34]  we have a custom type for that um and
[05:06:32 -> 05:06:37]  you can go here and there's there's a
[05:06:34 -> 05:06:39]  bunch of different um I think
[05:06:37 -> 05:06:41]  it's I don't know where exactly this is
[05:06:39 -> 05:06:42]  but I just like to right click on things
[05:06:41 -> 05:06:46]  to learn
[05:06:42 -> 05:06:49]  yeah good
[05:06:46 -> 05:06:51]  luck uh we create the ACT activation
[05:06:49 -> 05:06:53]  descriptor with its memory address and
[05:06:51 -> 05:06:56]  here we have the activation descriptor
[05:06:53 -> 05:06:59]  type as before we have the CNN or uh we
[05:06:56 -> 05:07:02]  have the activation type uh mode type so
[05:06:59 -> 05:07:05]  the activation mode uh is going to be
[05:07:02 -> 05:07:08]  10h and then the uh propagation type is
[05:07:05 -> 05:07:10]  just Nan um we're we're not we're not
[05:07:08 -> 05:07:13]  doing that
[05:07:10 -> 05:07:15]  um and then the coefficient is going to
[05:07:13 -> 05:07:17]  be uh zero so there's not going to I
[05:07:15 -> 05:07:20]  don't know what what is KF I don't know
[05:07:17 -> 05:07:22]  what that is um but anyways this is just
[05:07:20 -> 05:07:23]  like the template layout we're having
[05:07:22 -> 05:07:25]  and I don't expect you to like go
[05:07:23 -> 05:07:27]  through this and understand every single
[05:07:25 -> 05:07:28]  character that's happening here this is
[05:07:27 -> 05:07:30]  just kind of a more template example to
[05:07:28 -> 05:07:32]  show you like how we're comparing these
[05:07:30 -> 05:07:34]  how we're running and testing and you
[05:07:32 -> 05:07:35]  can like take these out and put them
[05:07:34 -> 05:07:36]  into other pieces of code so don't like
[05:07:35 -> 05:07:39]  feel bad at all if you don't understand
[05:07:36 -> 05:07:41]  this it is there there is a lot um but
[05:07:39 -> 05:07:45]  if we go to like this for example we
[05:07:41 -> 05:07:48]  have like sigmoid R 10 clip R um swish
[05:07:45 -> 05:07:51]  all this right so we just want the tan H
[05:07:48 -> 05:07:54]  function um
[05:07:51 -> 05:07:57]  now we go to activation forward as we
[05:07:54 -> 05:07:59]  saw just like a a few minutes ago um we
[05:07:57 -> 05:08:01]  have the scann handle which we defined
[05:07:59 -> 05:08:02]  before activation descriptor which we
[05:08:01 -> 05:08:05]  which we just covered the alpha
[05:08:02 -> 05:08:10]  parameter the tensor descriptor which we
[05:08:05 -> 05:08:12]  covered uh up here we have this um
[05:08:10 -> 05:08:15]  we have this void X so that's just
[05:08:12 -> 05:08:18]  that's just the input a beta term which
[05:08:15 -> 05:08:21]  we don't need um and then tensor
[05:08:18 -> 05:08:24]  descriptor type for y so and the output
[05:08:21 -> 05:08:26]  itself is just a uh it's it's nothing
[05:08:24 -> 05:08:28]  it's nothing particularly special it's
[05:08:26 -> 05:08:29]  just a void so we don't need any like
[05:08:28 -> 05:08:32]  special types for it it's just like a
[05:08:29 -> 05:08:34]  raw data like array output essentially
[05:08:32 -> 05:08:36]  um going to be a bunch of floats right
[05:08:34 -> 05:08:38]  um so that's that's that and it's going
[05:08:36 -> 05:08:40]  to be on device that's why we have the D
[05:08:38 -> 05:08:42]  there um
[05:08:40 -> 05:08:43]  um we're going to synchronize everything
[05:08:42 -> 05:08:44]  and then do our benchmarks run
[05:08:43 -> 05:08:46]  benchmarks runs we're going to find the
[05:08:44 -> 05:08:49]  average time we're going to verify
[05:08:46 -> 05:08:49]  against the
[05:08:49 -> 05:08:53]  CPU like a CPU tan for example it's not
[05:08:52 -> 05:08:55]  not going to take long since it's like a
[05:08:53 -> 05:08:57]  like a pointwise operation it's just
[05:08:55 -> 05:09:00]  going to go one by one through it it's
[05:08:57 -> 05:09:05]  very it's like linear time um so when we
[05:09:00 -> 05:09:10]  actually uh when we actually compile
[05:09:05 -> 05:09:12]  this 10 H to DNN see it knows
[05:09:10 -> 05:09:15]  we run this you're actually going to be
[05:09:12 -> 05:09:17]  surprised by something so give this a
[05:09:15 -> 05:09:20]  second to
[05:09:17 -> 05:09:22]  run my machine might be lagging a little
[05:09:20 -> 05:09:24]  bit I'm not sure
[05:09:22 -> 05:09:27]  but we're going to let that run for a
[05:09:24 -> 05:09:30]  second here our Matrix sizes are are
[05:09:27 -> 05:09:34]  quite big remember we have these
[05:09:30 -> 05:09:37]  so um you know tensor size as we wanted
[05:09:34 -> 05:09:41]  um the naive crud kernel time notice how
[05:09:37 -> 05:09:45]  this is actually faster than the CNN
[05:09:41 -> 05:09:49]  activation time um they're both correct
[05:09:45 -> 05:09:53]  we just compare the results um you know
[05:09:49 -> 05:09:55]  pointwise and this is faster why that
[05:09:53 -> 05:09:57]  really made me angry when I when I saw
[05:09:55 -> 05:09:59]  this I was like what is the point of
[05:09:57 -> 05:10:01]  having a having a CNN activation like
[05:09:59 -> 05:10:04]  what is the point of this um and the
[05:10:01 -> 05:10:06]  point of that is just to give give
[05:10:04 -> 05:10:07]  yourself things like Alpha and beta
[05:10:06 -> 05:10:09]  right so when you have Alpha and beta
[05:10:07 -> 05:10:11]  these are extra numbers that you have
[05:10:09 -> 05:10:13]  have to consider in the operation and
[05:10:11 -> 05:10:16]  when it's so simple when it's as simple
[05:10:13 -> 05:10:18]  as just a like exponentiation or like a
[05:10:16 -> 05:10:20]  multiplication um and it's it's like a
[05:10:18 -> 05:10:22]  very simple like for example Ru takes
[05:10:20 -> 05:10:24]  almost no time to complete 10h is like a
[05:10:22 -> 05:10:26]  very simple operation just on a single
[05:10:24 -> 05:10:28]  number um you just output that to the
[05:10:26 -> 05:10:31]  same index it's very computationally
[05:10:28 -> 05:10:34]  simple but when you add in little things
[05:10:31 -> 05:10:35]  like Alpha and beta I suspect these are
[05:10:34 -> 05:10:38]  what actually cause the performance
[05:10:35 -> 05:10:40]  difference it might be mostly these or
[05:10:38 -> 05:10:42]  some cdnn over head I mean again we
[05:10:40 -> 05:10:44]  don't even know what this what this is
[05:10:42 -> 05:10:46]  doing under the hood it's a complete
[05:10:44 -> 05:10:48]  Black Box opaque struct we don't know
[05:10:46 -> 05:10:50]  what's happening so it's hard to
[05:10:48 -> 05:10:53]  actually know why this happens um
[05:10:50 -> 05:10:55]  there's not very many resources on why
[05:10:53 -> 05:10:57]  custom kernels might be faster than C
[05:10:55 -> 05:10:59]  andn I haven't really found any of that
[05:10:57 -> 05:11:01]  so we're just going to hold the
[05:10:59 -> 05:11:03]  assumption that uh there's this big
[05:11:01 -> 05:11:05]  opaque struct Black Box thing that we
[05:11:03 -> 05:11:07]  don't know um and then you also have
[05:11:05 -> 05:11:09]  just the alpha and beta as well that
[05:11:07 -> 05:11:11]  you're you're um timesing
[05:11:09 -> 05:11:13]  and adding things by right so that's
[05:11:11 -> 05:11:15]  going to add some extra compute compute
[05:11:13 -> 05:11:17]  overhead but these are not very big
[05:11:15 -> 05:11:20]  differences at all right so you have
[05:11:17 -> 05:11:22]  like 8.52 3 and then 8.6 it's like this
[05:11:20 -> 05:11:25]  is like nothing this won't actually
[05:11:22 -> 05:11:28]  matter in production it's like if you if
[05:11:25 -> 05:11:31]  you take the difference um if you see
[05:11:28 -> 05:11:35]  how much faster um the naive cter
[05:11:31 -> 05:11:36]  formula is it's like literally 1.3%
[05:11:35 -> 05:11:38]  faster which you're not ever going to
[05:11:36 -> 05:11:39]  notice in a real environment like that
[05:11:38 -> 05:11:41]  it's just unnoticeable
[05:11:39 -> 05:11:44]  so it doesn't actually matter that much
[05:11:41 -> 05:11:46]  if you care like totally just like go go
[05:11:44 -> 05:11:47]  ahead go and R your own kernels but that
[05:11:46 -> 05:11:51]  that's that's a general idea it doesn't
[05:11:47 -> 05:11:53]  really matter that much um for for
[05:11:51 -> 05:11:55]  convolutions it will matter though um
[05:11:53 -> 05:11:57]  then going down to this I wrote a a p
[05:11:55 -> 05:12:00]  torch script to just kind of illustrate
[05:11:57 -> 05:12:02]  things out manually so like how pytorch
[05:12:00 -> 05:12:04]  handles um how pytorch handles the
[05:12:02 -> 05:12:06]  custom tan like when you write it from
[05:12:04 -> 05:12:09]  scratch on your own um versus when you
[05:12:06 -> 05:12:14]  just use the built-in the torch the
[05:12:09 -> 05:12:16]  torch. 10 where did it go yes torch. 10
[05:12:14 -> 05:12:20]  um so just kind of comparing those side
[05:12:16 -> 05:12:20]  by side and seeing how they perform
[05:12:23 -> 05:12:30]  um so if we write these out um o it's
[05:12:27 -> 05:12:34]  taking some time to do this yeah so
[05:12:30 -> 05:12:37]  custom t h um that's going to take 21
[05:12:34 -> 05:12:39]  milliseconds remember how CN beforehand
[05:12:37 -> 05:12:41]  uh was taking about like point I don't
[05:12:39 -> 05:12:44]  know what it was like
[05:12:41 -> 05:12:46]  point it was it was a very small number
[05:12:44 -> 05:12:49]  um the custom like built in 10 is still
[05:12:46 -> 05:12:53]  very fast um so if we go back and we
[05:12:49 -> 05:12:55]  just nbcc compile this and then you know
[05:12:53 -> 05:12:57]  run and let's just like look at which
[05:12:55 -> 05:12:59]  what the shapes of our numbers are real
[05:12:57 -> 05:13:04]  quick
[05:12:59 -> 05:13:11]  um if I do batch size by say or if I do
[05:13:04 -> 05:13:11]  256 by 32 by 224 by 224
[05:13:15 -> 05:13:20]  four Rand n and we wrap these in
[05:13:18 -> 05:13:20]  Brackets
[05:13:20 -> 05:13:26]  here we can we can check this one real
[05:13:23 -> 05:13:26]  quick and see how fast this
[05:13:27 -> 05:13:32]  runs and then we will compare the python
[05:13:30 -> 05:13:35]  script to that directly without like
[05:13:32 -> 05:13:37]  deleting the output so we can't see it
[05:13:35 -> 05:13:39]  um just so we understand like how much
[05:13:37 -> 05:13:41]  performance could be gained from say
[05:13:39 -> 05:13:45]  using uh using like a custom written
[05:13:41 -> 05:13:49]  Cuda kernel or custom written uh CNN
[05:13:45 -> 05:13:51]  function so if we go python TCH compare
[05:13:49 -> 05:13:54]  it's going to take a second oh yeah ran
[05:13:51 -> 05:13:57]  out to memory awesome
[05:13:54 -> 05:13:57]  so
[05:13:58 -> 05:14:07]  um Rand yeah no that doesn't work I bump
[05:14:02 -> 05:14:07]  this down to like 128 it might work
[05:14:11 -> 05:14:17]  it's not very fast yeah so like Custom
[05:14:15 -> 05:14:21]  Custom 10 is super slow and then the
[05:14:17 -> 05:14:25]  built-in one 4.3 let's say 4.4
[05:14:21 -> 05:14:27]  milliseconds right um and then we go up
[05:14:25 -> 05:14:29]  and we can see that you know this is
[05:14:27 -> 05:14:32]  this is actually
[05:14:29 -> 05:14:37]  um this is about double that right so if
[05:14:32 -> 05:14:37]  we do 4 point about 4.4 *
[05:14:37 -> 05:14:44]  2 8.8 right so we go back up and it's
[05:14:41 -> 05:14:45]  like this is less than that so if we
[05:14:44 -> 05:14:47]  write a naive kud kernel like this is
[05:14:45 -> 05:14:48]  just naive if you were to optimize this
[05:14:47 -> 05:14:50]  and imp Implement something like Loop
[05:14:48 -> 05:14:53]  unrolling which we'll go into later and
[05:14:50 -> 05:14:55]  you optimize this even just something as
[05:14:53 -> 05:14:57]  simple as an activation function you can
[05:14:55 -> 05:14:59]  get this surprisingly faster than Pi
[05:14:57 -> 05:15:00]  torch um and just like write everything
[05:14:59 -> 05:15:03]  manually you can actually get
[05:15:00 -> 05:15:07]  considerable speed UPS so uh that's just
[05:15:03 -> 05:15:08]  to provide some context there um but
[05:15:07 -> 05:15:10]  yeah activation functions don't worry
[05:15:08 -> 05:15:13]  about do that too much it's not really
[05:15:10 -> 05:15:15]  going to affect you um and then we go to
[05:15:13 -> 05:15:17]  like convolutions right this is this is
[05:15:15 -> 05:15:22]  an example of where we're actually using
[05:15:17 -> 05:15:23]  convolutions um so you know we we we
[05:15:22 -> 05:15:26]  have a lot more things to pay attention
[05:15:23 -> 05:15:27]  to I'm going to dive into this right now
[05:15:26 -> 05:15:29]  now we can jump into convolutions a
[05:15:27 -> 05:15:31]  little bit um I've been hacking around
[05:15:29 -> 05:15:33]  with some of these and uh let's just
[05:15:31 -> 05:15:35]  going to show you how these how these
[05:15:33 -> 05:15:38]  work so we're going to start off with a
[05:15:35 -> 05:15:40]  visualization first I'm going to bring
[05:15:38 -> 05:15:40]  over a
[05:15:41 -> 05:15:46]  convolutions visualized just pop over to
[05:15:43 -> 05:15:48]  here and um this is just what it looks
[05:15:46 -> 05:15:50]  like so we can like you have these input
[05:15:48 -> 05:15:54]  sizes kernel size padding dilation and
[05:15:50 -> 05:15:56]  stride input size is this I can change
[05:15:54 -> 05:15:59]  the input image the kernel size the
[05:15:56 -> 05:16:01]  actual weight itself the the weight term
[05:15:59 -> 05:16:04]  um the padding so how much black pixels
[05:16:01 -> 05:16:07]  are put around the input the dilation of
[05:16:04 -> 05:16:10]  it right
[05:16:07 -> 05:16:12]  um and then stride as well so I can make
[05:16:10 -> 05:16:13]  it stride by one each time or stride by
[05:16:12 -> 05:16:16]  [Music]
[05:16:13 -> 05:16:19]  two um and and that's that's pretty much
[05:16:16 -> 05:16:22]  just uh that that you can mess around
[05:16:19 -> 05:16:24]  with that in your own time um but I've
[05:16:22 -> 05:16:28]  written two scripts
[05:16:24 -> 05:16:30]  so one for p torch so the P torch and
[05:16:28 -> 05:16:32]  this one use the exact same values in
[05:16:30 -> 05:16:35]  the exact same order um the exact same
[05:16:32 -> 05:16:37]  parameters here they're all the same and
[05:16:35 -> 05:16:39]  I'm just doing a side by-side comparison
[05:16:37 -> 05:16:42]  of them so that we can get desired
[05:16:39 -> 05:16:44]  output so it's
[05:16:42 -> 05:16:48]  essentially it's essentially like a like
[05:16:44 -> 05:16:52]  a 4x4 the input is like a 4x4 image so 1
[05:16:48 -> 05:16:56]  2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 and
[05:16:52 -> 05:16:59]  then the kernel itself is 1 2 3 4 5 6 7
[05:16:56 -> 05:16:59]  8 n and it's just going
[05:16:59 -> 05:17:05]  to right it's going to do that and we
[05:17:02 -> 05:17:09]  put a padding over it uh so that this is
[05:17:05 -> 05:17:11]  going to end up flooring to one so Cur
[05:17:09 -> 05:17:15]  size is three and then divide that uh
[05:17:11 -> 05:17:18]  floor by two so if we go python go three
[05:17:15 -> 05:17:20]  two we get one
[05:17:18 -> 05:17:22]  and that's just going to be our padding
[05:17:20 -> 05:17:25]  and then torch is going to do that with
[05:17:22 -> 05:17:28]  this functional Library here um and and
[05:17:25 -> 05:17:29]  we get the output right so this should
[05:17:28 -> 05:17:31]  be very self-explanatory very easy we're
[05:17:29 -> 05:17:35]  just reshaping it and then you know
[05:17:31 -> 05:17:38]  reshaping it here as well um but when we
[05:17:35 -> 05:17:41]  look at this part this is actually where
[05:17:38 -> 05:17:44]  the fun kicks in so we do our cian N.H
[05:17:41 -> 05:17:46]  at the top I've written a naive Cuda
[05:17:44 -> 05:17:48]  kernel for this for doing 2D
[05:17:46 -> 05:17:50]  convolutions takes in an input a kernel
[05:17:48 -> 05:17:52]  the output the width height in channels
[05:17:50 -> 05:17:55]  out channels kernel size uh and batch
[05:17:52 -> 05:17:57]  size so uh don't don't worry about how
[05:17:55 -> 05:17:59]  this works internally just it just let
[05:17:57 -> 05:18:01]  it be there as some like template code
[05:17:59 -> 05:18:04]  that's going to do what we want um it's
[05:18:01 -> 05:18:07]  modular so um and then we have all the
[05:18:04 -> 05:18:09]  same settings as we do in our py script
[05:18:07 -> 05:18:12]  at the top here so
[05:18:09 -> 05:18:14]  um we Define like how big our inputs
[05:18:12 -> 05:18:16]  outputs and kernel elements are going to
[05:18:14 -> 05:18:18]  be we print this out um we do our
[05:18:16 -> 05:18:19]  classic just malic we have our our
[05:18:18 -> 05:18:21]  values
[05:18:19 -> 05:18:24]  organized
[05:18:21 -> 05:18:26]  um more kud M Copy stuff and then we
[05:18:24 -> 05:18:27]  create the CNN handle so this is all
[05:18:26 -> 05:18:31]  very similar to uh what we were doing
[05:18:27 -> 05:18:33]  with CNN and our t t function um we
[05:18:31 -> 05:18:36]  created we create an input and output
[05:18:33 -> 05:18:39]  tensor um filter descriptor for the
[05:18:36 -> 05:18:40]  kernel itself um
[05:18:39 -> 05:18:43]  convolution descriptor for the
[05:18:40 -> 05:18:44]  convolution operation we create all of
[05:18:43 -> 05:18:47]  these we create the convolution
[05:18:44 -> 05:18:48]  descriptor with that memory address um
[05:18:47 -> 05:18:49]  we're doing going to use the 4D
[05:18:48 -> 05:18:51]  descriptor because it's going to be
[05:18:49 -> 05:18:55]  shaped batch size by Channel by height
[05:18:51 -> 05:18:57]  by width um and then if we look at these
[05:18:55 -> 05:19:00]  so we have the we have the tensor
[05:18:57 -> 05:19:01]  descriptor the format so how which what
[05:19:00 -> 05:19:04]  it was like the shape of it and then the
[05:19:01 -> 05:19:08]  data type which is just a float 32 as we
[05:19:04 -> 05:19:10]  have here um and then we have the N by C
[05:19:08 -> 05:19:12]  by h by W right so batch size by
[05:19:10 -> 05:19:14]  channels by height by width and we do
[05:19:12 -> 05:19:16]  the same thing for all of these um the
[05:19:14 -> 05:19:17]  output is going to be out channels
[05:19:16 -> 05:19:19]  because it's not inch it's going to be
[05:19:17 -> 05:19:22]  how many out channels do we get in the
[05:19:19 -> 05:19:24]  end um and then the filter descriptor is
[05:19:22 -> 05:19:26]  going to have a different it's going to
[05:19:24 -> 05:19:29]  be organized a different way it's going
[05:19:26 -> 05:19:31]  to be um out channels by in channels by
[05:19:29 -> 05:19:34]  height by width so out channels in
[05:19:31 -> 05:19:37]  channels height uh height by width right
[05:19:34 -> 05:19:39]  for the for the kernel itself for the
[05:19:37 -> 05:19:41]  convolution filter you I'm going to use
[05:19:39 -> 05:19:44]  those interchangeably convolution kernel
[05:19:41 -> 05:19:46]  and convolution filter same thing um and
[05:19:44 -> 05:19:48]  then the actual descriptor itself um
[05:19:46 -> 05:19:49]  this is a 2d descriptor so it's going to
[05:19:48 -> 05:19:52]  be a 2d
[05:19:49 -> 05:19:57]  convolution um and we just dump all
[05:19:52 -> 05:19:59]  these in so the padding H padding W um u
[05:19:57 -> 05:20:02]  and v and dilations we we're just we're
[05:19:59 -> 05:20:04]  not even using dilations um convolution
[05:20:02 -> 05:20:06]  mode so it's going to be cross
[05:20:04 -> 05:20:09]  correlation and the data type is just
[05:20:06 -> 05:20:10]  float um so I don't expect you to know
[05:20:09 -> 05:20:12]  what what all like the convolution laws
[05:20:10 -> 05:20:13]  are and everything we're just comparing
[05:20:12 -> 05:20:18]  to P torch and making sure that
[05:20:13 -> 05:20:22]  everything lines up um and then in terms
[05:20:18 -> 05:20:24]  of the um in terms of the algorithm
[05:20:22 -> 05:20:26]  itself um we have a little thing that
[05:20:24 -> 05:20:27]  searches through stuff here uh I might
[05:20:26 -> 05:20:29]  change this later this doesn't it's not
[05:20:27 -> 05:20:32]  very beautiful to look at um but what
[05:20:29 -> 05:20:35]  you what you can do is actually just
[05:20:32 -> 05:20:38]  like literally this uh where is it this
[05:20:35 -> 05:20:40]  algo here when you do the when you give
[05:20:38 -> 05:20:42]  uh when you give CNN like a workspace
[05:20:40 -> 05:20:45]  size to do the operation in there's this
[05:20:42 -> 05:20:49]  CNN convolution forward algo type which
[05:20:45 -> 05:20:52]  is right here um forward algo type and
[05:20:49 -> 05:20:53]  there's a bunch of them in here so uh by
[05:20:52 -> 05:20:55]  default what that's supposed to do is
[05:20:53 -> 05:20:57]  cycle through them and find which one is
[05:20:55 -> 05:20:59]  the best but I find that it might be a
[05:20:57 -> 05:21:02]  little bit better to just cycle through
[05:20:59 -> 05:21:04]  these one one by one on your own so try
[05:21:02 -> 05:21:06]  out implicit gem Tri out pre-comp gem
[05:21:04 -> 05:21:09]  triy out gem tried out direct fft fft
[05:21:06 -> 05:21:12]  tiling WI noat nonfused count right so
[05:21:09 -> 05:21:13]  try all these out and see how they work
[05:21:12 -> 05:21:15]  um which I which I have in this uh
[05:21:13 -> 05:21:18]  comparison script we're going to do
[05:21:15 -> 05:21:20]  later on but uh yeah don't don't don't
[05:21:18 -> 05:21:23]  worry too much about those just you can
[05:21:20 -> 05:21:24]  kind of just run the script as is um but
[05:21:23 -> 05:21:26]  we're just trying to find the best
[05:21:24 -> 05:21:28]  convolution algorithm for our problem at
[05:21:26 -> 05:21:31]  hand right so when you have a a smaller
[05:21:28 -> 05:21:34]  kernel and a small image a certain like
[05:21:31 -> 05:21:36]  maybe an implicit gem might be faster
[05:21:34 -> 05:21:37]  than say an fft tiling uh because of the
[05:21:36 -> 05:21:39]  overhead right so you have to just
[05:21:37 -> 05:21:42]  consider things like that your problem
[05:21:39 -> 05:21:45]  size all that stuff um the workspace
[05:21:42 -> 05:21:48]  size is just how big how big you
[05:21:45 -> 05:21:49]  actually um return the minimum size of
[05:21:48 -> 05:21:52]  the workspace to be pass to the
[05:21:49 -> 05:21:53]  convolution given an algorithm right so
[05:21:52 -> 05:21:56]  you're essentially just saying how much
[05:21:53 -> 05:21:58]  do you get to work with here um and this
[05:21:56 -> 05:22:00]  is defined by a bunch of things that it
[05:21:58 -> 05:22:02]  it just it just decides this right so we
[05:22:00 -> 05:22:04]  give it a bunch of descriptions and it's
[05:22:02 -> 05:22:10]  going to use that context to decide um
[05:22:04 -> 05:22:10]  what the workspace size should be um
[05:22:11 -> 05:22:16]  now now we can do our Benchmark uh
[05:22:14 -> 05:22:18]  warmup and Benchmark runs so we have
[05:22:16 -> 05:22:21]  this um you know just skipping Alpha and
[05:22:18 -> 05:22:22]  beta we have this convolution forward
[05:22:21 -> 05:22:25]  function which I'll show you in the
[05:22:22 -> 05:22:28]  Nvidia docs in a second here um this
[05:22:25 -> 05:22:30]  consists of the handle Alpha tensor
[05:22:28 -> 05:22:33]  descriptor for the input the input
[05:22:30 -> 05:22:36]  itself which is just a you just any just
[05:22:33 -> 05:22:40]  a void pointer right
[05:22:36 -> 05:22:43]  um and then the the filter descriptor
[05:22:40 -> 05:22:47]  the kernel uh convolution kernel so a
[05:22:43 -> 05:22:51]  point again a pointer to to an array uh
[05:22:47 -> 05:22:54]  the convolution descriptor so actual uh
[05:22:51 -> 05:22:57]  operation algorithm itself the algorithm
[05:22:54 -> 05:22:59]  um the workspace which we just find in
[05:22:57 -> 05:23:03]  workspace size and bytes the workspace
[05:22:59 -> 05:23:05]  size and bytes um we pass in this we we
[05:23:03 -> 05:23:08]  do this size T which is like a size type
[05:23:05 -> 05:23:10]  for like storing large values and we put
[05:23:08 -> 05:23:13]  this into here and then this value
[05:23:10 -> 05:23:14]  changes based on these settings right so
[05:23:13 -> 05:23:17]  when we put this back in here it's going
[05:23:14 -> 05:23:18]  to decide um when it's actually running
[05:23:17 -> 05:23:20]  this how much do we need and and what
[05:23:18 -> 05:23:22]  are the resources requirements it's pred
[05:23:20 -> 05:23:25]  decided right
[05:23:22 -> 05:23:27]  um and then we do um we just we just
[05:23:25 -> 05:23:30]  enter the output description and then
[05:23:27 -> 05:23:32]  the output uh the output device array
[05:23:30 -> 05:23:35]  right and then we do the same thing but
[05:23:32 -> 05:23:37]  with our uh con 2D except there's like
[05:23:35 -> 05:23:39]  it's less complex to filter through and
[05:23:37 -> 05:23:41]  then we synchronize all of our threads
[05:23:39 -> 05:23:44]  and blocks in the GPU with Cuda device
[05:23:41 -> 05:23:45]  synchronized very simple and we do the
[05:23:44 -> 05:23:47]  same thing for benchmarks runs except we
[05:23:45 -> 05:23:51]  just add a time and an event recording
[05:23:47 -> 05:23:53]  as well right um so fairly simple
[05:23:51 -> 05:23:55]  Concepts happening here just timing and
[05:23:53 -> 05:23:57]  benchmarking and mainly just filtering
[05:23:55 -> 05:23:59]  through what the heck a function takes
[05:23:57 -> 05:24:01]  in and what are all these types doing
[05:23:59 -> 05:24:03]  right that's that's really the mess you
[05:24:01 -> 05:24:06]  have to dig through
[05:24:03 -> 05:24:08]  um now if we go down um we can actually
[05:24:06 -> 05:24:11]  we actually print out the CNN output and
[05:24:08 -> 05:24:13]  the naive kernel output so and then the
[05:24:11 -> 05:24:15]  flattened one as well so that we can
[05:24:13 -> 05:24:17]  compare back to Pi torch element by
[05:24:15 -> 05:24:18]  element um we just destroy all the
[05:24:17 -> 05:24:21]  context afterwards same thing with the
[05:24:18 -> 05:24:27]  tan uh same thing with the tan Cuda
[05:24:21 -> 05:24:30]  script um go and run this so out
[05:24:27 -> 05:24:34]  01 just like that link
[05:24:30 -> 05:24:38]  CN we end we run this
[05:24:34 -> 05:24:40]  um all of these are uh as expected and
[05:24:38 -> 05:24:42]  so it's it's going to just yeah it's
[05:24:40 -> 05:24:43]  going to select an algorithm these these
[05:24:42 -> 05:24:45]  are all messed up I might change these
[05:24:43 -> 05:24:50]  later but
[05:24:45 -> 05:24:51]  um we notice that qnn is slower than the
[05:24:50 -> 05:24:54]  naive kernel and that's because it's
[05:24:51 -> 05:24:56]  just really small right it's very small
[05:24:54 -> 05:24:58]  um qn probably has more to organize it's
[05:24:56 -> 05:25:01]  got these Alpha Beta terms everywhere
[05:24:58 -> 05:25:02]  it's got to take care of and um yeah
[05:25:01 -> 05:25:06]  there there might just be some extra
[05:25:02 -> 05:25:08]  overhead there so it is it is indeed um
[05:25:06 -> 05:25:10]  you know like three four four times
[05:25:08 -> 05:25:13]  slower than this just just because it's
[05:25:10 -> 05:25:16]  smaller and it's not a big problem size
[05:25:13 -> 05:25:21]  so if we look at the output here we see1
[05:25:16 -> 05:25:26]  178 217 and then the end is like 274 275
[05:25:21 -> 05:25:26]  175 now if I go and run um torch
[05:25:27 -> 05:25:32]  compare it's going to do the exact same
[05:25:29 -> 05:25:39]  thing notice how we get 111 178 217 and
[05:25:32 -> 05:25:43]  then 295 1 uh or sorry 274 299 295 175
[05:25:39 -> 05:25:44]  so go back here 274 295 175 perfect so
[05:25:43 -> 05:25:49]  everything we can look through these
[05:25:44 -> 05:25:52]  these all line up 1 2 3 4 5 6 7 8 9 10
[05:25:49 -> 05:25:55]  11 12 13 14 15 16 elements uh and this
[05:25:52 -> 05:25:58]  one has you know 16 elements as we print
[05:25:55 -> 05:26:00]  out um at the bottom here just the
[05:25:58 -> 05:26:02]  length of
[05:26:00 -> 05:26:05]  that so I try to be like fa quick with
[05:26:02 -> 05:26:07]  that it's very you know kind of just a
[05:26:05 -> 05:26:11]  bunch of boiler plate code that we we
[05:26:07 -> 05:26:13]  run through um but now uh now that we
[05:26:11 -> 05:26:15]  know that this works and it's outputting
[05:26:13 -> 05:26:17]  what we want it to we can actually go
[05:26:15 -> 05:26:20]  take a look at the comparison script so
[05:26:17 -> 05:26:22]  the comparison script um actually real
[05:26:20 -> 05:26:24]  quick before I go to this comparison
[05:26:22 -> 05:26:27]  script I'm going to bring up the uh the
[05:26:24 -> 05:26:31]  cian and docs here just to kind of show
[05:26:27 -> 05:26:33]  you the
[05:26:31 -> 05:26:37]  uh go to
[05:26:33 -> 05:26:40]  CNN so we're doing this convolution to
[05:26:37 -> 05:26:42]  CN convolution forward right so we look
[05:26:40 -> 05:26:46]  at this um and there's a bunch of things
[05:26:42 -> 05:26:46]  in it so like this descriptor type
[05:26:49 -> 05:26:53]  um handle Alpha Beta so it just
[05:26:51 -> 05:26:55]  describes everything that we need to
[05:26:53 -> 05:26:57]  know right so if something I said didn't
[05:26:55 -> 05:26:59]  make sense maybe just like look at here
[05:26:57 -> 05:27:02]  and it might make better sense to you
[05:26:59 -> 05:27:04]  that way um but if we want to say look
[05:27:02 -> 05:27:06]  at something like the forward algo type
[05:27:04 -> 05:27:07]  we go here click on it and there's a
[05:27:06 -> 05:27:10]  bunch of there's a bunch of values so
[05:27:07 -> 05:27:11]  implicit um expresses as a matrix
[05:27:10 -> 05:27:12]  product without actually explicitly
[05:27:11 -> 05:27:14]  forming the Matrix that holds the input
[05:27:12 -> 05:27:15]  tensor in so there's a bunch of
[05:27:14 -> 05:27:18]  descriptions here about like different
[05:27:15 -> 05:27:20]  algorithms you can use um and just like
[05:27:18 -> 05:27:22]  when you might want to use it there's
[05:27:20 -> 05:27:24]  also other articles on like which ones
[05:27:22 -> 05:27:26]  are good for different cases um
[05:27:24 -> 05:27:27]  convolutions are very very well covered
[05:27:26 -> 05:27:31]  uh in the whole deep learning space so
[05:27:27 -> 05:27:33]  shouldn't be too hard to navigate but
[05:27:31 -> 05:27:35]  these are these are going to be your
[05:27:33 -> 05:27:38]  forward forward pass
[05:27:35 -> 05:27:42]  algorithms now if we
[05:27:38 -> 05:27:44]  go back to um we go back to this
[05:27:42 -> 05:27:48]  comparison script I'm essentially doing
[05:27:44 -> 05:27:51]  the exact same thing except I set the um
[05:27:48 -> 05:27:55]  I set the algorithm used to implicit gem
[05:27:51 -> 05:27:59]  so that's the um that's this one right
[05:27:55 -> 05:28:04]  here so I just set that manually and uh
[05:27:59 -> 05:28:08]  yeah that's that's that's pretty much it
[05:28:04 -> 05:28:10]  um now we we we set this to algo and
[05:28:08 -> 05:28:12]  then we just plug an algo in there so
[05:28:10 -> 05:28:13]  that's that's like the main difference
[05:28:12 -> 05:28:15]  and then the other one is just how we
[05:28:13 -> 05:28:18]  initialize our data everything else is
[05:28:15 -> 05:28:20]  the same as this initial like uh just
[05:28:18 -> 05:28:26]  the the convolution uh compare like
[05:28:20 -> 05:28:30]  between P torch um so we where is
[05:28:26 -> 05:28:33]  it yes so we initialize uh on the CPU
[05:28:30 -> 05:28:34]  with just a bunch of random values um
[05:28:33 -> 05:28:38]  and then we just do an operation with
[05:28:34 -> 05:28:40]  those so I make the I make the element a
[05:28:38 -> 05:28:46]  lot make this whole thing a lot bigger
[05:28:40 -> 05:28:51]  so it's 224 by 224 by 11 by 32 by
[05:28:46 -> 05:28:52]  uh 11 11 32 64 it's not it's not times
[05:28:51 -> 05:28:56]  all of these but the input is going to
[05:28:52 -> 05:29:01]  be of size width by height uh it's going
[05:28:56 -> 05:29:05]  to be width by height by by in channels
[05:29:01 -> 05:29:08]  by batch size so n chw as we did before
[05:29:05 -> 05:29:09]  it's going to take up a lot of space um
[05:29:08 -> 05:29:12]  but we're just going to Benchmark this
[05:29:09 -> 05:29:12]  and see how it
[05:29:15 -> 05:29:21]  goes so we go and run this get a batch
[05:29:18 -> 05:29:25]  size of four and we notice CN average
[05:29:21 -> 05:29:26]  time is 14.8 millisecs and the naive
[05:29:25 -> 05:29:30]  kernel average time is about 82
[05:29:26 -> 05:29:33]  milliseconds so if we do that if we do
[05:29:30 -> 05:29:33]  that division
[05:29:34 -> 05:29:42]  there we notice that we get a 5.5x speed
[05:29:40 -> 05:29:44]  up by using 2D andm how awesome is that
[05:29:42 -> 05:29:46]  right I mean if you optimized this naive
[05:29:44 -> 05:29:48]  kernel up here and made it like more
[05:29:46 -> 05:29:50]  specific to your your specific use case
[05:29:48 -> 05:29:52]  rather than calculating a bunch of stuff
[05:29:50 -> 05:29:55]  it would be faster um but this is kind
[05:29:52 -> 05:29:57]  of just like for demonstration purposes
[05:29:55 -> 05:29:59]  um CNN is still wildly fast and it would
[05:29:57 -> 05:30:02]  take a while to get something that is
[05:29:59 -> 05:30:04]  actually um more performant than this um
[05:30:02 -> 05:30:07]  like significantly more performant it
[05:30:04 -> 05:30:09]  would take a lot to do that um but
[05:30:07 -> 05:30:11]  that's that's the idea right uh this is
[05:30:09 -> 05:30:13]  why you use things like CNN and P torch
[05:30:11 -> 05:30:15]  is because they're they're just like
[05:30:13 -> 05:30:18]  super fast they just it just did a
[05:30:15 -> 05:30:21]  massive convolution operation of like an
[05:30:18 -> 05:30:25]  image with 32 channels of batch size 4
[05:30:21 -> 05:30:27]  and a kernel size with like 11 by 11 um
[05:30:25 -> 05:30:28]  it it's just like insane the amount of
[05:30:27 -> 05:30:30]  operations it does in such a little
[05:30:28 -> 05:30:33]  amount of time so that's what we're
[05:30:30 -> 05:30:36]  working with um that's that's uh that's
[05:30:33 -> 05:30:38]  that's C and in for
[05:30:36 -> 05:30:41]  you now when you're working with big
[05:30:38 -> 05:30:42]  massive data centers and GPU clusters
[05:30:41 -> 05:30:45]  even if it's your own local rig that's
[05:30:42 -> 05:30:47]  just on the side and you have 8 409s or
[05:30:45 -> 05:30:50]  309s connected to it um this part might
[05:30:47 -> 05:30:54]  come in a little bit of Handy right so
[05:30:50 -> 05:30:56]  larger rigs or data centers um let me
[05:30:54 -> 05:31:00]  just change that how that looks um so
[05:30:56 -> 05:31:02]  you have kublos MP versus nccl versus
[05:31:00 -> 05:31:03]  Mig uh now these are all different I'm
[05:31:02 -> 05:31:05]  going to start with Mig because it's the
[05:31:03 -> 05:31:08]  easiest one to explain essentially think
[05:31:05 -> 05:31:11]  of it as your like your Amazon your AWS
[05:31:08 -> 05:31:13]  and you have a you have a giant you know
[05:31:11 -> 05:31:15]  GPU inside of your inside of your
[05:31:13 -> 05:31:17]  facility your data center and so
[05:31:15 -> 05:31:19]  typically with this type of GPU most
[05:31:17 -> 05:31:21]  people aren't going to uh use the
[05:31:19 -> 05:31:22]  entirety of it just like a small chunk
[05:31:21 -> 05:31:24]  they just want the parallel processing
[05:31:22 -> 05:31:27]  aspect for like some some whatever
[05:31:24 -> 05:31:28]  signal processing I don't know and so
[05:31:27 -> 05:31:32]  what you can do is you can actually
[05:31:28 -> 05:31:34]  split that node um you split that node
[05:31:32 -> 05:31:37]  into a bunch of a bunch of smaller gpus
[05:31:34 -> 05:31:39]  it's multi-instance GPU you can have
[05:31:37 -> 05:31:42]  multiple inst es connected to the same
[05:31:39 -> 05:31:45]  card and you can split workloads evenly
[05:31:42 -> 05:31:47]  and securely across those and so that's
[05:31:45 -> 05:31:49]  what that's what Mig does it's used in
[05:31:47 -> 05:31:54]  uh data center environments use cases
[05:31:49 -> 05:31:56]  right um and then you have nccl so nccl
[05:31:54 -> 05:31:59]  is actually really really important for
[05:31:56 -> 05:32:01]  um distributed cluster Computing so this
[05:31:59 -> 05:32:04]  is essentially going
[05:32:01 -> 05:32:07]  to it's exactly how it sounds right it's
[05:32:04 -> 05:32:09]  not going to it's not going to do
[05:32:07 -> 05:32:12]  operations across but it's going to help
[05:32:09 -> 05:32:14]  manage and communicate uh different
[05:32:12 -> 05:32:16]  things across a cluster so use for
[05:32:14 -> 05:32:18]  Distributing information collecting it
[05:32:16 -> 05:32:22]  acting as a general cluster level
[05:32:18 -> 05:32:24]  Communicator whereas kublos MP up here
[05:32:22 -> 05:32:26]  is going to be doing the grunt work of
[05:32:24 -> 05:32:28]  doing like say giant Matrix
[05:32:26 -> 05:32:30]  multiplications across like a note of
[05:32:28 -> 05:32:33]  eight 8
[05:32:30 -> 05:32:36]  8100s um and then nccl is going to is
[05:32:33 -> 05:32:38]  going to uh like run this in batches so
[05:32:36 -> 05:32:41]  remember Collective Communications uh
[05:32:38 -> 05:32:43]  these the operations within that um and
[05:32:41 -> 05:32:45]  and there's more resources on this uh
[05:32:43 -> 05:32:48]  would be like all reduce broadcast
[05:32:45 -> 05:32:51]  gather scatter across right so not like
[05:32:48 -> 05:32:53]  there's no like multiply or like fused
[05:32:51 -> 05:32:55]  operations in here it's it's doing it
[05:32:53 -> 05:32:59]  across a cluster and communicating uh
[05:32:55 -> 05:33:02]  communicating data right um so just for
[05:32:59 -> 05:33:05]  reference in pytorch you would use
[05:33:02 -> 05:33:07]  distributed data parallel um for this
[05:33:05 -> 05:33:09]  distributed cluster level Computing so
[05:33:07 -> 05:33:11]  distributed data parallel data
[05:33:09 -> 05:33:13]  parallelism at the module level which
[05:33:11 -> 05:33:15]  can run across multiple machines so
[05:33:13 -> 05:33:17]  module would be like say a function in P
[05:33:15 -> 05:33:20]  torch right um which can run across
[05:33:17 -> 05:33:22]  multiple machines um should spawn
[05:33:20 -> 05:33:23]  multiple processes and create a single
[05:33:22 -> 05:33:25]  data distributed parallel instance per
[05:33:23 -> 05:33:27]  process uh and there's there's a bunch
[05:33:25 -> 05:33:29]  of more stuff you can read about this
[05:33:27 -> 05:33:32]  this is used a lot actually in pytorch
[05:33:29 -> 05:33:34]  it's fairly simple to set up um but this
[05:33:32 -> 05:33:38]  is this is like kind of all of that
[05:33:34 -> 05:33:41]  simplified into one usable thing um
[05:33:38 -> 05:33:43]  now going back um there's I do have more
[05:33:41 -> 05:33:45]  resources on this too um I'm not going
[05:33:43 -> 05:33:46]  to cover all of this since I don't
[05:33:45 -> 05:33:48]  actually have a cluster in my house to
[05:33:46 -> 05:33:50]  experiment it on uh but there are some
[05:33:48 -> 05:33:52]  links and resources that you could you
[05:33:50 -> 05:33:53]  know you could find yourself going down
[05:33:52 -> 05:33:59]  a rabbit hole with these which is what
[05:33:53 -> 05:34:01]  might be quite fun um but kuas MP is
[05:33:59 -> 05:34:04]  actually going to it's literally
[05:34:01 -> 05:34:06]  designed to do distributed basic dense
[05:34:04 -> 05:34:08]  linear algebra so if you're doing like a
[05:34:06 -> 05:34:12]  multi-gpu
[05:34:08 -> 05:34:15]  um tensor tensor operation um this kualo
[05:34:12 -> 05:34:17]  MP would handle that so if we go back
[05:34:15 -> 05:34:22]  here and we go to
[05:34:17 -> 05:34:24]  kuas uh kuas MP high performance Cuda
[05:34:22 -> 05:34:29]  library for distributed dens linear
[05:34:24 -> 05:34:32]  algebra um getting started um for
[05:34:29 -> 05:34:35]  example like code samples um
[05:34:32 -> 05:34:38]  right you you have these giant like
[05:34:35 -> 05:34:39]  grids and stuff that you handle um
[05:34:38 -> 05:34:43]  how to use
[05:34:39 -> 05:34:44]  it um yeah maybe like how to use it for
[05:34:43 -> 05:34:47]  example cap
[05:34:44 -> 05:34:48]  API um and there's a bunch of
[05:34:47 -> 05:34:51]  interesting resources on here as to how
[05:34:48 -> 05:34:53]  you would do things maybe there's like
[05:34:51 -> 05:34:53]  an
[05:34:57 -> 05:35:02]  operation I don't know um that's this is
[05:35:00 -> 05:35:04]  up for you to navigate it's optional you
[05:35:02 -> 05:35:05]  don't actually have to know kublos MP
[05:35:04 -> 05:35:06]  because pytorch does a lot of that for
[05:35:05 -> 05:35:08]  you if you're just working with those
[05:35:06 -> 05:35:10]  workloads um but if you're going to be
[05:35:08 -> 05:35:11]  working on like data center
[05:35:10 -> 05:35:15]  infrastructure this is something you
[05:35:11 -> 05:35:18]  want to learn like Koss MP and nccl
[05:35:15 -> 05:35:23]  so um hopefully that helps to provide
[05:35:18 -> 05:35:23]  some context on uh like larger larger
[05:35:23 -> 05:35:27]  setups give yourself a pad on the back
[05:35:25 -> 05:35:30]  if you made it this far this has been a
[05:35:27 -> 05:35:31]  lot so far and we've covered we've
[05:35:30 -> 05:35:33]  covered actually an insane amount and
[05:35:31 -> 05:35:36]  now we're going to cover one of the more
[05:35:33 -> 05:35:37]  technical parts of the course called uh
[05:35:36 -> 05:35:41]  matrix multiplication and how we we
[05:35:37 -> 05:35:44]  optimize it so this is going to be one
[05:35:41 -> 05:35:47]  of the most technical Parts uh mainly
[05:35:44 -> 05:35:49]  because we're looking at uh like
[05:35:47 -> 05:35:51]  lowlevel optimizations how do we
[05:35:49 -> 05:35:53]  actually speed this up on the hardware
[05:35:51 -> 05:35:55]  and so it's no longer just like the
[05:35:53 -> 05:35:56]  general idea of how it works we're
[05:35:55 -> 05:35:58]  actually using our knowledge and
[05:35:56 -> 05:36:00]  additional knowledge that I'm going to
[05:35:58 -> 05:36:02]  share with you on how we can makes uh on
[05:36:00 -> 05:36:05]  how we can make the fundamental matrix
[05:36:02 -> 05:36:08]  multiplication or the mat Mill algorithm
[05:36:05 -> 05:36:10]  uh really really fast so this algorithm
[05:36:08 -> 05:36:13]  is proprietary in deep learning it is
[05:36:10 -> 05:36:15]  everywhere uh and so I figured the best
[05:36:13 -> 05:36:18]  way to teach you how to optimize kernels
[05:36:15 -> 05:36:22]  would be to use this as an example and
[05:36:18 -> 05:36:24]  luckily uh we have we have a repo by uh
[05:36:22 -> 05:36:26]  Simon bohim I think I think that's how
[05:36:24 -> 05:36:28]  you pronounce it this guy's a this guy's
[05:36:26 -> 05:36:31]  a performance or Colonel engineer at
[05:36:28 -> 05:36:33]  anthropic so he probably knows what he's
[05:36:31 -> 05:36:36]  doing and uh he made this really cool
[05:36:33 -> 05:36:38]  repo called sjem cuda as well as a blog
[05:36:36 -> 05:36:39]  post to go along with
[05:36:38 -> 05:36:41]  so I'm just going to be following this
[05:36:39 -> 05:36:43]  um and I was kind of lazy and not going
[05:36:41 -> 05:36:45]  to like write all of this from scratch
[05:36:43 -> 05:36:48]  on my own so I kind of just went along
[05:36:45 -> 05:36:50]  with this and I want to explain to you
[05:36:48 -> 05:36:53]  uh the steps of going through this and
[05:36:50 -> 05:36:54]  how we actually how we Pro how we
[05:36:53 -> 05:36:57]  progress through these different steps
[05:36:54 -> 05:37:00]  getting up to uh pretty close to kublos
[05:36:57 -> 05:37:01]  Performance and gigaflops uh or perhaps
[05:37:00 -> 05:37:03]  even surpassing it depending on which
[05:37:01 -> 05:37:05]  Hardware you have but uh this this is
[05:37:03 -> 05:37:07]  going to be the goal so you might have
[05:37:05 -> 05:37:09]  come across this article already uh but
[05:37:07 -> 05:37:11]  but in case you haven't or in case you
[05:37:09 -> 05:37:13]  have maybe this was too hard I'm just
[05:37:11 -> 05:37:15]  going to go over this and uh we're we're
[05:37:13 -> 05:37:17]  going to we're going to go superow level
[05:37:15 -> 05:37:19]  things are going to be super clear after
[05:37:17 -> 05:37:21]  you finish this part um you're going to
[05:37:19 -> 05:37:23]  understand how to optimize uh Cuda
[05:37:21 -> 05:37:25]  kernels so let's get started we're not
[05:37:23 -> 05:37:27]  actually going to occupy the cud course
[05:37:25 -> 05:37:29]  repo with this I'm going to link it in
[05:37:27 -> 05:37:30]  the read me file so you can follow along
[05:37:29 -> 05:37:32]  in case you're just going through the in
[05:37:30 -> 05:37:34]  case you're going through this this
[05:37:32 -> 05:37:36]  course repo but I'm actually going to
[05:37:34 -> 05:37:38]  take some steps back here and I'm going
[05:37:36 -> 05:37:42]  to clone it into my CA directory here so
[05:37:38 -> 05:37:42]  I'm just going to delete the old version
[05:37:42 -> 05:37:47]  um and we're going to get clone the
[05:37:45 -> 05:37:50]  other repo in so it's just that
[05:37:47 -> 05:37:53]  literally just uh pop in here copy copy
[05:37:50 -> 05:37:56]  paste this in um go Ahad and get clone
[05:37:53 -> 05:37:59]  that uh and then we're going to go ahead
[05:37:56 -> 05:38:02]  and open this up and open this in vs
[05:37:59 -> 05:38:05]  code so I'm going to drag that to my
[05:38:02 -> 05:38:10]  side monitor here going to close
[05:38:05 -> 05:38:13]  this and open this one up
[05:38:10 -> 05:38:15]  again now inside of here we're going to
[05:38:13 -> 05:38:18]  look at the read me first for setup um
[05:38:15 -> 05:38:20]  so we can see that it uh in in the in
[05:38:18 -> 05:38:22]  the build instructions we have to do um
[05:38:20 -> 05:38:28]  we have to
[05:38:22 -> 05:38:31]  do make directory build um CD into
[05:38:28 -> 05:38:31]  it
[05:38:31 -> 05:38:37]  oh uh and then we're going to
[05:38:34 -> 05:38:41]  cake it's going to take take a second
[05:38:37 -> 05:38:45]  there and then we go D- build period and
[05:38:41 -> 05:38:48]  it's going to build everything for us um
[05:38:45 -> 05:38:49]  now the idea here is to uh build
[05:38:48 -> 05:38:53]  everything show all the different
[05:38:49 -> 05:38:57]  benchmarks uh after after we actually uh
[05:38:53 -> 05:39:01]  go through uh each optimization so we're
[05:38:57 -> 05:39:03]  going to essentially print out the kuas
[05:39:01 -> 05:39:04]  performance and then we're going to
[05:39:03 -> 05:39:06]  print out naive and then we're going to
[05:39:04 -> 05:39:07]  print out the next optimization and then
[05:39:06 -> 05:39:09]  we're going to print the next one X One
[05:39:07 -> 05:39:11]  XX until we get to the end then we're
[05:39:09 -> 05:39:13]  going to compare all of them and see
[05:39:11 -> 05:39:17]  which one is the fastest
[05:39:13 -> 05:39:18]  so we can go ahead and start off with
[05:39:17 -> 05:39:21]  just going
[05:39:18 -> 05:39:24]  into just going into here so have a
[05:39:21 -> 05:39:26]  bunch of different kernels we have naive
[05:39:24 -> 05:39:29]  we have a global global memory colest so
[05:39:26 -> 05:39:30]  if we actually go back to the uh blog
[05:39:29 -> 05:39:32]  post which is what you know I kind of
[05:39:30 -> 05:39:34]  expect you guys to follow there's not
[05:39:32 -> 05:39:38]  too much here it's mainly just the code
[05:39:34 -> 05:39:40]  so if we go back to the blog post um
[05:39:38 -> 05:39:41]  we're doing we're we're we're
[05:39:40 -> 05:39:43]  essentially just going in this order and
[05:39:41 -> 05:39:45]  he does it in this order too so the
[05:39:43 -> 05:39:48]  night of implantation I mean we've
[05:39:45 -> 05:39:50]  already done this right um I can
[05:39:48 -> 05:39:52]  actually go back and if I like make this
[05:39:50 -> 05:39:56]  full
[05:39:52 -> 05:39:59]  screen and I go out of this
[05:39:56 -> 05:40:05]  um and I go into the Cuda course writing
[05:39:59 -> 05:40:05]  your first kernels and then matm notice
[05:40:08 -> 05:40:11]  I'm going to copy this for a
[05:40:14 -> 05:40:20]  second and
[05:40:16 -> 05:40:23]  then we're just going to paste it uh
[05:40:20 -> 05:40:23]  inside of here just for
[05:40:24 -> 05:40:29]  reference so we have these two kernels
[05:40:28 -> 05:40:30]  and I just want to and I just want to
[05:40:29 -> 05:40:31]  make sure that we're all like caught up
[05:40:30 -> 05:40:33]  here the one I taught you before is the
[05:40:31 -> 05:40:39]  exact same one that we're showing here
[05:40:33 -> 05:40:41]  so this takes in um a b and C this one
[05:40:39 -> 05:40:43]  takes an A B and C it it takes an alpha
[05:40:41 -> 05:40:44]  and a beta as well I mean it's doing
[05:40:43 -> 05:40:46]  it's doing a different it's slightly
[05:40:44 -> 05:40:48]  different operation so it's not exactly
[05:40:46 -> 05:40:51]  matrix multiplication but it's it's
[05:40:48 -> 05:40:53]  essentially doing this uh this
[05:40:51 -> 05:40:55]  essentially the the CU loss map that I
[05:40:53 -> 05:40:57]  showed you before where it does the
[05:40:55 -> 05:41:01]  alpha term uh like times every single
[05:40:57 -> 05:41:03]  element in in the in the Matrix that we
[05:41:01 -> 05:41:09]  calculate in the actual matl output and
[05:41:03 -> 05:41:12]  then it adds um beta Plus plus a c
[05:41:09 -> 05:41:14]  Matrix and then assign C to that new one
[05:41:12 -> 05:41:16]  so it's a slightly different it's a
[05:41:14 -> 05:41:17]  slightly different operation that we do
[05:41:16 -> 05:41:19]  but we're mainly going to worry about
[05:41:17 -> 05:41:23]  the matrix multiplication mechanics that
[05:41:19 -> 05:41:25]  get get us to this temp variable so when
[05:41:23 -> 05:41:29]  we look at this we have M K and N so
[05:41:25 -> 05:41:30]  it's a matrix of shape so a is shape M
[05:41:29 -> 05:41:32]  so it's like M vertical that's like the
[05:41:30 -> 05:41:35]  batch size you could say and then it's
[05:41:32 -> 05:41:37]  like K as like the length number of
[05:41:35 -> 05:41:41]  columns right and then the B Matrix is
[05:41:37 -> 05:41:44]  going to be K vertically and N long
[05:41:41 -> 05:41:45]  right so we we essentially pass it in
[05:41:44 -> 05:41:48]  the same way and we index them the same
[05:41:45 -> 05:41:51]  way but in this example in in uh in
[05:41:48 -> 05:41:53]  Simon's example we just we just pass
[05:41:51 -> 05:41:55]  these in differently so it's like m and
[05:41:53 -> 05:41:56]  n as like the edges and then K is the
[05:41:55 -> 05:41:58]  middle one that we want to pay attention
[05:41:56 -> 05:42:00]  to so we save that for the third one I
[05:41:58 -> 05:42:02]  guess maybe that's that's thought the
[05:42:00 -> 05:42:05]  thought process there but anyways we
[05:42:02 -> 05:42:09]  have the we have X and Y so row and
[05:42:05 -> 05:42:12]  column um the Y is the same as the row
[05:42:09 -> 05:42:14]  here so uh which which y index is it at
[05:42:12 -> 05:42:16]  because a row like row is vertical it
[05:42:14 -> 05:42:18]  could be this row this row this it's
[05:42:16 -> 05:42:22]  like a vertical scale right and then the
[05:42:18 -> 05:42:24]  x is uh is column so which which column
[05:42:22 -> 05:42:27]  is it at this is this is
[05:42:24 -> 05:42:29]  horizontal and so we make sure it's not
[05:42:27 -> 05:42:30]  out of bounds and then we continue with
[05:42:29 -> 05:42:32]  what's inside of that little chunk of
[05:42:30 -> 05:42:35]  memory and and we we proceed right so we
[05:42:32 -> 05:42:37]  have this accumulator sum uh and then we
[05:42:35 -> 05:42:40]  have this L term
[05:42:37 -> 05:42:43]  uh we we could say that's the length and
[05:42:40 -> 05:42:45]  uh and then we have this K which is
[05:42:43 -> 05:42:49]  which is the length of a and then the
[05:42:45 -> 05:42:50]  height of uh the the height of B right
[05:42:49 -> 05:42:54]  so when you're do producting you're
[05:42:50 -> 05:42:57]  you're taking the uh the row uh the a
[05:42:54 -> 05:42:59]  row of a and a column of B so you're
[05:42:57 -> 05:43:02]  iterating over you're iterating over k
[05:42:59 -> 05:43:04]  and a and you're iterating over K and in
[05:43:02 -> 05:43:06]  B right so that that's where that that
[05:43:04 -> 05:43:07]  comes from that K stuff and so you're
[05:43:06 -> 05:43:09]  just iterating
[05:43:07 -> 05:43:11]  uh like in a row you're going through a
[05:43:09 -> 05:43:14]  like this you're going through each
[05:43:11 -> 05:43:15]  piece like each number and then in B
[05:43:14 -> 05:43:18]  you're going through each number
[05:43:15 -> 05:43:20]  vertically right and then we just plus
[05:43:18 -> 05:43:23]  plus each time and then this sum output
[05:43:20 -> 05:43:27]  this accumulator uh is just going to be
[05:43:23 -> 05:43:29]  essentially the so a it we're looking to
[05:43:27 -> 05:43:32]  uh multiply the first the first element
[05:43:29 -> 05:43:33]  here by the first element here and then
[05:43:32 -> 05:43:35]  and then advance and then advance and
[05:43:33 -> 05:43:37]  then advance and then Advance right
[05:43:35 -> 05:43:39]  that's what we're trying to do
[05:43:37 -> 05:43:41]  you could also think of it as like a a
[05:43:39 -> 05:43:44]  nice way I like to visualize this is
[05:43:41 -> 05:43:47]  like a a 2X two tile so you have you
[05:43:44 -> 05:43:49]  have like a 2X two tile you
[05:43:47 -> 05:43:53]  have I'm try to visualize this from your
[05:43:49 -> 05:43:56]  perspective you have like a up here and
[05:43:53 -> 05:44:01]  then you have B down here and it's like
[05:43:56 -> 05:44:03]  is that is that correct maybe it's maybe
[05:44:01 -> 05:44:06]  it's yeah it's a here and then B here
[05:44:03 -> 05:44:10]  and so when a has like a row that row is
[05:44:06 -> 05:44:12]  going to like point to uh like the the
[05:44:10 -> 05:44:14]  uh the y-coordinate in C and B is going
[05:44:12 -> 05:44:17]  to point to the x coordinate in C so
[05:44:14 -> 05:44:19]  when they like when they like intersect
[05:44:17 -> 05:44:20]  it's going to find the the index in C
[05:44:19 -> 05:44:22]  that you're going to calculate that dot
[05:44:20 -> 05:44:26]  product result from that's a cool way I
[05:44:22 -> 05:44:28]  like to visualize it for um but anyways
[05:44:26 -> 05:44:32]  getting back to the point uh so we have
[05:44:28 -> 05:44:34]  this row times uh times K so like which
[05:44:32 -> 05:44:37]  row are we at the length of the row
[05:44:34 -> 05:44:39]  right or sorry which which row are you
[05:44:37 -> 05:44:42]  at so which row relative to you know the
[05:44:39 -> 05:44:43]  the Cuda architecture itself and then K
[05:44:42 -> 05:44:45]  which is the length of it so you're
[05:44:43 -> 05:44:47]  going down you're essentially like
[05:44:45 -> 05:44:48]  wrapping you're striding around and
[05:44:47 -> 05:44:51]  you're doing this as many times as you
[05:44:48 -> 05:44:54]  want to as like depending on which row
[05:44:51 -> 05:44:56]  you're at and then you add the the K
[05:44:54 -> 05:44:58]  offset to it so you might not be all the
[05:44:56 -> 05:45:00]  way through the the length of it and so
[05:44:58 -> 05:45:01]  you stop and that's where that plus
[05:45:00 -> 05:45:04]  comes
[05:45:01 -> 05:45:08]  from and then same thing for here we
[05:45:04 -> 05:45:09]  have this L term which is K so how um
[05:45:08 -> 05:45:11]  essentially which column are you at
[05:45:09 -> 05:45:15]  right so here we have rows and then here
[05:45:11 -> 05:45:17]  we have columns so it's uh L which is
[05:45:15 -> 05:45:20]  essentially the the length of of that
[05:45:17 -> 05:45:23]  vertical the length of a column and then
[05:45:20 -> 05:45:27]  you iterate over uh you iterate Over N
[05:45:23 -> 05:45:28]  times right so n is the n is the length
[05:45:27 -> 05:45:31]  there and
[05:45:28 -> 05:45:34]  so you essentially you essentially
[05:45:31 -> 05:45:36]  Advance as many times you need to to the
[05:45:34 -> 05:45:38]  to the I guess you could say to the
[05:45:36 -> 05:45:40]  right and then you offset at whichever
[05:45:38 -> 05:45:44]  column index it is so so same idea we're
[05:45:40 -> 05:45:47]  just we're just advancing uh instead of
[05:45:44 -> 05:45:49]  instead of going rows we're going
[05:45:47 -> 05:45:52]  columns right and then we just
[05:45:49 -> 05:45:56]  essentially assign uh
[05:45:52 -> 05:45:59]  whichever in C we we index it like we go
[05:45:56 -> 05:46:02]  uh row time uh time n so it's an it's an
[05:45:59 -> 05:46:08]  M it's
[05:46:02 -> 05:46:10]  an it's an M by n so it's going to be
[05:46:08 -> 05:46:12]  like m is here and N is here so it's
[05:46:10 -> 05:46:15]  going to go
[05:46:12 -> 05:46:18]  uh row number times n so it's going to
[05:46:15 -> 05:46:20]  stride n every time it wraps and then
[05:46:18 -> 05:46:23]  it's going to do plus the column index
[05:46:20 -> 05:46:24]  which is that X component right and
[05:46:23 -> 05:46:26]  that's how you do the naive again just
[05:46:24 -> 05:46:28]  just to give you a little refresher
[05:46:26 -> 05:46:31]  there that was a while back we did it um
[05:46:28 -> 05:46:33]  and then the same idea here so you
[05:46:31 -> 05:46:36]  have m&n
[05:46:33 -> 05:46:38]  m&n you have the accumulator we have
[05:46:36 -> 05:46:39]  this I term that we're iterating through
[05:46:38 -> 05:46:42]  and we have
[05:46:39 -> 05:46:46]  K uh and then we go we have this temp
[05:46:42 -> 05:46:51]  term we go X so X is the same as row so
[05:46:46 -> 05:46:53]  notice how row is assigned to Y and X is
[05:46:51 -> 05:46:57]  um well actually got a little bit stuck
[05:46:53 -> 05:47:00]  there I was looking at the uh the block
[05:46:57 -> 05:47:02]  and thread uh indexing scheme here and
[05:47:00 -> 05:47:04]  it was kind of misleading so like notice
[05:47:02 -> 05:47:06]  here how we have rows and those are by
[05:47:04 -> 05:47:08]  the the Y index so whichever y position
[05:47:06 -> 05:47:10]  I it's at that's like the row that it's
[05:47:08 -> 05:47:12]  going to pluck out or the the column
[05:47:10 -> 05:47:15]  it's like X so it's going to pluck out a
[05:47:12 -> 05:47:19]  row or sorry a column in this in this
[05:47:15 -> 05:47:24]  example we do um like X so that refers
[05:47:19 -> 05:47:25]  to like right here um is is picking out
[05:47:24 -> 05:47:27]  like it it kind of makes sense right
[05:47:25 -> 05:47:31]  like X matches up with X and Y matches
[05:47:27 -> 05:47:33]  up with Y but when we look in here um
[05:47:31 -> 05:47:36]  like in comparison to this it's like the
[05:47:33 -> 05:47:39]  row times the stride of K so we're going
[05:47:36 -> 05:47:40]  to stride the the K length over and then
[05:47:39 -> 05:47:42]  then come back to the next one and then
[05:47:40 -> 05:47:45]  offset with
[05:47:42 -> 05:47:48]  L in this one we have X which is like a
[05:47:45 -> 05:47:50]  column Index right so it's like what why
[05:47:48 -> 05:47:53]  would you do that we want to we want a
[05:47:50 -> 05:47:55]  row index but this actually works and we
[05:47:53 -> 05:47:58]  don't have to worry too much because
[05:47:55 -> 05:48:01]  this is a square Matrix so because these
[05:47:58 -> 05:48:04]  values are actually the same because uh
[05:48:01 -> 05:48:07]  the grid and the and the thread index in
[05:48:04 -> 05:48:08]  both the the X and Y dimensions are equ
[05:48:07 -> 05:48:10]  we don't actually have to worry about
[05:48:08 -> 05:48:12]  that so this is something youd want to
[05:48:10 -> 05:48:14]  pay attention to in rectangular matrices
[05:48:12 -> 05:48:16]  but we don't have to worry about this
[05:48:14 -> 05:48:19]  right now so just kind of assume that uh
[05:48:16 -> 05:48:21]  we can kind of just say that this is uh
[05:48:19 -> 05:48:22]  like why and treat it that way but I'm
[05:48:21 -> 05:48:24]  not going to edit this because we might
[05:48:22 -> 05:48:28]  have to deal with this later on in uh in
[05:48:24 -> 05:48:30]  future kernels so you kind of get the
[05:48:28 -> 05:48:33]  idea though this is very similar to what
[05:48:30 -> 05:48:34]  we were doing before um don't pretty
[05:48:33 -> 05:48:37]  much just don't worry about the indexing
[05:48:34 -> 05:48:40]  scheme it's it's going to be fine
[05:48:37 -> 05:48:43]  um and then we yeah literally the only
[05:48:40 -> 05:48:47]  change here is that we write out uh
[05:48:43 -> 05:48:49]  using Alpha Beta and C right so that's
[05:48:47 -> 05:48:52]  really the only difference there so
[05:48:49 -> 05:48:57]  let's go ahead and actually run this
[05:48:52 -> 05:48:58]  now so we pop into uh SJ Cuda and then
[05:48:57 -> 05:49:02]  we go into
[05:48:58 -> 05:49:04]  build now we can go uh sjem and then we
[05:49:02 -> 05:49:08]  go number one so this is the naive
[05:49:04 -> 05:49:10]  kernel that we run and we can see that
[05:49:08 -> 05:49:14]  we're going to do a Max size so
[05:49:10 -> 05:49:16]  Dimensions M = Nal K right so these
[05:49:14 -> 05:49:20]  are okay I might have lagged there for a
[05:49:16 -> 05:49:22]  second but yeah I mean as we can see um
[05:49:20 -> 05:49:24]  Dimensions m equal nals K right so these
[05:49:22 -> 05:49:26]  are all the same it's just like 128
[05:49:24 -> 05:49:27]  essentially and then 256 512 1024 all
[05:49:26 -> 05:49:29]  the way up to
[05:49:27 -> 05:49:31]  496 and we can actually see the
[05:49:29 -> 05:49:35]  throughput and G A flops per second so
[05:49:31 -> 05:49:39]  what this means is how many billion Giga
[05:49:35 -> 05:49:42]  right Giga is uh * 10 9 so billion and
[05:49:39 -> 05:49:43]  then flops is floating Point operations
[05:49:42 -> 05:49:45]  per
[05:49:43 -> 05:49:50]  second and this is on a given size right
[05:49:45 -> 05:49:52]  so on size 128 we get on average 46.2
[05:49:50 -> 05:49:56]  gig uh billion floating Point operations
[05:49:52 -> 05:49:58]  per second and on 496 we get about 166
[05:49:56 -> 05:49:59]  billion floating Point operations per
[05:49:58 -> 05:50:01]  second which sounds like a lot that
[05:49:59 -> 05:50:04]  sounds like a lot of operations right
[05:50:01 -> 05:50:07]  166 billion per second wow that that's
[05:50:04 -> 05:50:08]  really high but the answer is that's
[05:50:07 -> 05:50:11]  actually not that high it's going to get
[05:50:08 -> 05:50:13]  a lot higher than this so high actually
[05:50:11 -> 05:50:14]  that it it's going to seem like this is
[05:50:13 -> 05:50:18]  minus kill this is going to be seem very
[05:50:14 -> 05:50:22]  very small and actually slow so notice
[05:50:18 -> 05:50:26]  how this took um this took uh eight
[05:50:22 -> 05:50:31]  about is 83 seconds to do 50 runs right
[05:50:26 -> 05:50:35]  so very um very slow or no per per run
[05:50:31 -> 05:50:37]  sorry so not for 50 runs but for for
[05:50:35 -> 05:50:40]  each run that it did which there were 50
[05:50:37 -> 05:50:43]  of it took about 83 seconds to do that
[05:50:40 -> 05:50:45]  on 496 with a naive kernel so a few
[05:50:43 -> 05:50:48]  other points before we actually jump
[05:50:45 -> 05:50:51]  into this um I want to First Look at the
[05:50:48 -> 05:50:54]  uh I want to First Look at the blog post
[05:50:51 -> 05:50:56]  here so when we're calculating the
[05:50:54 -> 05:50:58]  output in the KN implementation I mean
[05:50:56 -> 05:51:01]  like even just like looking at this uh
[05:50:58 -> 05:51:02]  it's like really intuitive uh I I love
[05:51:01 -> 05:51:06]  this example but
[05:51:02 -> 05:51:10]  anyways uh when we actually look at the
[05:51:06 -> 05:51:11]  the simple naive kernel um essentially
[05:51:10 -> 05:51:13]  we're trying
[05:51:11 -> 05:51:16]  to
[05:51:13 -> 05:51:19]  uh we're trying to
[05:51:16 -> 05:51:23]  find uh a certain part inside
[05:51:19 -> 05:51:24]  of we are trying to find a certain index
[05:51:23 -> 05:51:27]  inside of C that is going to be the
[05:51:24 -> 05:51:30]  output so we're saying we want to do the
[05:51:27 -> 05:51:34]  fastest possible calculation to get say
[05:51:30 -> 05:51:37]  this number uh this index calculated in
[05:51:34 -> 05:51:40]  the C output right in C
[05:51:37 -> 05:51:43]  uh and so right now the way to do that
[05:51:40 -> 05:51:45]  is to load in a row and a column and
[05:51:43 -> 05:51:47]  then just just calculate that that's
[05:51:45 -> 05:51:48]  that's what we found out naively so far
[05:51:47 -> 05:51:51]  and it takes very few lines of code to
[05:51:48 -> 05:51:52]  do that and so we just kind of iterate
[05:51:51 -> 05:51:54]  through like I was saying before how you
[05:51:52 -> 05:51:56]  put a on the side here and you let it
[05:51:54 -> 05:51:57]  you let them sort of act as coordinates
[05:51:56 -> 05:52:03]  that's that's what I was referring to
[05:51:57 -> 05:52:04]  there um but anyways uh that's that's
[05:52:03 -> 05:52:05]  kind of one of the one of the goals we
[05:52:04 -> 05:52:07]  want to keep in mind is how do we
[05:52:05 -> 05:52:09]  calculate the output in index it'll help
[05:52:07 -> 05:52:10]  you it'll help provide some context on
[05:52:09 -> 05:52:13]  how we actually get there cuz when we
[05:52:10 -> 05:52:14]  deal with more complex kernels you'll
[05:52:13 -> 05:52:16]  actually see there's a lot of steps to
[05:52:14 -> 05:52:18]  actually get to a certain place and so
[05:52:16 -> 05:52:20]  it helps when you're able to keep in a
[05:52:18 -> 05:52:22]  consistent uh frame of thought where
[05:52:20 -> 05:52:23]  it's like okay how are we actually
[05:52:22 -> 05:52:25]  ending up at this result and then you
[05:52:23 -> 05:52:28]  can sort of backtrack through and see
[05:52:25 -> 05:52:29]  what's happening um so instead of just
[05:52:28 -> 05:52:31]  like going and like reading like a novel
[05:52:29 -> 05:52:33]  from from the start of the konal to the
[05:52:31 -> 05:52:34]  end and just seeing like oh I guess like
[05:52:33 -> 05:52:36]  we we'll see what we stumble into it's
[05:52:34 -> 05:52:37]  like you actually want to see what
[05:52:36 -> 05:52:39]  you're trying to calculate in the end
[05:52:37 -> 05:52:42]  and that helps and and this blog post
[05:52:39 -> 05:52:43]  refat a lot of context on that so
[05:52:42 -> 05:52:45]  another little note I wanted to add is
[05:52:43 -> 05:52:47]  like don't worry about 3D structures too
[05:52:45 -> 05:52:50]  much so when we have the like the dim 3
[05:52:47 -> 05:52:52]  type and we have the the YX and Zed uh
[05:52:50 -> 05:52:54]  Dimensions all populated with numbers
[05:52:52 -> 05:52:56]  greater than one it's like four 2 and
[05:52:54 -> 05:52:58]  three it's like don't worry about that
[05:52:56 -> 05:52:59]  we're not going to be dealing with 3D
[05:52:58 -> 05:53:01]  stuff it's not going to be that
[05:52:59 -> 05:53:03]  complicated it's going to be more so
[05:53:01 -> 05:53:07]  like how do you transform uh
[05:53:03 -> 05:53:12]  onedimensional and two-dimensional um
[05:53:07 -> 05:53:13]  um dimensions and index efficiently so
[05:53:12 -> 05:53:17]  don't worry about 3D stuff we're not
[05:53:13 -> 05:53:22]  going to do any of that um and then the
[05:53:17 -> 05:53:25]  actual uh indexing scheme here is giving
[05:53:22 -> 05:53:26]  us coest memory access so I'll jump into
[05:53:25 -> 05:53:28]  this in the next current a little bit
[05:53:26 -> 05:53:31]  more in depth but essentially what's
[05:53:28 -> 05:53:34]  happening is uh when we when we're doing
[05:53:31 -> 05:53:36]  this uh like row calculation for example
[05:53:34 -> 05:53:39]  um what it's like when it goes through
[05:53:36 -> 05:53:41]  through um each row essentially what's
[05:53:39 -> 05:53:43]  happening is uh we have this we have
[05:53:41 -> 05:53:47]  this like X term from here and we put
[05:53:43 -> 05:53:49]  this x term in all of the like in Cuda
[05:53:47 -> 05:53:50]  when you have adjacent meaning like in
[05:53:49 -> 05:53:53]  the X dimension in like in like the
[05:53:50 -> 05:53:55]  length part horizontal when they are
[05:53:53 -> 05:54:00]  next to each other you actually get CEST
[05:53:55 -> 05:54:04]  memory accesses so when you're accessing
[05:54:00 -> 05:54:05]  um a you actually you can actually uh in
[05:54:04 -> 05:54:08]  in assembly it's actually going to group
[05:54:05 -> 05:54:11]  multiple of the together into one so
[05:54:08 -> 05:54:13]  when you have like a when you have a a
[05:54:11 -> 05:54:17]  block inside of that you have a a warp
[05:54:13 -> 05:54:22]  and inside of the warps you have um you
[05:54:17 -> 05:54:25]  have uh 32 threads per warp right and so
[05:54:22 -> 05:54:28]  we can like in inside of the actual warp
[05:54:25 -> 05:54:30]  itself it's going to uh it's going to
[05:54:28 -> 05:54:32]  call us memory access if possible and
[05:54:30 -> 05:54:34]  when things are adjacent it actually
[05:54:32 -> 05:54:35]  makes that possible so that's kind of
[05:54:34 -> 05:54:37]  why we're seeing the weird indexing
[05:54:35 -> 05:54:39]  scheme here again doesn't work for
[05:54:37 -> 05:54:41]  rectangular matrices but in this case
[05:54:39 -> 05:54:45]  it's kind of a an efficiency Improvement
[05:54:41 -> 05:54:47]  for for indexing um the a matrix so
[05:54:45 -> 05:54:49]  before we pop over to the global memory
[05:54:47 -> 05:54:52]  colas kernel I thought I should probably
[05:54:49 -> 05:54:53]  highlight something so this is important
[05:54:52 -> 05:54:55]  to know for all kernels and even the
[05:54:53 -> 05:54:58]  previous ones too but this is just kind
[05:54:55 -> 05:55:01]  of like how memory is laid out right so
[05:54:58 -> 05:55:04]  when we have a 2 X2 Matrix 1 2 34 um in
[05:55:01 -> 05:55:07]  memory this is going to be laid out as
[05:55:04 -> 05:55:10]  literally just a vector 1 2 3 4
[05:55:07 -> 05:55:11]  so when we want to like for example in a
[05:55:10 -> 05:55:14]  when we want to go to the next
[05:55:11 -> 05:55:16]  essentially the next row and then do an
[05:55:14 -> 05:55:21]  offset what we're doing is we're going a
[05:55:16 -> 05:55:23]  current row times K which is this
[05:55:21 -> 05:55:26]  Dimension here the the the horizontal
[05:55:23 -> 05:55:28]  one so we're doing let's just say we
[05:55:26 -> 05:55:30]  want to get to the number four here
[05:55:28 -> 05:55:33]  right so if we want to get to number
[05:55:30 -> 05:55:35]  four it's going to be current row well
[05:55:33 -> 05:55:37]  the current row is going to be zero and
[05:55:35 -> 05:55:40]  one right so current is going to be one
[05:55:37 -> 05:55:44]  and then K is twoo long so it's going to
[05:55:40 -> 05:55:46]  be uh 1 * 2 which is going to give us
[05:55:44 -> 05:55:49]  this index so uh like the array at index
[05:55:46 -> 05:55:53]  two is this so it's like 0 1 and two and
[05:55:49 -> 05:55:54]  then the offset from that is going to be
[05:55:53 -> 05:55:57]  the the the column index that's going to
[05:55:54 -> 05:56:00]  be one so it's going to be uh 2 + 1 and
[05:55:57 -> 05:56:01]  it's going to give us array at index 3
[05:56:00 -> 05:56:04]  so that's just kind of what I mean by
[05:56:01 -> 05:56:05]  strides it's like how how you go and you
[05:56:04 -> 05:56:08]  like jump across the whole row that's
[05:56:05 -> 05:56:11]  kind of what I mean there but going into
[05:56:08 -> 05:56:13]  this uh glob memory Cass kernel scroll
[05:56:11 -> 05:56:15]  down
[05:56:13 -> 05:56:17]  and uh and I'm just going to I'm kind of
[05:56:15 -> 05:56:20]  going to like skip this part and and
[05:56:17 -> 05:56:22]  just lay it out for you but the whole
[05:56:20 -> 05:56:26]  idea here and this is the critical
[05:56:22 -> 05:56:28]  concept so the Matrix memory layout as I
[05:56:26 -> 05:56:29]  just highlighted it's going to be it's
[05:56:28 -> 05:56:30]  going to be consecutive memory like this
[05:56:29 -> 05:56:33]  what going to
[05:56:30 -> 05:56:34]  be it's going to be laid out like that
[05:56:33 -> 05:56:36]  um and this is not going to be
[05:56:34 -> 05:56:39]  consecutive memory right
[05:56:36 -> 05:56:42]  so when we do do product of this and
[05:56:39 -> 05:56:43]  this it's going to go to the third uh
[05:56:42 -> 05:56:45]  third row and this is going to go to the
[05:56:43 -> 05:56:48]  third column and we get this value right
[05:56:45 -> 05:56:51]  this is a consecutive this is not in the
[05:56:48 -> 05:56:53]  naive kernel we jump through these and
[05:56:51 -> 05:56:56]  we iterate through uh we iterate through
[05:56:53 -> 05:56:57]  a so we start off this like B column and
[05:56:56 -> 05:57:01]  then we
[05:56:57 -> 05:57:01]  go and then we we go to the next
[05:57:02 -> 05:57:07]  one right we we we index in that fashion
[05:57:06 -> 05:57:10]  we advance through the arrays in that
[05:57:07 -> 05:57:12]  fashion and what we end up with in the
[05:57:10 -> 05:57:14]  output is we get this like we get this
[05:57:12 -> 05:57:16]  stack of blocks right and this and this
[05:57:14 -> 05:57:17]  is what it looks like when we write to
[05:57:16 -> 05:57:21]  the output it's going to be a stack of
[05:57:17 -> 05:57:24]  blocks because we write um vertically as
[05:57:21 -> 05:57:28]  as the rows advance in a that's what we
[05:57:24 -> 05:57:30]  prioritize however if we uh
[05:57:28 -> 05:57:34]  instead cess the memory
[05:57:30 -> 05:57:37]  accesses uh we can get we can get these
[05:57:34 -> 05:57:40]  laid out in this fashion so essentially
[05:57:37 -> 05:57:43]  what we're doing is we're changing the
[05:57:40 -> 05:57:46]  indexing scheme here um all of this
[05:57:43 -> 05:57:48]  essentially Remains the Same except we
[05:57:46 -> 05:57:51]  change the way that this is indexed and
[05:57:48 -> 05:57:53]  we ensure that we're using thread idx
[05:57:51 -> 05:57:56]  right so remember when we did uh when I
[05:57:53 -> 05:57:59]  I was previously talking about how um
[05:57:56 -> 05:58:03]  all of the essentially all of the all of
[05:57:59 -> 05:58:05]  the the X like the thread idxx component
[05:58:03 -> 05:58:07]  those are grouped together in a warp so
[05:58:05 -> 05:58:10]  if you have like for example block size
[05:58:07 -> 05:58:13]  32 um there's going to be 32 threads in
[05:58:10 -> 05:58:16]  a warp that that's the maximum and so in
[05:58:13 -> 05:58:19]  a in like blocks if you have this like
[05:58:16 -> 05:58:22]  this this Square Block it's like 32 by
[05:58:19 -> 05:58:23]  32 and you get the X Dimension you're
[05:58:22 -> 05:58:25]  going to get the maximum number of
[05:58:23 -> 05:58:27]  threads in a single warp if each element
[05:58:25 -> 05:58:29]  there is dedicated to a different thread
[05:58:27 -> 05:58:33]  right and so this way you're maximizing
[05:58:29 -> 05:58:35]  the memory accesses because you can put
[05:58:33 -> 05:58:37]  all of those together and you can make
[05:58:35 -> 05:58:39]  that load uh you can make that data
[05:58:37 -> 05:58:41]  transfer operation much more efficient
[05:58:39 -> 05:58:44]  when you let an entire warp take care of
[05:58:41 -> 05:58:45]  it it can do all the values at once or
[05:58:44 -> 05:58:48]  or or group them together and make them
[05:58:45 -> 05:58:51]  like way faster as opposed to going
[05:58:48 -> 05:58:54]  through each individual uh each
[05:58:51 -> 05:58:58]  individual uh like y component right so
[05:58:54 -> 05:59:00]  when it goes like thread idx doy it's
[05:58:58 -> 05:59:03]  actually it's actually not as efficient
[05:59:00 -> 05:59:06]  right um and so we kind of just Chang
[05:59:03 -> 05:59:08]  the indexing scheme here with these to
[05:59:06 -> 05:59:10]  sort of illustrate the previous point I
[05:59:08 -> 05:59:12]  wrote a little table for uh like what's
[05:59:10 -> 05:59:14]  in the brackets here so this division of
[05:59:12 -> 05:59:17]  thread idx and block size then the
[05:59:14 -> 05:59:19]  modulo of or the modulus of that um so I
[05:59:17 -> 05:59:20]  just kind of wrote a table here of what
[05:59:19 -> 05:59:23]  these would actually look like in
[05:59:20 -> 05:59:24]  practicality so if we just assume block
[05:59:23 -> 05:59:26]  size is four which mean it's not in this
[05:59:24 -> 05:59:28]  case but we can just simplify and
[05:59:26 -> 05:59:29]  understand what's going on that way I
[05:59:28 -> 05:59:32]  don't have to write out like a bunch of
[05:59:29 -> 05:59:34]  numbers uh we assume block size is four
[05:59:32 -> 05:59:36]  and so because block the because block
[05:59:34 -> 05:59:39]  size that the size of an individual
[05:59:36 -> 05:59:42]  block is four that means the thread idx
[05:59:39 -> 05:59:44]  is going to have four indices uh in it
[05:59:42 -> 05:59:46]  so it's going to be thread idx Z and
[05:59:44 -> 05:59:49]  then one two and three it's going to
[05:59:46 -> 05:59:51]  have four inside of it right so when we
[05:59:49 -> 05:59:53]  divide we're going to floor the
[05:59:51 -> 05:59:54]  operation uh that that's just what's
[05:59:53 -> 05:59:56]  going to happen naturally is this is
[05:59:54 -> 05:59:58]  going to get floored it's going to it's
[05:59:56 -> 06:00:00]  going to truncate the end of it because
[05:59:58 -> 06:00:06]  we're doing integer
[06:00:00 -> 06:00:09]  Division and we're going to get 0 0 0 0
[06:00:06 -> 06:00:11]  right 3 / 4 is 75 it's going to trunk it
[06:00:09 -> 06:00:14]  75 and you're still going to have zero
[06:00:11 -> 06:00:16]  um and then we jump up to when like this
[06:00:14 -> 06:00:18]  advances then it's going to be uh well
[06:00:16 -> 06:00:21]  the block size is four and when the idx
[06:00:18 -> 06:00:23]  jumps to one then it's going to it's
[06:00:21 -> 06:00:26]  going to be 1 Time 4 is four and then
[06:00:23 -> 06:00:28]  plus Z and it wrap it kind of just like
[06:00:26 -> 06:00:30]  resets right except it's plus one so we
[06:00:28 -> 06:00:33]  have that going on then we have the
[06:00:30 -> 06:00:36]  modulus as well so modulus is uh you
[06:00:33 -> 06:00:38]  divide so 0 divided by four
[06:00:36 -> 06:00:41]  um like integer and then what's the
[06:00:38 -> 06:00:42]  remainder of that so if we do 1 / 4 um
[06:00:41 -> 06:00:44]  that doesn't actually equal a whole
[06:00:42 -> 06:00:45]  number so you end up with a remainder of
[06:00:44 -> 06:00:49]  one and then you do that for the rest of
[06:00:45 -> 06:00:51]  them so like 3id 4 is or three mod mod
[06:00:49 -> 06:00:53]  four is three and then four mod four
[06:00:51 -> 06:00:54]  since it just equals one there's no
[06:00:53 -> 06:00:56]  remainder left so which just is zero
[06:00:54 -> 06:01:02]  right and you get this thing where it's
[06:00:56 -> 06:01:06]  like 0 0 uh 0 0000 0 1111 one and then
[06:01:02 -> 06:01:11]  here it's like 0 1 2 3 0 1 2 3 right so
[06:01:06 -> 06:01:14]  when we actually look at um this example
[06:01:11 -> 06:01:17]  here inside of
[06:01:14 -> 06:01:21]  the where is it no not this one inside
[06:01:17 -> 06:01:23]  of the Coles kernel um notice how in I'm
[06:01:21 -> 06:01:27]  going to show you a second in our code
[06:01:23 -> 06:01:29]  how this row does not actually change
[06:01:27 -> 06:01:31]  and what we're doing is we're just
[06:01:29 -> 06:01:33]  indexing very carefully these values so
[06:01:31 -> 06:01:35]  when we're when we have different
[06:01:33 -> 06:01:37]  threads that are like because each
[06:01:35 -> 06:01:41]  thread is going to calculate its own uh
[06:01:37 -> 06:01:43]  dot product right um like this thread
[06:01:41 -> 06:01:46]  and this thread adjacent to each other
[06:01:43 -> 06:01:48]  in the same warp they're going to access
[06:01:46 -> 06:01:50]  um adjacent values so when they're
[06:01:48 -> 06:01:52]  accessing adjacent values in the same
[06:01:50 -> 06:01:54]  warp you can actually group all these
[06:01:52 -> 06:01:56]  together whereas instead if you just uh
[06:01:54 -> 06:01:58]  did this one if you did this thread and
[06:01:56 -> 06:02:00]  then this thread and then this thread
[06:01:58 -> 06:02:03]  that means that the first index of all
[06:02:00 -> 06:02:05]  those uh all those threads um you you
[06:02:03 -> 06:02:07]  you cannot actually you cannot actually
[06:02:05 -> 06:02:08]  col that because you have to do like a
[06:02:07 -> 06:02:12]  stride and they're not they're not
[06:02:08 -> 06:02:14]  adjacent right so that's essentially
[06:02:12 -> 06:02:15]  what we're doing here um and then we end
[06:02:14 -> 06:02:17]  up with this like instead of a stacking
[06:02:15 -> 06:02:21]  like blocks we end up with this with
[06:02:17 -> 06:02:23]  this horizontal layout um so when we go
[06:02:21 -> 06:02:25]  here we can see that c row so this is
[06:02:23 -> 06:02:28]  only actually going to change every
[06:02:25 -> 06:02:30]  every um every time we advance right so
[06:02:28 -> 06:02:32]  this is going to stay at zero which
[06:02:30 -> 06:02:34]  means that c row that's going to stay at
[06:02:32 -> 06:02:36]  zero and then the plus I part that's
[06:02:34 -> 06:02:40]  going to advance with with the dot
[06:02:36 -> 06:02:42]  product itself um and then here I is
[06:02:40 -> 06:02:44]  automatically going to advance so that
[06:02:42 -> 06:02:47]  means it's going to uh it's it's going
[06:02:44 -> 06:02:49]  to advance a column each time while the
[06:02:47 -> 06:02:52]  row is going to stay at the same place
[06:02:49 -> 06:02:53]  because c row is staying at zero right
[06:02:52 -> 06:02:56]  and so you can kind of see how this
[06:02:53 -> 06:02:58]  works out we have we have C column or or
[06:02:56 -> 06:03:00]  or current column and this is actually
[06:02:58 -> 06:03:02]  going to change over the threads so it's
[06:03:00 -> 06:03:04]  going to go zero it's going to go 0 1 2
[06:03:02 -> 06:03:07]  3 4 and then it's going to jump to the
[06:03:04 -> 06:03:09]  next block block idea right and so what
[06:03:07 -> 06:03:14]  you end up with is literally what I just
[06:03:09 -> 06:03:16]  demonstrated um you end up by
[06:03:14 -> 06:03:18]  essentially each each thread within that
[06:03:16 -> 06:03:21]  warp is is accessing an adjacent value
[06:03:18 -> 06:03:24]  and so you can group those and and cess
[06:03:21 -> 06:03:26]  or combine the memory aises together and
[06:03:24 -> 06:03:28]  we get more performance efficiency with
[06:03:26 -> 06:03:30]  this so that's kind of like what this
[06:03:28 -> 06:03:32]  article that this section talked about
[06:03:30 -> 06:03:35]  um and if
[06:03:32 -> 06:03:39]  we if we bump back bump back to here and
[06:03:35 -> 06:03:42]  actually run this so kernel number
[06:03:39 -> 06:03:46]  two we can see um we're actually getting
[06:03:42 -> 06:03:50]  a lot higher Giga flops on this
[06:03:46 -> 06:03:53]  one uh and then we can see that we get
[06:03:50 -> 06:03:55]  about 1183 gig of flops here so that's
[06:03:53 -> 06:03:57]  actually a pretty big increase of
[06:03:55 -> 06:04:01]  performance I think previously it was in
[06:03:57 -> 06:04:03]  the it was about 10 and uh what was it
[06:04:01 -> 06:04:05]  like 180 or something or
[06:04:03 -> 06:04:07]  like 160 I can't remember but it was
[06:04:05 -> 06:04:09]  very low so this is actually like
[06:04:07 -> 06:04:12]  significantly it's like 10 almost 10x
[06:04:09 -> 06:04:14]  higher this is like maybe 5 8 10x higher
[06:04:12 -> 06:04:16]  than what we had before on the 496
[06:04:14 -> 06:04:18]  Square Matrix right so that's actually a
[06:04:16 -> 06:04:23]  crazy performance Improvement we were
[06:04:18 -> 06:04:26]  previously at like 83 seconds uh per uh
[06:04:23 -> 06:04:30]  per
[06:04:26 -> 06:04:32]  um per run and now this is at point
[06:04:30 -> 06:04:38]  point about2 so if you if you actually
[06:04:32 -> 06:04:40]  do the math there um 83 over uh 12
[06:04:38 -> 06:04:42]  that's about a 7x increase in
[06:04:40 -> 06:04:47]  performance in throughput so uh that's
[06:04:42 -> 06:04:50]  that's pretty good um now we can uh now
[06:04:47 -> 06:04:52]  we can move on to Shared memory cache
[06:04:50 -> 06:04:55]  blocking which introduces a different
[06:04:52 -> 06:04:57]  concept still uses what we've currently
[06:04:55 -> 06:04:59]  done but introduces a whole another
[06:04:57 -> 06:05:01]  Paradigm that's going to uh really help
[06:04:59 -> 06:05:03]  accelerate and speed things up so next
[06:05:01 -> 06:05:05]  we jump into something called shared
[06:05:03 -> 06:05:07]  memory or SRAM and this is abs abolutely
[06:05:05 -> 06:05:10]  critical to take care of when we're
[06:05:07 -> 06:05:12]  optimizing algorithms for performance so
[06:05:10 -> 06:05:18]  let me just kind of explain what the
[06:05:12 -> 06:05:22]  deals with this so right now we're using
[06:05:18 -> 06:05:25]  uh Global memory right uh the the host
[06:05:22 -> 06:05:26]  is just our little Ram slots uh going to
[06:05:25 -> 06:05:28]  the CPU and that's like really slow
[06:05:26 -> 06:05:33]  that's about 5 gigabytes per second
[06:05:28 -> 06:05:35]  still fast but very slow compared to um
[06:05:33 -> 06:05:39]  this 200 GB per second that we get with
[06:05:35 -> 06:05:41]  our vram this is what we're using now or
[06:05:39 -> 06:05:43]  you can get even faster and use shared
[06:05:41 -> 06:05:46]  memory which is around 1.5 tab per
[06:05:43 -> 06:05:48]  second of memory bandwidth or registers
[06:05:46 -> 06:05:49]  which is about 8 terabytes per second of
[06:05:48 -> 06:05:52]  memory bandwidth we're just going to
[06:05:49 -> 06:05:53]  focus on registers right now or or sorry
[06:05:52 -> 06:05:59]  shared memory right
[06:05:53 -> 06:06:01]  now um now in this blog post he had uh
[06:05:59 -> 06:06:03]  about 700 gigabyt of glove memory band
[06:06:01 -> 06:06:06]  which is really fast compared to this
[06:06:03 -> 06:06:08]  and then about uh
[06:06:06 -> 06:06:11]  12 terabytes of or 12.1 terabytes of
[06:06:08 -> 06:06:14]  shared memory bandwidth
[06:06:11 -> 06:06:18]  so uh or sorry not 12 terabytes 1 Point
[06:06:14 -> 06:06:19]  1 Point 1.2 terabytes memory bandwidth
[06:06:18 -> 06:06:21]  um terabytes per
[06:06:19 -> 06:06:24]  second
[06:06:21 -> 06:06:26]  now how do we capitalize on that how do
[06:06:24 -> 06:06:29]  we actually use shared memory well it's
[06:06:26 -> 06:06:33]  actually easy
[06:06:29 -> 06:06:36]  um you use this little keyword called uh
[06:06:33 -> 06:06:39]  it's not here but
[06:06:36 -> 06:06:42]  I have it it's called Uh shared so
[06:06:39 -> 06:06:45]  shared memory is literally just how you
[06:06:42 -> 06:06:47]  use the that Shar that little L1 cache
[06:06:45 -> 06:06:50]  so when we look up at the actual
[06:06:47 -> 06:06:54]  architecture of this um I could open
[06:06:50 -> 06:06:56]  image and new tab so you have your
[06:06:54 -> 06:06:58]  Global memory so like the the big chunk
[06:06:56 -> 06:07:01]  of memory that you have that's about two
[06:06:58 -> 06:07:03]  200 gigas 200 gigb a second then the L2
[06:07:01 -> 06:07:06]  cache for like a transfer medium and
[06:07:03 -> 06:07:09]  then each little SM or streaming
[06:07:06 -> 06:07:12]  multiprocessor um these have their own
[06:07:09 -> 06:07:14]  little L1 cache or the shared memory and
[06:07:12 -> 06:07:16]  this is very small compared to these two
[06:07:14 -> 06:07:18]  right um but they are extremely fast and
[06:07:16 -> 06:07:20]  they connect directly with registers and
[06:07:18 -> 06:07:23]  the and the cores on your
[06:07:20 -> 06:07:25]  GPU so when we can utilize these there's
[06:07:23 -> 06:07:26]  actually far less travel distance you
[06:07:25 -> 06:07:28]  have to go so instead of like every time
[06:07:26 -> 06:07:32]  you need to access a float you go all
[06:07:28 -> 06:07:34]  the way through um like s m or shared
[06:07:32 -> 06:07:37]  and then to L2 and then to Global you
[06:07:34 -> 06:07:39]  literally just just uh store a bunch of
[06:07:37 -> 06:07:41]  them temporarily in s and in shared
[06:07:39 -> 06:07:45]  memory have all of the threads use them
[06:07:41 -> 06:07:47]  for like like essentially do a ton of
[06:07:45 -> 06:07:49]  work with the memory that it has and
[06:07:47 -> 06:07:52]  then once you're finished with that you
[06:07:49 -> 06:07:56]  can replace it uh you can you can write
[06:07:52 -> 06:07:58]  new values from Global so instead of uh
[06:07:56 -> 06:08:01]  writing from Global every time you need
[06:07:58 -> 06:08:03]  to access something you instead load a
[06:08:01 -> 06:08:05]  you preemptively load a bunch into
[06:08:03 -> 06:08:08]  shared memory and then you use them for
[06:08:05 -> 06:08:09]  a bunch of work and then you then you
[06:08:08 -> 06:08:11]  replace them with a new one once you
[06:08:09 -> 06:08:14]  advance
[06:08:11 -> 06:08:17]  right and sort of the goal here is just
[06:08:14 -> 06:08:20]  making sure that we do this properly so
[06:08:17 -> 06:08:20]  if I just go out
[06:08:21 -> 06:08:26]  again shared memory is located on chip
[06:08:24 -> 06:08:30]  much lower latency higher higher memory
[06:08:26 -> 06:08:32]  bandwidth um heed this on a voltage GPU
[06:08:30 -> 06:08:33]  so to go over how exactly we'll be using
[06:08:32 -> 06:08:35]  shared memory it's actually a bit of a
[06:08:33 -> 06:08:37]  different philosophy now I'm not not
[06:08:35 -> 06:08:39]  going to like do like write this out and
[06:08:37 -> 06:08:42]  everything because it can get like quite
[06:08:39 -> 06:08:44]  uh it can get quite intensive when we
[06:08:42 -> 06:08:46]  write stuff out
[06:08:44 -> 06:08:48]  but this is essentially what we're doing
[06:08:46 -> 06:08:50]  we're doing a little thing called tiling
[06:08:48 -> 06:08:54]  which I demonstrated earlier where you
[06:08:50 -> 06:08:56]  have uh little tiles that you do Matrix
[06:08:54 -> 06:08:58]  multiplies for in a in a bigger Matrix
[06:08:56 -> 06:09:00]  multiply
[06:08:58 -> 06:09:04]  so instead of doing instead of doing
[06:09:00 -> 06:09:06]  rows and columns we actually go and do
[06:09:04 -> 06:09:08]  something a bit different and instead
[06:09:06 -> 06:09:10]  what that is is is say we have this this
[06:09:08 -> 06:09:13]  Chunk in C here and and so what you
[06:09:10 -> 06:09:14]  would do is you would essentially like
[06:09:13 -> 06:09:17]  if you have
[06:09:14 -> 06:09:21]  um say
[06:09:17 -> 06:09:25]  a let me try to use the current vs code
[06:09:21 -> 06:09:29]  thing as an example so we have this uh
[06:09:25 -> 06:09:31]  we have a we have a a c right now going
[06:09:29 -> 06:09:31]  back
[06:09:31 -> 06:09:36]  to going back to this one I'm going to
[06:09:33 -> 06:09:39]  try to exp this is this have to explain
[06:09:36 -> 06:09:41]  so bear with me um we're essentially
[06:09:39 -> 06:09:43]  going to load in tiles and we have this
[06:09:41 -> 06:09:44]  little C like the coordinates here and
[06:09:43 -> 06:09:48]  all we're going to do is we're just
[06:09:44 -> 06:09:50]  going to multiply multiply these two um
[06:09:48 -> 06:09:52]  together and then multiply these two
[06:09:50 -> 06:09:54]  together and multiply these two together
[06:09:52 -> 06:09:58]  and then like we just essentially
[06:09:54 -> 06:10:01]  multiply every Matrix um and then that
[06:09:58 -> 06:10:04]  that that like coordinates up to uh this
[06:10:01 -> 06:10:06]  this final thing here so we go we go
[06:10:04 -> 06:10:08]  like through this way way and we sort of
[06:10:06 -> 06:10:12]  we start at the very like the the very
[06:10:08 -> 06:10:14]  top of B and the Very left of of a and
[06:10:12 -> 06:10:16]  we multiply those together and then we
[06:10:14 -> 06:10:18]  add it to the next ones we go in and in
[06:10:16 -> 06:10:20]  and in and in and in until we like maybe
[06:10:18 -> 06:10:24]  Cross or something and then that like
[06:10:20 -> 06:10:27]  intersection Point
[06:10:24 -> 06:10:30]  um that intersection point right here
[06:10:27 -> 06:10:32]  that's where C is right and so when when
[06:10:30 -> 06:10:34]  you have a bunch of these smaller um
[06:10:32 -> 06:10:36]  less intensive map moles that can be
[06:10:34 -> 06:10:38]  actually done on blocks
[06:10:36 -> 06:10:39]  uh on on thread blocks then it actually
[06:10:38 -> 06:10:42]  makes the job a lot easier because what
[06:10:39 -> 06:10:43]  you can do is you can be smart about it
[06:10:42 -> 06:10:45]  and actually store these blocks in
[06:10:43 -> 06:10:47]  shared memory right uh if you're doing
[06:10:45 -> 06:10:49]  individual rows or columns I mean sure
[06:10:47 -> 06:10:51]  you could do that but it actually allows
[06:10:49 -> 06:10:54]  us to uh distribute some of the work
[06:10:51 -> 06:10:57]  more when we're when we're using blocks
[06:10:54 -> 06:11:01]  when we're using literal tiles of the
[06:10:57 -> 06:11:03]  Matrix right uh so let's go ahead and
[06:11:01 -> 06:11:05]  dig into how this actually works under
[06:11:03 -> 06:11:07]  the hood you're going to understand ort
[06:11:05 -> 06:11:09]  of how tiling Works more once I explain
[06:11:07 -> 06:11:13]  it and how I explain how everything
[06:11:09 -> 06:11:16]  advances as as we dig more into detail
[06:11:13 -> 06:11:18]  but uh this this is the idea here we
[06:11:16 -> 06:11:19]  just we tile and we we we store these
[06:11:18 -> 06:11:22]  blocks temporarily in shared memory and
[06:11:19 -> 06:11:25]  do as much work with them as possible
[06:11:22 -> 06:11:28]  okay so this is the code for a shared uh
[06:11:25 -> 06:11:32]  a shared memory cach or or tiled mapal
[06:11:28 -> 06:11:34]  you could say and uh pretty much a lot
[06:11:32 -> 06:11:36]  of it uh well not a lot of it but the
[06:11:34 -> 06:11:40]  start is pretty close to or actually the
[06:11:36 -> 06:11:43]  exact same as as the last C we wrote so
[06:11:40 -> 06:11:47]  we have this uh c row maps to block idx
[06:11:43 -> 06:11:52]  dox c row maps to block idx dox and C
[06:11:47 -> 06:11:54]  column isy C column isy now we have this
[06:11:52 -> 06:11:58]  thread column uh
[06:11:54 -> 06:12:01]  is uh maps to the mod operator and then
[06:11:58 -> 06:12:05]  row maps to division operator right so
[06:12:01 -> 06:12:09]  row maps to division and column maps to
[06:12:05 -> 06:12:12]  uh mod right so these are these are
[06:12:09 -> 06:12:14]  the the we essentially use the the same
[06:12:12 -> 06:12:16]  idea as before and then we add this
[06:12:14 -> 06:12:18]  additional piece in which is going to
[06:12:16 -> 06:12:20]  allocate uh some some space in the
[06:12:18 -> 06:12:24]  shared memory which is going to be of
[06:12:20 -> 06:12:26]  size uh block size by block size right
[06:12:24 -> 06:12:27]  so it's just giant thing you could say
[06:12:26 -> 06:12:29]  that like each of these little rows just
[06:12:27 -> 06:12:32]  like wraps around and you have this like
[06:12:29 -> 06:12:33]  super long thing laid out in memory but
[06:12:32 -> 06:12:36]  we're going to treat it as an actual
[06:12:33 -> 06:12:39]  block like a square um and so why we use
[06:12:36 -> 06:12:41]  block size by block size is because um
[06:12:39 -> 06:12:42]  if we just say I mean we're going to
[06:12:41 -> 06:12:45]  lower we're going to lower what what we
[06:12:42 -> 06:12:47]  interpret block size to be in in the
[06:12:45 -> 06:12:49]  examples just for intuition purposes but
[06:12:47 -> 06:12:52]  in practicality this would be 32 right
[06:12:49 -> 06:12:55]  block size will be 32 you have 32
[06:12:52 -> 06:12:58]  threads that fit in the warp and maximum
[06:12:55 -> 06:13:00]  1,24 threads per block so if you
[06:12:58 -> 06:13:00]  actually
[06:13:00 -> 06:13:08]  divide um if you divide
[06:13:04 -> 06:13:11]  1,24 by 32 you get 32 so what we end up
[06:13:08 -> 06:13:12]  doing is we have a warp a warp takes
[06:13:11 -> 06:13:14]  care of these warp takes care of these
[06:13:12 -> 06:13:15]  warp takes care of these and we have
[06:13:14 -> 06:13:17]  like we're literally taking up the
[06:13:15 -> 06:13:21]  maximum amount everywhere uh just by
[06:13:17 -> 06:13:23]  using block size uh or a shared a shared
[06:13:21 -> 06:13:26]  allocation of block size by by block
[06:13:23 -> 06:13:29]  size right that that's the idea there um
[06:13:26 -> 06:13:33]  and so we go down and I'm just going to
[06:13:29 -> 06:13:35]  pull this up on the side here just for
[06:13:33 -> 06:13:37]  reference um
[06:13:35 -> 06:13:41]  so in
[06:13:37 -> 06:13:42]  a what we do is we say uh we're going to
[06:13:41 -> 06:13:45]  advance the pointers to the starting
[06:13:42 -> 06:13:49]  positions right
[06:13:45 -> 06:13:53]  so essentially we're going to
[06:13:49 -> 06:13:57]  multiply uh c row we're going to do c
[06:13:53 -> 06:14:02]  the the the current row times uh times
[06:13:57 -> 06:14:04]  oh can zoom out the current row times K
[06:14:02 -> 06:14:06]  right so K is going to be this Dimension
[06:14:04 -> 06:14:09]  here this this this long one the the
[06:14:06 -> 06:14:11]  sort the horizontal one and so if we
[06:14:09 -> 06:14:14]  multiply the current row by K let's just
[06:14:11 -> 06:14:19]  say we have like a current row
[06:14:14 -> 06:14:21]  of uh if we have a current row of if we
[06:14:19 -> 06:14:23]  want to do current row of one right so
[06:14:21 -> 06:14:25]  we want to do current row time K which
[06:14:23 -> 06:14:27]  is this length so it's going to jump
[06:14:25 -> 06:14:29]  down to this one and then we want to do
[06:14:27 -> 06:14:32]  times block size which in this case
[06:14:29 -> 06:14:33]  since we split it into a bunch of tiles
[06:14:32 -> 06:14:35]  is going to be two right it's going to
[06:14:33 -> 06:14:37]  be 2 by two
[06:14:35 -> 06:14:40]  uh and so we end up jumping two instead
[06:14:37 -> 06:14:41]  of that so we go um the current row is
[06:14:40 -> 06:14:45]  going to be one so we're going to do
[06:14:41 -> 06:14:46]  this this one here so it's going to jump
[06:14:45 -> 06:14:49]  uh the length of
[06:14:46 -> 06:14:51]  that times the number we want to do
[06:14:49 -> 06:14:52]  which is essentially one so we don't
[06:14:51 -> 06:14:54]  jump down one and then that doubles
[06:14:52 -> 06:14:55]  because we have block size equal to two
[06:14:54 -> 06:14:57]  right so it's going downum two rows and
[06:14:55 -> 06:15:01]  then we end up exactly where we want we
[06:14:57 -> 06:15:03]  want to start at the first number um on
[06:15:01 -> 06:15:05]  the first on the first uh like
[06:15:03 -> 06:15:09]  essentially tile of this row
[06:15:05 -> 06:15:12]  right uh and then B we we advance that
[06:15:09 -> 06:15:14]  pointer to uh the current column times
[06:15:12 -> 06:15:18]  block size so in this case b let's say
[06:15:14 -> 06:15:23]  we want B to be uh like two right so the
[06:15:18 -> 06:15:28]  current column is uh is is 2 so we do 2
[06:15:23 -> 06:15:32]  * 2 which is four so we go 0 1 2 3 4 and
[06:15:28 -> 06:15:34]  we end up there right so very intuitive
[06:15:32 -> 06:15:37]  uh and then we have C which essentially
[06:15:34 -> 06:15:38]  combines the to so uh in this case we
[06:15:37 -> 06:15:43]  want to do c so it's going to it's going
[06:15:38 -> 06:15:45]  to be um like it's going to be this row
[06:15:43 -> 06:15:48]  and this column here so we're going to
[06:15:45 -> 06:15:51]  end up it's going to be um the the first
[06:15:48 -> 06:15:53]  row and then and then this column so
[06:15:51 -> 06:15:55]  it's going to be this tile uh that we
[06:15:53 -> 06:15:58]  want to take care of so it's going to
[06:15:55 -> 06:16:00]  jump it's going to jump over to uh this
[06:15:58 -> 06:16:05]  one from
[06:16:00 -> 06:16:06]  the uh from the uh what's it called from
[06:16:05 -> 06:16:10]  from the a matrix we're going to jump
[06:16:06 -> 06:16:12]  all the way to or what was it no no no
[06:16:10 -> 06:16:14]  in B we're going to jump all the way
[06:16:12 -> 06:16:17]  here and then in a uh we're we're
[06:16:14 -> 06:16:19]  essentially just going to add the we're
[06:16:17 -> 06:16:24]  going to add the offset right so we want
[06:16:19 -> 06:16:26]  to we want to jump down to because this
[06:16:24 -> 06:16:31]  is the end Dimension so instead of
[06:16:26 -> 06:16:31]  multiplying by K we would do
[06:16:32 -> 06:16:38]  uh we would do uh we would do times n so
[06:16:35 -> 06:16:40]  it ends up being like essentially this
[06:16:38 -> 06:16:43]  we essentially this plus this right
[06:16:40 -> 06:16:44]  except instead of uh instead of using K
[06:16:43 -> 06:16:46]  we use n because that's that's the
[06:16:44 -> 06:16:49]  length of C right so that that's the
[06:16:46 -> 06:16:54]  idea there um and then we end up at uh I
[06:16:49 -> 06:16:57]  believe it was this this totle 24 25 34
[06:16:54 -> 06:16:58]  35 um so we continue to go down and this
[06:16:57 -> 06:17:01]  is where we Define our accumulator right
[06:16:58 -> 06:17:03]  so the temporary accumulator we have uh
[06:17:01 -> 06:17:05]  and then stuff really gets interesting
[06:17:03 -> 06:17:07]  once we once we get inside of this for
[06:17:05 -> 06:17:11]  Loop this is where the magic happens um
[06:17:07 -> 06:17:13]  so we have this term uh block idx and
[06:17:11 -> 06:17:17]  we're going to iterate over K right so K
[06:17:13 -> 06:17:18]  is that K is that uh the row the the
[06:17:17 -> 06:17:21]  sorry the column The Columns number of
[06:17:18 -> 06:17:23]  columns in a and the number of rows in B
[06:17:21 -> 06:17:25]  right so we're going to advance block
[06:17:23 -> 06:17:28]  size each time uh or sorry going to
[06:17:25 -> 06:17:32]  advance advance block idx by block size
[06:17:28 -> 06:17:35]  each time um and so inside of here
[06:17:32 -> 06:17:37]  initially we want to store uh the stuff
[06:17:35 -> 06:17:40]  in SRAM right or or shared memory so we
[06:17:37 -> 06:17:44]  want to store it in here and literally
[06:17:40 -> 06:17:47]  all we do is we look at uh the index
[06:17:44 -> 06:17:50]  inside of here so thread row which row
[06:17:47 -> 06:17:51]  is it times the block size so block size
[06:17:50 -> 06:17:53]  is going to be that stride or that
[06:17:51 -> 06:17:55]  wrapper and then plus the thread column
[06:17:53 -> 06:17:56]  so which offset do we want to be at
[06:17:55 -> 06:18:01]  right it's going to pick out a certain
[06:17:56 -> 06:18:03]  spot in there um and luckily enough uh I
[06:18:01 -> 06:18:06]  picked a block size of two so it's going
[06:18:03 -> 06:18:08]  to be 2 by two and that actually makes
[06:18:06 -> 06:18:09]  our job a lot more simpler to understand
[06:18:08 -> 06:18:12]  I mean you can abstract it up to like
[06:18:09 -> 06:18:14]  four or 8 or even 32 but we're going to
[06:18:12 -> 06:18:16]  stick with block size of two for now and
[06:18:14 -> 06:18:19]  this means we're just going to have um
[06:18:16 -> 06:18:21]  we're just going to have two a thread
[06:18:19 -> 06:18:26]  thread index thread
[06:18:21 -> 06:18:28]  idxx of zero and one right so very very
[06:18:26 -> 06:18:31]  basic threads to work with here and so
[06:18:28 -> 06:18:32]  we're essentially just going to load uh
[06:18:31 -> 06:18:35]  into this this shared memory which we
[06:18:32 -> 06:18:38]  defined up here uh and we're just going
[06:18:35 -> 06:18:40]  to essentially that that little spot
[06:18:38 -> 06:18:43]  inside of it we're going to we're going
[06:18:40 -> 06:18:46]  to pick that out from A and B so in a
[06:18:43 -> 06:18:48]  it's going to be the thread row times K
[06:18:46 -> 06:18:51]  so K is going to be that that that
[06:18:48 -> 06:18:54]  length right
[06:18:51 -> 06:18:57]  uh and then it's going to be
[06:18:54 -> 06:19:00]  uh plus the plus the thread column
[06:18:57 -> 06:19:02]  offset right so just that that ex
[06:19:00 -> 06:19:05]  essentially the the same idea as what
[06:19:02 -> 06:19:09]  we're doing here um
[06:19:05 -> 06:19:13]  and then we're going to have the uh same
[06:19:09 -> 06:19:16]  idea for B which is going to be n so n
[06:19:13 -> 06:19:20]  is that again n is
[06:19:16 -> 06:19:22]  the n is the top one here um and then K
[06:19:20 -> 06:19:26]  is the top one here so K corresponds to
[06:19:22 -> 06:19:28]  A and N corresponds to B right um I hope
[06:19:26 -> 06:19:30]  that kind of makes sense and then
[06:19:28 -> 06:19:32]  afterwards we just sync up everything so
[06:19:30 -> 06:19:34]  this part's a little weird because we're
[06:19:32 -> 06:19:36]  doing like sync threads but this kernel
[06:19:34 -> 06:19:39]  itself like everything in here up till
[06:19:36 -> 06:19:41]  now is like a thread so what this means
[06:19:39 -> 06:19:42]  is that in the entire Block it's going
[06:19:41 -> 06:19:44]  to make sure that all the threads catch
[06:19:42 -> 06:19:45]  up to this point it's going to make sure
[06:19:44 -> 06:19:47]  that every it's going to put a barrier
[06:19:45 -> 06:19:48]  and makes all make sure all the threads
[06:19:47 -> 06:19:51]  have like put what they needed to in
[06:19:48 -> 06:19:53]  memory or else if we start doing other
[06:19:51 -> 06:19:55]  things then you might have like a zero
[06:19:53 -> 06:19:56]  value there where there's like there's
[06:19:55 -> 06:19:58]  nothing that exists at that place in
[06:19:56 -> 06:20:00]  memory and you're using that to do
[06:19:58 -> 06:20:01]  operations which is then going to make
[06:20:00 -> 06:20:03]  your answer wrong so you want to make
[06:20:01 -> 06:20:05]  sure that all of the threads within the
[06:20:03 -> 06:20:07]  block are actually caught up to here so
[06:20:05 -> 06:20:09]  like these are on the level of threads
[06:20:07 -> 06:20:11]  but we're essentially telling Cuda that
[06:20:09 -> 06:20:12]  we want all of the different threads
[06:20:11 -> 06:20:14]  that are doing all these parallel
[06:20:12 -> 06:20:16]  operations to catch up within the for
[06:20:14 -> 06:20:18]  Loop that's what we're
[06:20:16 -> 06:20:23]  doing
[06:20:18 -> 06:20:24]  um then we uh Advance a uh then we
[06:20:23 -> 06:20:27]  advance a by block size so this is just
[06:20:24 -> 06:20:29]  like advancing it preemptively we
[06:20:27 -> 06:20:31]  already have all of this stuff St uh
[06:20:29 -> 06:20:33]  stored in shared memory so we can
[06:20:31 -> 06:20:35]  actually just Advance a we can ADV we
[06:20:33 -> 06:20:37]  can advance the a point because remember
[06:20:35 -> 06:20:39]  a is a pointer right um we can only
[06:20:37 -> 06:20:42]  actually use like the index to get the
[06:20:39 -> 06:20:44]  values but a itself is a pointer so we
[06:20:42 -> 06:20:47]  advance that in memory we advance that
[06:20:44 -> 06:20:51]  in the memory space by block size so a
[06:20:47 -> 06:20:54]  is uh so a is like this this side one
[06:20:51 -> 06:20:56]  that's like going to point inwards to c
[06:20:54 -> 06:20:59]  and then B is going to point downwards
[06:20:56 -> 06:21:02]  so uh a is going to advance a single
[06:20:59 -> 06:21:05]  block so if a is like here for example
[06:21:02 -> 06:21:09]  um a is going to advance block side so
[06:21:05 -> 06:21:10]  two it's going to jump to here right um
[06:21:09 -> 06:21:14]  and then
[06:21:10 -> 06:21:16]  B uh B is going to B is going to do the
[06:21:14 -> 06:21:19]  same but it's going to jump so it's
[06:21:16 -> 06:21:19]  going to
[06:21:20 -> 06:21:24]  go it it's just going to it's going to
[06:21:22 -> 06:21:26]  Jump N right so it's n is like this
[06:21:24 -> 06:21:28]  length it's going to do block size times
[06:21:26 -> 06:21:30]  n it's going to jump it's going to jump
[06:21:28 -> 06:21:31]  down two right and it's it's going to do
[06:21:30 -> 06:21:34]  exactly what we want so it's going to
[06:21:31 -> 06:21:37]  advance the tiles in the directions that
[06:21:34 -> 06:21:39]  we expect Ed them to you might have had
[06:21:37 -> 06:21:41]  some confusion about how we're just
[06:21:39 -> 06:21:45]  using like the thread columns and just a
[06:21:41 -> 06:21:47]  reminder uh these this a term is already
[06:21:45 -> 06:21:49]  advanced to the correct position right
[06:21:47 -> 06:21:52]  so once we're Advanced to like this this
[06:21:49 -> 06:21:55]  Tile For example then we can just then
[06:21:52 -> 06:21:57]  we can pretty much just use threads we
[06:21:55 -> 06:21:59]  can use the the the thread indexing
[06:21:57 -> 06:22:01]  scheme and that'll give us exactly what
[06:21:59 -> 06:22:01]  we
[06:22:02 -> 06:22:06]  want so now notice inside of this for
[06:22:04 -> 06:22:09]  Loop we have the same indexing scheme as
[06:22:06 -> 06:22:12]  we did in the global memory colest uh
[06:22:09 -> 06:22:15]  kernel so when we uh when when we're
[06:22:12 -> 06:22:18]  just efficient about going through the
[06:22:15 -> 06:22:20]  columns and having the C as like a as
[06:22:18 -> 06:22:22]  like a horizontal layout instead of like
[06:22:20 -> 06:22:23]  a vertical stack of blocks that we're
[06:22:22 -> 06:22:27]  just doing the exact same thing here
[06:22:23 -> 06:22:31]  right um and so you might be wondering
[06:22:27 -> 06:22:33]  about this temp variable um so this temp
[06:22:31 -> 06:22:34]  this temporary is just is just going to
[06:22:33 -> 06:22:37]  start off as nothing
[06:22:34 -> 06:22:39]  and all we're doing is each thread
[06:22:37 -> 06:22:41]  essentially each thread has its own temp
[06:22:39 -> 06:22:43]  variable right that's going to be stored
[06:22:41 -> 06:22:45]  uh in the register each thre has its own
[06:22:43 -> 06:22:49]  temp variable and it's going to
[06:22:45 -> 06:22:51]  accumulate this temp variable for uh
[06:22:49 -> 06:22:52]  it's going to accumulate a DOT product
[06:22:51 -> 06:22:54]  so what this is going to
[06:22:52 -> 06:22:57]  do is it's not actually going to
[06:22:54 -> 06:22:59]  multiply matrices together instead what
[06:22:57 -> 06:23:01]  it's going to do is it's going to just
[06:22:59 -> 06:23:03]  accumulate as it goes through tiles
[06:23:01 -> 06:23:07]  right so as it goes through it's going
[06:23:03 -> 06:23:11]  to like accumulate each value in the
[06:23:07 -> 06:23:14]  output of C as it as it goes along right
[06:23:11 -> 06:23:16]  so based on the thread it's going to say
[06:23:14 -> 06:23:19]  say one is going to do like the row of
[06:23:16 -> 06:23:21]  this one and then the column of this one
[06:23:19 -> 06:23:23]  right and so when they when they
[06:23:21 -> 06:23:24]  interact together or when they when they
[06:23:23 -> 06:23:26]  interact they're going to end up at like
[06:23:24 -> 06:23:28]  this top left part maybe and the thread
[06:23:26 -> 06:23:30]  holds the temporary value for that
[06:23:28 -> 06:23:32]  specific part um and what it's going to
[06:23:30 -> 06:23:35]  do as like as it goes down through the
[06:23:32 -> 06:23:36]  tiles it's going to accumulate this dot
[06:23:35 -> 06:23:39]  product right so you get the first you
[06:23:36 -> 06:23:41]  get the first dot product and then you
[06:23:39 -> 06:23:42]  add it to the next one from the next
[06:23:41 -> 06:23:44]  tile right because you're you're
[06:23:42 -> 06:23:46]  essentially just doing the normal naive
[06:23:44 -> 06:23:48]  matrix multiplication but you're just
[06:23:46 -> 06:23:52]  accumulating through the
[06:23:48 -> 06:23:55]  tiles and so in the end uh you end up
[06:23:52 -> 06:23:59]  with just this this accumulated temp
[06:23:55 -> 06:24:01]  however this is only for a single dot
[06:23:59 -> 06:24:03]  product operation this is only for one
[06:24:01 -> 06:24:05]  tile and the reason why we have this for
[06:24:03 -> 06:24:08]  Loop inside of this one we're being very
[06:24:05 -> 06:24:10]  clever about this is so that we can
[06:24:08 -> 06:24:12]  actually do this accumulation through
[06:24:10 -> 06:24:14]  the tiles right so that way we can kind
[06:24:12 -> 06:24:16]  of be we could just be clever about how
[06:24:14 -> 06:24:18]  we uh go about doing that uh and then
[06:24:16 -> 06:24:21]  after we're finished we can just go
[06:24:18 -> 06:24:22]  ahead and uh you know sync up all the
[06:24:21 -> 06:24:24]  threads make sure that they're all
[06:24:22 -> 06:24:27]  caught up and before we actually write
[06:24:24 -> 06:24:30]  this out to C so I guess just going a
[06:24:27 -> 06:24:33]  little like iterating over this again
[06:24:30 -> 06:24:35]  the thread row so that's like which
[06:24:33 -> 06:24:36]  which row within the tile we want times
[06:24:35 -> 06:24:39]  the block size so that's going to be our
[06:24:36 -> 06:24:41]  wrapper and then the the idx is going to
[06:24:39 -> 06:24:44]  be how we're iterating through it so a
[06:24:41 -> 06:24:45]  is row so we're going to that's the idx
[06:24:44 -> 06:24:48]  we're going to go through this way
[06:24:45 -> 06:24:52]  that's going to be the the offset on the
[06:24:48 -> 06:24:56]  horizontal part and then B is going to
[06:24:52 -> 06:24:58]  be uh idx times block size uh and then
[06:24:56 -> 06:25:00]  plus that offset as we were doing uh
[06:24:58 -> 06:25:03]  before right so we're maintaining that
[06:25:00 -> 06:25:06]  Global memory access uh the that access
[06:25:03 -> 06:25:08]  pattern that we had before um and we're
[06:25:06 -> 06:25:11]  and we're just simply writing out that
[06:25:08 -> 06:25:13]  temporary variable as it accumulates
[06:25:11 -> 06:25:15]  through the tiles right so that that's
[06:25:13 -> 06:25:18]  the idea here is we're accumulating
[06:25:15 -> 06:25:19]  through the tiles so now we can uh
[06:25:18 -> 06:25:21]  hopefully that makes sense feel free to
[06:25:19 -> 06:25:23]  rewatch some parts of that feel free to
[06:25:21 -> 06:25:25]  plug it into something like chat GPT or
[06:25:23 -> 06:25:28]  CLA son it or something like that and
[06:25:25 -> 06:25:31]  try to try to visualize what's happening
[06:25:28 -> 06:25:33]  I have I have a separate uh additional
[06:25:31 -> 06:25:36]  like diagram here just of like what this
[06:25:33 -> 06:25:38]  looks like laid out uh I decid to add
[06:25:36 -> 06:25:41]  this to the to the the course assets
[06:25:38 -> 06:25:43]  folder inside of the faster Mill section
[06:25:41 -> 06:25:46]  so if you want to check this out I might
[06:25:43 -> 06:25:48]  add other ones to it but uh this is yeah
[06:25:46 -> 06:25:50]  this this is the uh shared memory
[06:25:48 -> 06:25:54]  blocking uh cud
[06:25:50 -> 06:25:57]  kernel so now we can actually go in and
[06:25:54 -> 06:26:01]  uh and profile this thing so I up into
[06:25:57 -> 06:26:03]  here and just go sjem number
[06:26:01 -> 06:26:05]  three we run this for a second it's
[06:26:03 -> 06:26:08]  going to be really fast and so if we
[06:26:05 -> 06:26:11]  actually compare this give it a second
[06:26:08 -> 06:26:14]  it's doing the the last one there so if
[06:26:11 -> 06:26:18]  I actually compare this to number
[06:26:14 -> 06:26:18]  two which wasn't
[06:26:19 -> 06:26:25]  uh give it a
[06:26:21 -> 06:26:27]  second yeah so our number two with just
[06:26:25 -> 06:26:30]  the Cass memory access was achieving
[06:26:27 -> 06:26:33]  about you know 1,200 and this one is
[06:26:30 -> 06:26:35]  achieving about 1,600 so we have a
[06:26:33 -> 06:26:37]  decent improvement from that right um
[06:26:35 -> 06:26:39]  but I probably should have done this
[06:26:37 -> 06:26:42]  earlier but just to like spoil just to
[06:26:39 -> 06:26:46]  spoil the surprise kublos is actually a
[06:26:42 -> 06:26:51]  lot fast we run the 0 kublos is uh about
[06:26:46 -> 06:26:52]  11,400 gig flops or 11.5 Tera flops
[06:26:51 -> 06:26:58]  which is really fast especially compared
[06:26:52 -> 06:27:01]  to our previous uh our our previous
[06:26:58 -> 06:27:03]  naive kernel right this is extremely
[06:27:01 -> 06:27:06]  fast now just to iterate before we move
[06:27:03 -> 06:27:08]  on uh this this uh shared memory kernel
[06:27:06 -> 06:27:10]  this is not implementing like the full
[06:27:08 -> 06:27:12]  version of tiling this is just
[06:27:10 -> 06:27:15]  implementing like a partial dot product
[06:27:12 -> 06:27:18]  version of tiling known as blocking so
[06:27:15 -> 06:27:20]  when we when we accumulate like dot
[06:27:18 -> 06:27:22]  products that's not actually like the
[06:27:20 -> 06:27:23]  full like the what you would Inuit it as
[06:27:22 -> 06:27:25]  tiling right tiling as I described
[06:27:23 -> 06:27:27]  before is when you like take the
[06:27:25 -> 06:27:29]  matrices you multiply them and then you
[06:27:27 -> 06:27:31]  you advance and then you multiply again
[06:27:29 -> 06:27:35]  and you like add them every time they
[06:27:31 -> 06:27:38]  multiply uh element wise and this is not
[06:27:35 -> 06:27:40]  what we did here what we did was a
[06:27:38 -> 06:27:42]  partial dot product so keep that in mind
[06:27:40 -> 06:27:44]  the next one is going to be a little bit
[06:27:42 -> 06:27:49]  different though
[06:27:44 -> 06:27:52]  so 1D uh 1D block tiling is a bit more
[06:27:49 -> 06:27:54]  advanced um but we'll get through it so
[06:27:52 -> 06:27:56]  just to help get tiling crystal clear in
[06:27:54 -> 06:27:58]  your head I'm going to use two matrices
[06:27:56 -> 06:28:00]  as an example just to show how this
[06:27:58 -> 06:28:02]  intuition works and I've actually
[06:28:00 -> 06:28:04]  written out I did a little I was testing
[06:28:02 -> 06:28:05]  a little bit and I I did the math Wrong
[06:28:04 -> 06:28:09]  by hand so we're just going to use use
[06:28:05 -> 06:28:13]  the computer for that um but I wrote out
[06:28:09 -> 06:28:17]  a matrix a here so 1 2 3 4 5 6 7 8 9 10
[06:28:13 -> 06:28:19]  11 12 13 14 15 16 and then this one B it
[06:28:17 -> 06:28:23]  counts by twos but it goes backwards so
[06:28:19 -> 06:28:25]  2 4 6 8 10 12 14 16 and then and then
[06:28:23 -> 06:28:28]  continuous up to 32 so I've written
[06:28:25 -> 06:28:29]  these out here in the terminal um and if
[06:28:28 -> 06:28:32]  we just do
[06:28:29 -> 06:28:35]  a multiply
[06:28:32 -> 06:28:36]  B we notice that we get this output
[06:28:35 -> 06:28:37]  result and these are pretty I mean these
[06:28:36 -> 06:28:40]  are pretty easy numbers to work with
[06:28:37 -> 06:28:42]  right so what I want to do is work
[06:28:40 -> 06:28:46]  specifically with this upper right tile
[06:28:42 -> 06:28:48]  the 200 180 456 404 that's exactly what
[06:28:46 -> 06:28:51]  I want to work with so how we're going
[06:28:48 -> 06:28:53]  to do this is we're going to use the
[06:28:51 -> 06:28:55]  we're going to use an idea from
[06:28:53 -> 06:28:58]  here
[06:28:55 -> 06:29:02]  um to get this top right piece we
[06:28:58 -> 06:29:03]  essentially want to uh cross inward
[06:29:02 -> 06:29:08]  right so we're going to start off with
[06:29:03 -> 06:29:09]  multiply a this this a portion with uh
[06:29:08 -> 06:29:12]  with this B portion we're going to
[06:29:09 -> 06:29:16]  multiply those together a * not B * a a
[06:29:12 -> 06:29:18]  * B and then we're going to add that
[06:29:16 -> 06:29:22]  with the product the the matrix model
[06:29:18 -> 06:29:25]  product of this piece and this piece
[06:29:22 -> 06:29:28]  right so if we start out first by doing
[06:29:25 -> 06:29:31]  1 two 5 six and
[06:29:28 -> 06:29:34]  go say um
[06:29:31 -> 06:29:37]  a uh temp
[06:29:34 -> 06:29:37]  torch.
[06:29:38 -> 06:29:43]  tensor and then inside of here we're
[06:29:40 -> 06:29:47]  going to have a smaller ones so it's
[06:29:43 -> 06:29:47]  first going to be uh 1
[06:29:50 -> 06:29:56]  1256 uh and then we multiply that with
[06:29:53 -> 06:29:56]  torch.
[06:30:00 -> 06:30:05]  tensor tor. tenser 4 to 1210
[06:30:09 -> 06:30:15]  right and if we print out a temp we get
[06:30:13 -> 06:30:20]  this
[06:30:15 -> 06:30:20]  result now we do uh B
[06:30:22 -> 06:30:28]  temp tch. tensor and I'm just going to
[06:30:26 -> 06:30:30]  print the uh actually I'll just I'll
[06:30:28 -> 06:30:31]  just print the layout here we'll do a
[06:30:30 -> 06:30:33]  we'll do the same idea for a temp but
[06:30:31 -> 06:30:34]  I'm just going to remove these values
[06:30:33 -> 06:30:37]  and we'll place this
[06:30:34 -> 06:30:40]  with uh with B temp so this is going to
[06:30:37 -> 06:30:40]  be uh
[06:30:46 -> 06:30:52]  3478 multiply that with uh 2018 28
[06:31:02 -> 06:31:10]  26 20 18 28 26 all right okay awesome
[06:31:08 -> 06:31:12]  now we print out B
[06:31:10 -> 06:31:18]  temp
[06:31:12 -> 06:31:20]  oh and then we do a temp plus b
[06:31:18 -> 06:31:24]  temp and we get the result that we were
[06:31:20 -> 06:31:30]  expecting 200 180 456
[06:31:24 -> 06:31:31]  404 um 2 180 45644 and that's a TOD M
[06:31:30 -> 06:31:32]  that that's really all there is to it it
[06:31:31 -> 06:31:34]  just helps when you're able to draw this
[06:31:32 -> 06:31:36]  out by hand and understand what the
[06:31:34 -> 06:31:38]  purpose is when we like advance for
[06:31:36 -> 06:31:41]  example when we advance pointers by like
[06:31:38 -> 06:31:42]  a a greater offset than like one or two
[06:31:41 -> 06:31:44]  or three or four it's like when you're
[06:31:42 -> 06:31:46]  skipping entire rows and you're going to
[06:31:44 -> 06:31:48]  like a c when we when we when we do that
[06:31:46 -> 06:31:50]  stuff this will help you understand why
[06:31:48 -> 06:31:54]  we're doing it
[06:31:50 -> 06:31:55]  um but uh yeah we can go ahead and begin
[06:31:54 -> 06:31:57]  awesome so now let's go ahead and look
[06:31:55 -> 06:32:02]  at the uh boiler plate code for this as
[06:31:57 -> 06:32:04]  well as the runner script uh for this so
[06:32:02 -> 06:32:06]  in the runner script we'll actually pop
[06:32:04 -> 06:32:08]  down and you can see each uh each little
[06:32:06 -> 06:32:10]  like function and how we call everything
[06:32:08 -> 06:32:11]  in here so like the kuas function for
[06:32:10 -> 06:32:15]  example there's like different ones for
[06:32:11 -> 06:32:17]  like fp32 brain float 16 tensor float 32
[06:32:15 -> 06:32:20]  right uh and then we have like the naive
[06:32:17 -> 06:32:21]  the colest the shared me caching which
[06:32:20 -> 06:32:24]  we just recently did and then we have
[06:32:21 -> 06:32:26]  the 1D block tiling right so notice how
[06:32:24 -> 06:32:29]  in the shared memory block we just have
[06:32:26 -> 06:32:31]  32s everywhere we have 32 32 32 32 and
[06:32:29 -> 06:32:33]  32 right everything's 32 because
[06:32:31 -> 06:32:36]  everything is a square but in this one
[06:32:33 -> 06:32:38]  we we actually mix it up a little bit so
[06:32:36 -> 06:32:42]  we we have these essentially this this
[06:32:38 -> 06:32:46]  block uh so M it's going to be this this
[06:32:42 -> 06:32:49]  block on uh on a so it's going to
[06:32:46 -> 06:32:52]  because it's specific to uh A and C
[06:32:49 -> 06:32:54]  because when we do our our uh like our
[06:32:52 -> 06:32:56]  matrix multiplication shapes we're going
[06:32:54 -> 06:32:59]  to cancel out the inner ones uh K and
[06:32:56 -> 06:33:01]  then you're going to be left with um
[06:32:59 -> 06:33:04]  we're going to be left with M and N
[06:33:01 -> 06:33:05]  right so
[06:33:04 -> 06:33:08]  just pay attention to these shapes here
[06:33:05 -> 06:33:10]  uh we also have the eights down below so
[06:33:08 -> 06:33:13]  uh just just like be aware of this we
[06:33:10 -> 06:33:15]  use we use different uh shapes here to
[06:33:13 -> 06:33:18]  help speed up operations because we
[06:33:15 -> 06:33:21]  introduce A New Concept um and that
[06:33:18 -> 06:33:24]  concept is 1D block tiling for
[06:33:21 -> 06:33:28]  calculating multiple results per thread
[06:33:24 -> 06:33:31]  so if we actually go back into uh the
[06:33:28 -> 06:33:34]  shared me uh blocking uh when we write
[06:33:31 -> 06:33:36]  the output we have the specific spe ific
[06:33:34 -> 06:33:38]  index per thread that we write out it's
[06:33:36 -> 06:33:41]  just one we write out one output of that
[06:33:38 -> 06:33:43]  block or or sorry of that tile and
[06:33:41 -> 06:33:45]  that's it uh we just leave it at that
[06:33:43 -> 06:33:48]  there's no iterations we're not we're
[06:33:45 -> 06:33:50]  not wunning multiple per thread just one
[06:33:48 -> 06:33:52]  now if we go to here we notice that
[06:33:50 -> 06:33:56]  we're actually writing multiple so we go
[06:33:52 -> 06:33:59]  over this res or this result idx um
[06:33:56 -> 06:34:01]  that's essentially goes uh it iterates
[06:33:59 -> 06:34:03]  through uh
[06:34:01 -> 06:34:07]  TM which is this term that we found in
[06:34:03 -> 06:34:10]  in here which is uh essentially threads
[06:34:07 -> 06:34:13]  per M Dimension you could think of it
[06:34:10 -> 06:34:16]  that way so in short we're going to be
[06:34:13 -> 06:34:17]  writing out multiple results per thread
[06:34:16 -> 06:34:19]  and that's going to speed up um
[06:34:17 -> 06:34:21]  everything a lot so like imagine if you
[06:34:19 -> 06:34:23]  had to do uh you had to issue a new
[06:34:21 -> 06:34:25]  thread every time you're going to write
[06:34:23 -> 06:34:26]  an output of this entire Matrix so if
[06:34:25 -> 06:34:28]  you have a
[06:34:26 -> 06:34:31]  496 uh by
[06:34:28 -> 06:34:34]  496 that's going to be about 16
[06:34:31 -> 06:34:36]  different threads that are writing out
[06:34:34 -> 06:34:37]  right that that is a lot of threads you
[06:34:36 -> 06:34:41]  have
[06:34:37 -> 06:34:42]  to there's a lot going on there so uh
[06:34:41 -> 06:34:45]  when
[06:34:42 -> 06:34:48]  we um when when we use when we iterate
[06:34:45 -> 06:34:50]  over we can get one thread calculating
[06:34:48 -> 06:34:51]  uh multiple and make things more
[06:34:50 -> 06:34:54]  efficient it does it does make the
[06:34:51 -> 06:34:55]  indexing more complex and this is
[06:34:54 -> 06:34:57]  probably one of the most intuitively
[06:34:55 -> 06:35:00]  difficult kernels to understand but once
[06:34:57 -> 06:35:03]  we get there uh it should be a breeze so
[06:35:00 -> 06:35:07]  going up for our boiler play code we
[06:35:03 -> 06:35:10]  have this c r is block ID x.y so we're
[06:35:07 -> 06:35:14]  going to use blocks uh each individual
[06:35:10 -> 06:35:17]  uh block of threads is going to
[06:35:14 -> 06:35:19]  calculate uh a specific tile on the
[06:35:17 -> 06:35:23]  output right so we have this block
[06:35:19 -> 06:35:25]  essentially the current the current row
[06:35:23 -> 06:35:28]  and the the current column so current
[06:35:25 -> 06:35:30]  row is like it's like a like vertical
[06:35:28 -> 06:35:32]  which row are we selecting and that's y
[06:35:30 -> 06:35:34]  right so that's vertical and then the
[06:35:32 -> 06:35:38]  column like we doing before it's you
[06:35:34 -> 06:35:42]  know horizontal Dimension um now we go
[06:35:38 -> 06:35:45]  to here which is essentially the the
[06:35:42 -> 06:35:48]  thread uh the the lower level thread
[06:35:45 -> 06:35:51]  column within that so we have this we
[06:35:48 -> 06:35:54]  have this BN term that's used and this
[06:35:51 -> 06:35:57]  BN term we remember back to here was 64
[06:35:54 -> 06:36:01]  right so let me pop back to
[06:35:57 -> 06:36:04]  this this BN term we see it in both the
[06:36:01 -> 06:36:07]  B Matrix and and the C Matrix right this
[06:36:04 -> 06:36:10]  is BN right it's that length right there
[06:36:07 -> 06:36:13]  so when we actually
[06:36:10 -> 06:36:16]  um when we do mod BN what that's going
[06:36:13 -> 06:36:19]  to do is it's like if if BN is 64 it's
[06:36:16 -> 06:36:21]  going to be like um thread idx like zero
[06:36:19 -> 06:36:22]  divide by that it's just going to be
[06:36:21 -> 06:36:24]  zero right and then it's going to go 1 2
[06:36:22 -> 06:36:27]  3 4 5 because we just have this
[06:36:24 -> 06:36:29]  remainder that's like not 64 right so
[06:36:27 -> 06:36:31]  once we pass 64 then it then it's going
[06:36:29 -> 06:36:32]  to Loop so it's going to it's going to
[06:36:31 -> 06:36:36]  iterate through all the columns and it's
[06:36:32 -> 06:36:40]  going to go up 0 to 63 and then once we
[06:36:36 -> 06:36:42]  actually hit 64 it's going to um it's
[06:36:40 -> 06:36:44]  going to it's going to divide right so
[06:36:42 -> 06:36:46]  it's going to floor up it's going to
[06:36:44 -> 06:36:48]  floor to zero for each of these indices
[06:36:46 -> 06:36:50]  leading up to 64 and then once we
[06:36:48 -> 06:36:52]  actually hit it it's going to divide and
[06:36:50 -> 06:36:54]  it's it's going to go to one right
[06:36:52 -> 06:36:55]  because it's going to floor down to one
[06:36:54 -> 06:36:57]  and we're going to be it's going to be
[06:36:55 -> 06:36:59]  like a bunch of zeros and then a one
[06:36:57 -> 06:37:03]  right and we use this to pick out our
[06:36:59 -> 06:37:04]  rows so the columns it's like 0 1 2 3 4
[06:37:03 -> 06:37:07]  5 all the way up up to 64 and then the
[06:37:04 -> 06:37:09]  rows it's like every time we stride this
[06:37:07 -> 06:37:11]  many it's going to it's going to bump up
[06:37:09 -> 06:37:12]  one so the division is going to go up to
[06:37:11 -> 06:37:15]  like one and then two and then three
[06:37:12 -> 06:37:17]  every time we stride that 64 length and
[06:37:15 -> 06:37:19]  then it's going to increase the row
[06:37:17 -> 06:37:21]  index so it's going to it's essentially
[06:37:19 -> 06:37:24]  going to be like like I said a row index
[06:37:21 -> 06:37:27]  it's going to move us downward right in
[06:37:24 -> 06:37:30]  C uh and this is this this is kind of
[06:37:27 -> 06:37:33]  why we we have this here so moving
[06:37:30 -> 06:37:35]  further down we have the a shared and
[06:37:33 -> 06:37:37]  the B shared so just the the normal
[06:37:35 -> 06:37:41]  shared memory that we allocate um so
[06:37:37 -> 06:37:42]  this going to be M by K and then K by n
[06:37:41 -> 06:37:47]  right that's that's the space we're
[06:37:42 -> 06:37:49]  storing and then uh in this specific uh
[06:37:47 -> 06:37:55]  in this specific
[06:37:49 -> 06:37:58]  thread we advance the we we advance the
[06:37:55 -> 06:38:01]  block tile to the beginning of A's row
[06:37:58 -> 06:38:03]  and B's column right so we did the same
[06:38:01 -> 06:38:06]  thing in our past one where we had
[06:38:03 -> 06:38:07]  Advanced everything uh forward right so
[06:38:06 -> 06:38:10]  this is
[06:38:07 -> 06:38:13]  literally uh this is literally the same
[06:38:10 -> 06:38:15]  idea right so we're we're doing the
[06:38:13 -> 06:38:16]  exact same thing there we're just using
[06:38:15 -> 06:38:19]  a little bit different terms because
[06:38:16 -> 06:38:21]  it's now rectangular tiles um these
[06:38:19 -> 06:38:23]  assertions here are pretty much in place
[06:38:21 -> 06:38:26]  to say uh we don't want to go out of the
[06:38:23 -> 06:38:27]  block dimx range right it's just like a
[06:38:26 -> 06:38:29]  essentially like a you could think of it
[06:38:27 -> 06:38:32]  as a boundary Checker so when we're when
[06:38:29 -> 06:38:34]  we iterate through BM or BK we don't
[06:38:32 -> 06:38:35]  want to go out of range we want to make
[06:38:34 -> 06:38:38]  sure that these kind of add up we want
[06:38:35 -> 06:38:40]  to make sure that um when we have this
[06:38:38 -> 06:38:42]  like 2D structure that when we flatten
[06:38:40 -> 06:38:44]  it out it stretches the length of block
[06:38:42 -> 06:38:48]  dim dox and that that's pretty much
[06:38:44 -> 06:38:51]  what's happening there um we we assert
[06:38:48 -> 06:38:55]  both of these so uh essentially when we
[06:38:51 -> 06:38:57]  go back it's like um you know n and K
[06:38:55 -> 06:39:00]  are the same and then M and K are also
[06:38:57 -> 06:39:02]  the same so that kind of lines up there
[06:39:00 -> 06:39:05]  and then same and then for these ones
[06:39:02 -> 06:39:08]  the same IDE as what we were doing here
[06:39:05 -> 06:39:11]  so the thread column so we do this thisx
[06:39:08 -> 06:39:14]  divided by the uh the X Dimension there
[06:39:11 -> 06:39:16]  which is BN that's the that's that's the
[06:39:14 -> 06:39:20]  trailing dimension in C it's going to be
[06:39:16 -> 06:39:23]  M by n and so in here we just do thread
[06:39:20 -> 06:39:25]  idx divided or for the for the column
[06:39:23 -> 06:39:28]  index of a the inner column index of k
[06:39:25 -> 06:39:32]  or a sorry uh we're going to do thread
[06:39:28 -> 06:39:35]  idx mod BK so BK is that that horizontal
[06:39:32 -> 06:39:37]  dimension in a right cuz it's M by
[06:39:35 -> 06:39:39]  K and
[06:39:37 -> 06:39:43]  then the inner row in
[06:39:39 -> 06:39:45]  a is going to be just the the division
[06:39:43 -> 06:39:47]  of that so whenever we stride the length
[06:39:45 -> 06:39:48]  of K it's going to notch up one and it's
[06:39:47 -> 06:39:50]  going to tell us which row index we're
[06:39:48 -> 06:39:52]  at right that's that's kind of how we
[06:39:50 -> 06:39:53]  that's how we use the threads to decide
[06:39:52 -> 06:39:56]  which index we're at and then same idea
[06:39:53 -> 06:39:58]  for uh same idea for B here except we
[06:39:56 -> 06:40:03]  use the trailing dimension in B which is
[06:39:58 -> 06:40:04]  n uh instead of K in the a matrix right
[06:40:03 -> 06:40:07]  and and then in in
[06:40:04 -> 06:40:09]  uh we essentially here we we allocate
[06:40:07 -> 06:40:11]  memory this is going to be very
[06:40:09 -> 06:40:14]  important for when we write things out
[06:40:11 -> 06:40:17]  later on so notice how we iterate over
[06:40:14 -> 06:40:20]  this term TM we're going to make thread
[06:40:17 -> 06:40:23]  results like a like an actual uh thread
[06:40:20 -> 06:40:25]  local cache that has the size TM TM is
[06:40:23 -> 06:40:27]  very small right so in here TM is TM is
[06:40:25 -> 06:40:30]  actually eight so that can that can
[06:40:27 -> 06:40:32]  easily fit in registers um and then we
[06:40:30 -> 06:40:33]  just initialize this with a number just
[06:40:32 -> 06:40:35]  say zero and then we're going to
[06:40:33 -> 06:40:36]  populate that later on right so we just
[06:40:35 -> 06:40:40]  initialize this beforehand and then
[06:40:36 -> 06:40:42]  we're going to change it later um now we
[06:40:40 -> 06:40:46]  actually jump into a little bit more
[06:40:42 -> 06:40:48]  advanced stuff so this entire sorry this
[06:40:46 -> 06:40:51]  entire Loop here is where a lot of the
[06:40:48 -> 06:40:53]  magic actually happens so when we're
[06:40:51 -> 06:40:57]  when we're in a single uh when we're in
[06:40:53 -> 06:40:58]  a single um when we're in one when we're
[06:40:57 -> 06:41:02]  in one of these
[06:40:58 -> 06:41:06]  iterations we are trying to calculate
[06:41:02 -> 06:41:09]  the uh compl complete tile for uh an
[06:41:06 -> 06:41:11]  output in uh C right within a block
[06:41:09 -> 06:41:14]  that's what we're trying to do a a block
[06:41:11 -> 06:41:17]  a block in like a certain block idx
[06:41:14 -> 06:41:20]  within the grid is going to calculate uh
[06:41:17 -> 06:41:23]  an output tile in C that's the goal here
[06:41:20 -> 06:41:26]  so we outer we we Loop over these block
[06:41:23 -> 06:41:31]  tiles by iterating over K right K is
[06:41:26 -> 06:41:34]  that um K is the uh horizontal dimension
[06:41:31 -> 06:41:37]  in a and the vertical dimension in B
[06:41:34 -> 06:41:39]  right so we we iterate over those and we
[06:41:37 -> 06:41:42]  advance by this much each time we're not
[06:41:39 -> 06:41:44]  actually going to use uh we're not
[06:41:42 -> 06:41:46]  actually going to use block idx like as
[06:41:44 -> 06:41:48]  you can see we only actually have it in
[06:41:46 -> 06:41:50]  this one line here it doesn't show up
[06:41:48 -> 06:41:52]  anywhere else so this is just for making
[06:41:50 -> 06:41:54]  sure uh that we don't if we just like
[06:41:52 -> 06:41:55]  iterate by one each time then it's going
[06:41:54 -> 06:41:58]  to go out of range and we're going to do
[06:41:55 -> 06:42:01]  this Loop way more than we need to so we
[06:41:58 -> 06:42:03]  just want to do it for as many blocks or
[06:42:01 -> 06:42:07]  for as many blocks uh as we need
[06:42:03 -> 06:42:11]  right we then populate the uh shared
[06:42:07 -> 06:42:14]  memory caches so this is within uh this
[06:42:11 -> 06:42:17]  is within a single uh tile right so
[06:42:14 -> 06:42:19]  notice how we use the inner row a times
[06:42:17 -> 06:42:24]  K right so it's the that's the that's
[06:42:19 -> 06:42:27]  the which row are we at inside of it um
[06:42:24 -> 06:42:30]  and this is these are like um these are
[06:42:27 -> 06:42:31]  like very small ranges of indices right
[06:42:30 -> 06:42:33]  and that's going to in a it's going to
[06:42:31 -> 06:42:35]  loop around K and then that EXT column
[06:42:33 -> 06:42:37]  index is going to tell us which position
[06:42:35 -> 06:42:39]  we're at right relative to uh the thread
[06:42:37 -> 06:42:41]  of course so we can like parallelize the
[06:42:39 -> 06:42:44]  ACT actual loading
[06:42:41 -> 06:42:45]  part and then we do the same thing for B
[06:42:44 -> 06:42:48]  all right so we have this they have this
[06:42:45 -> 06:42:52]  row um and we're we're we have we have
[06:42:48 -> 06:42:54]  like which row are we at and then uh we
[06:42:52 -> 06:42:56]  want to essentially stride that number
[06:42:54 -> 06:42:59]  based on n and then end up with that
[06:42:56 -> 06:43:02]  offset column index um so that's what
[06:42:59 -> 06:43:04]  this is Here We sync everything up so
[06:43:02 -> 06:43:07]  this is again at the at the Block Level
[06:43:04 -> 06:43:09]  so it so a kernel normally runs at the
[06:43:07 -> 06:43:11]  level of threads but because we're doing
[06:43:09 -> 06:43:13]  sync threads it's going to apply to all
[06:43:11 -> 06:43:15]  of them so all of the threads uh within
[06:43:13 -> 06:43:16]  this within this block are actually
[06:43:15 -> 06:43:17]  going to line up they're all going we're
[06:43:16 -> 06:43:19]  going to put a barrier and they're all
[06:43:17 -> 06:43:22]  going to meet up and they're going to
[06:43:19 -> 06:43:24]  synchronize at the same same spot right
[06:43:22 -> 06:43:27]  um and then we just Advance the block
[06:43:24 -> 06:43:29]  Tile For The Next Step so when we uh
[06:43:27 -> 06:43:31]  when we need to do this load again this
[06:43:29 -> 06:43:32]  is already ready and we don't want to we
[06:43:31 -> 06:43:33]  don't want to like worry about this
[06:43:32 -> 06:43:35]  anymore right
[06:43:33 -> 06:43:38]  okay so remember A and B are just
[06:43:35 -> 06:43:41]  pointers right when we scroll up we see
[06:43:38 -> 06:43:44]  A and B are Pointers to uh float arrays
[06:43:41 -> 06:43:46]  right so when these are laid out in
[06:43:44 -> 06:43:49]  memory uh they're they're not like it's
[06:43:46 -> 06:43:50]  not like an array of arrays um or like
[06:43:49 -> 06:43:52]  an an array of pointers where each
[06:43:50 -> 06:43:54]  pointer inside that array is a new array
[06:43:52 -> 06:43:56]  like we did in the in the C and C++
[06:43:54 -> 06:43:58]  review chapter it's not like that it's
[06:43:56 -> 06:44:01]  it's literally just a it's literally
[06:43:58 -> 06:44:04]  just a pointer and it's the pointer is
[06:44:01 -> 06:44:05]  at the start of that thing that's start
[06:44:04 -> 06:44:07]  of the array that's laid out in memory
[06:44:05 -> 06:44:08]  so it's not the actual value it's just
[06:44:07 -> 06:44:10]  the memory address so if we take that
[06:44:08 -> 06:44:12]  memory address and we we plus one it'll
[06:44:10 -> 06:44:15]  go to the next index next one next one
[06:44:12 -> 06:44:17]  next one right um that's what we're
[06:44:15 -> 06:44:20]  doing here so we already did this in the
[06:44:17 -> 06:44:23]  last we already did this here um where
[06:44:20 -> 06:44:27]  we or is it this part we just we just
[06:44:23 -> 06:44:30]  Advance further um so a advances um
[06:44:27 -> 06:44:33]  essentially plus an an entire block size
[06:44:30 -> 06:44:37]  so we advance the we we just Advance
[06:44:33 -> 06:44:38]  Plus uh whatever this value is to a so
[06:44:37 -> 06:44:40]  it's going
[06:44:38 -> 06:44:42]  to it's just going to increase that much
[06:44:40 -> 06:44:44]  whatever we set that to and then this is
[06:44:42 -> 06:44:46]  going to increase but it's going to have
[06:44:44 -> 06:44:50]  that n stride right so the the trailing
[06:44:46 -> 06:44:52]  dimension in B is n so it's like K byn
[06:44:50 -> 06:44:54]  and so it's going to it's going to wrap
[06:44:52 -> 06:44:57]  right it's going to wrap and it's just
[06:44:54 -> 06:45:00]  going to find the sort of the the next
[06:44:57 -> 06:45:01]  the next one in uh in in B right and
[06:45:00 -> 06:45:03]  that's that that's really all we're
[06:45:01 -> 06:45:06]  doing there
[06:45:03 -> 06:45:08]  so then we go to this next part uh this
[06:45:06 -> 06:45:10]  is where actually a lot of magic happens
[06:45:08 -> 06:45:11]  and I'll do my best to explain this but
[06:45:10 -> 06:45:15]  this part's like kind of intuitively
[06:45:11 -> 06:45:18]  hard so we have multiple Forbes in here
[06:45:15 -> 06:45:21]  we have this idx that we iterate over BK
[06:45:18 -> 06:45:25]  with uh we have this float we have this
[06:45:21 -> 06:45:28]  this specific float um this this this
[06:45:25 -> 06:45:30]  sorry this this temp temporary variable
[06:45:28 -> 06:45:31]  and then we have an inner loop here so
[06:45:30 -> 06:45:33]  I'll try to explain this as best as
[06:45:31 -> 06:45:36]  possible um
[06:45:33 -> 06:45:38]  we jump back to here we notice how uh
[06:45:36 -> 06:45:40]  like initially we have these two
[06:45:38 -> 06:45:43]  matrices that we're trying to multiply
[06:45:40 -> 06:45:44]  together so this is uh this is a and
[06:45:43 -> 06:45:46]  this is B right and we have this this
[06:45:44 -> 06:45:50]  tile intersection as we did on the
[06:45:46 -> 06:45:52]  Whiteboard there um and so the really
[06:45:50 -> 06:45:55]  the the magic happens is is like in
[06:45:52 -> 06:45:56]  these Loops right we've already taken um
[06:45:55 -> 06:45:58]  in this outer one we've already actually
[06:45:56 -> 06:45:59]  taken the block tiles we already have
[06:45:58 -> 06:46:03]  these in shared memory and now we have
[06:45:59 -> 06:46:04]  to do fast operations with them so we go
[06:46:03 -> 06:46:06]  down to here where we actually have
[06:46:04 -> 06:46:09]  these in shared memory right this is a
[06:46:06 -> 06:46:11]  tall and and not very wide this is uh
[06:46:09 -> 06:46:13]  wide and a little bit tall right so it's
[06:46:11 -> 06:46:18]  it kind of matches
[06:46:13 -> 06:46:23]  up and then inside of here what we do is
[06:46:18 -> 06:46:26]  we notice how we do we iterate over idx
[06:46:23 -> 06:46:28]  and then in here we iterate uh through
[06:46:26 -> 06:46:31]  through this this this TM right so we
[06:46:28 -> 06:46:35]  have this idx and then res idx uh over
[06:46:31 -> 06:46:38]  TM so if we actually pop back to here um
[06:46:35 -> 06:46:40]  essentially what's happening is this at
[06:46:38 -> 06:46:44]  the lowest level in the innermost Loop
[06:46:40 -> 06:46:46]  res idx is going through uh like we we
[06:46:44 -> 06:46:48]  have this jump here which I'll explain
[06:46:46 -> 06:46:52]  that indexing in a second like how we
[06:46:48 -> 06:46:54]  actually arve there but res idx is going
[06:46:52 -> 06:46:57]  through these it's advancing uh
[06:46:54 -> 06:46:59]  vertically downward and it's multiplying
[06:46:57 -> 06:47:01]  with whatever value this is so that
[06:46:59 -> 06:47:03]  little top left corner in B that top
[06:47:01 -> 06:47:06]  left corner value in B is going to stay
[06:47:03 -> 06:47:08]  the same and res idx is just going to
[06:47:06 -> 06:47:11]  multiply with that value it's going to
[06:47:08 -> 06:47:12]  go and it's going to compute a partial
[06:47:11 -> 06:47:16]  dotproduct along this
[06:47:12 -> 06:47:19]  column right now now when we do uh when
[06:47:16 -> 06:47:22]  we iterate over uh idx what's going to
[06:47:19 -> 06:47:24]  happen is uh idx is going to it's going
[06:47:22 -> 06:47:26]  to go forward this way and it's going to
[06:47:24 -> 06:47:29]  go down this way right so notice how
[06:47:26 -> 06:47:30]  these these arrows are are colored very
[06:47:29 -> 06:47:33]  similarly they're they're actually the
[06:47:30 -> 06:47:36]  same color so that means we're GNA
[06:47:33 -> 06:47:38]  idx is going to evolve downwards in B
[06:47:36 -> 06:47:43]  and idx is going to evolve uh to the
[06:47:38 -> 06:47:45]  right in a so whenever we evolve like
[06:47:43 -> 06:47:47]  one it's going to like essentially res
[06:47:45 -> 06:47:50]  idx is going to take this value it's
[06:47:47 -> 06:47:52]  going to go and then we're going to uh
[06:47:50 -> 06:47:54]  evolve one forward and then where IX is
[06:47:52 -> 06:47:56]  going to reset and then it's going to do
[06:47:54 -> 06:47:57]  a DOT a partial dot product for the next
[06:47:56 -> 06:47:59]  column right it's going to do a partial
[06:47:57 -> 06:48:02]  dot product uh for the next column and
[06:47:59 -> 06:48:05]  it's going to do this all the way until
[06:48:02 -> 06:48:07]  it is finished inside of inside of uh
[06:48:05 -> 06:48:09]  this this entire section and then when
[06:48:07 -> 06:48:12]  these evolve uh inwards right so when
[06:48:09 -> 06:48:14]  this is when this is going forwards um
[06:48:12 -> 06:48:16]  and like just not even considering res
[06:48:14 -> 06:48:17]  idx just like think about like sure
[06:48:16 -> 06:48:20]  these ones are all filled in this one
[06:48:17 -> 06:48:21]  too but when this goes forward when this
[06:48:20 -> 06:48:23]  goes forward and multiplies with this
[06:48:21 -> 06:48:26]  and they are both inching in one at a
[06:48:23 -> 06:48:30]  time as dot this is what's happening as
[06:48:26 -> 06:48:32]  idx is is moving up um these are
[06:48:30 -> 06:48:34]  actually acting as little um like
[06:48:32 -> 06:48:39]  essentially little tiles right and so
[06:48:34 -> 06:48:41]  you end up Computing this specific uh
[06:48:39 -> 06:48:44]  you end up Computing the full dot
[06:48:41 -> 06:48:47]  product of this so when when this one
[06:48:44 -> 06:48:50]  moves like here it's like 1/4 of it is
[06:48:47 -> 06:48:54]  done right 1/4 of it is done and then it
[06:48:50 -> 06:48:56]  moves up half is done half is done 75 75
[06:48:54 -> 06:48:59]  and then it's fully done once that
[06:48:56 -> 06:49:02]  evolves all four steps this entire Index
[06:48:59 -> 06:49:03]  right here is computed and so we notice
[06:49:02 -> 06:49:06]  that when we do res idx and we go
[06:49:03 -> 06:49:09]  through all of these we end up
[06:49:06 -> 06:49:11]  completing the entire row so when we go
[06:49:09 -> 06:49:14]  through this way and these this this
[06:49:11 -> 06:49:18]  goes forward we complete one column at a
[06:49:14 -> 06:49:24]  time right one column at a time is uh
[06:49:18 -> 06:49:27]  completed per thread and so when we um
[06:49:24 -> 06:49:30]  when we have other threads acting like
[06:49:27 -> 06:49:32]  thread column and the thread row which
[06:49:30 -> 06:49:33]  is acting as these little blocks that
[06:49:32 -> 06:49:36]  are shiting downwards instead of like
[06:49:33 -> 06:49:38]  little individual 1D columns um you
[06:49:36 -> 06:49:41]  actually end up Computing the entire
[06:49:38 -> 06:49:42]  thing so thread column is laid out this
[06:49:41 -> 06:49:45]  way through like it's like essentially
[06:49:42 -> 06:49:48]  all the column indices in in the B
[06:49:45 -> 06:49:52]  tile and then you have the thread rows
[06:49:48 -> 06:49:54]  which are here and so this is going to
[06:49:52 -> 06:49:56]  compute all of the all of the columns in
[06:49:54 -> 06:49:58]  C it's going to it's going to cover all
[06:49:56 -> 06:50:01]  of them in the tiling aspect and then
[06:49:58 -> 06:50:02]  this is also going to cover all of them
[06:50:01 -> 06:50:05]  so what you actually end up with is you
[06:50:02 -> 06:50:08]  can complete the entire tile by giving a
[06:50:05 -> 06:50:10]  thread more operations to do so when we
[06:50:08 -> 06:50:13]  actually jump into this
[06:50:10 -> 06:50:15]  um we can see so first of all we have
[06:50:13 -> 06:50:18]  this temp B right so this is coming from
[06:50:15 -> 06:50:19]  the B shared memory this is so we just
[06:50:18 -> 06:50:22]  say
[06:50:19 -> 06:50:23]  idxx is zero this is the first iteration
[06:50:22 -> 06:50:26]  it hasn't or the zero with iteration it
[06:50:23 -> 06:50:27]  hasn't changed yet so this is zero and
[06:50:26 -> 06:50:30]  it's going
[06:50:27 -> 06:50:35]  to it's going to evolve you know across
[06:50:30 -> 06:50:36]  BN zero number of times right so 0 *
[06:50:35 -> 06:50:40]  that is z and then plus the thread
[06:50:36 -> 06:50:41]  column if that is also if that is also
[06:50:40 -> 06:50:43]  zero then it's just going to be here
[06:50:41 -> 06:50:47]  right it's literally just going to be
[06:50:43 -> 06:50:49]  there um and when idx moves up then it's
[06:50:47 -> 06:50:51]  going to
[06:50:49 -> 06:50:53]  um like this isn't it's going it's not
[06:50:51 -> 06:50:55]  going to move this way that the this
[06:50:53 -> 06:50:57]  offset here is from the thread itself
[06:50:55 -> 06:50:58]  the thread index itself but we're
[06:50:57 -> 06:51:00]  actually going to Traverse downward
[06:50:58 -> 06:51:04]  right that's that's the do idx as I was
[06:51:00 -> 06:51:07]  explaining before and then uh you have
[06:51:04 -> 06:51:10]  the uh this res idx the result idx and
[06:51:07 -> 06:51:13]  this goes through TM right so this is
[06:51:10 -> 06:51:15]  this little block that we have here
[06:51:13 -> 06:51:18]  um
[06:51:15 -> 06:51:20]  and when we actually look at how this is
[06:51:18 -> 06:51:23]  accumulating remember this is the size
[06:51:20 -> 06:51:26]  of um this is the size of TM we're going
[06:51:23 -> 06:51:27]  to iterate through TM with this red idx
[06:51:26 -> 06:51:29]  or res idx that's going to be the the
[06:51:27 -> 06:51:31]  POS the the amount through TM which
[06:51:29 -> 06:51:34]  we've iterated through we're setting
[06:51:31 -> 06:51:36]  that index to uh a place in shared
[06:51:34 -> 06:51:41]  memory and this exact place is going to
[06:51:36 -> 06:51:45]  be so the thread row times uh TM plus
[06:51:41 -> 06:51:47]  res idx right so thread Row in this
[06:51:45 -> 06:51:50]  case thread Row in this case is
[06:51:47 -> 06:51:52]  whichever one of these it falls at right
[06:51:50 -> 06:51:55]  so that that's a specific row that it
[06:51:52 -> 06:51:57]  falls at considering that we evolve uh
[06:51:55 -> 06:52:00]  like in TM blocks right that's that's
[06:51:57 -> 06:52:02]  the that's the amount that we evolve we
[06:52:00 -> 06:52:07]  we progress each time and then this res
[06:52:02 -> 06:52:09]  idx part um res idx is how much are we
[06:52:07 -> 06:52:11]  vertically offset so we we advance like
[06:52:09 -> 06:52:12]  four or like eight or four or eight
[06:52:11 -> 06:52:15]  downwards and then we have this
[06:52:12 -> 06:52:17]  additional offset res idx um but we have
[06:52:15 -> 06:52:19]  to make sure that we actually arrive at
[06:52:17 -> 06:52:21]  that specific piece cuz it's it's going
[06:52:19 -> 06:52:23]  to be laid out in memory like this right
[06:52:21 -> 06:52:25]  so we have to make sure that we evolve
[06:52:23 -> 06:52:28]  straight downwards and that we get to
[06:52:25 -> 06:52:28]  that certain res ID ex
[06:52:28 -> 06:52:34]  position now we multiply this by the k
[06:52:32 -> 06:52:35]  which makes this a lot easier for us
[06:52:34 -> 06:52:37]  right that's that essentially solves
[06:52:35 -> 06:52:40]  that problem so however many we want to
[06:52:37 -> 06:52:43]  go down this this thread whichever
[06:52:40 -> 06:52:46]  thread row we're talking about times TM
[06:52:43 -> 06:52:49]  which is you know that that block
[06:52:46 -> 06:52:51]  space um plus res idx which is that
[06:52:49 -> 06:52:54]  offset and then times that all of that
[06:52:51 -> 06:52:56]  times BK right this this K Dimension
[06:52:54 -> 06:52:57]  here and that's just going to times it's
[06:52:56 -> 06:53:02]  going to
[06:52:57 -> 06:53:02]  go right where we need to be
[06:53:02 -> 06:53:06]  then we just add the idx offset which as
[06:53:05 -> 06:53:08]  I highlighted
[06:53:06 -> 06:53:10]  before is literally just going to
[06:53:08 -> 06:53:11]  progress that way so it's going to
[06:53:10 -> 06:53:14]  iterate all the way to the starting
[06:53:11 -> 06:53:16]  position uh and then idx is going to
[06:53:14 -> 06:53:19]  tell us how much has it gone to the
[06:53:16 -> 06:53:22]  right uh and so you have this do
[06:53:19 -> 06:53:24]  idx here that traverses downwards and
[06:53:22 -> 06:53:27]  here it traverses to the
[06:53:24 -> 06:53:31]  right now if we go back we're
[06:53:27 -> 06:53:33]  multiplying this um by this one we're
[06:53:31 -> 06:53:37]  multiplying it by uh the the temporary B
[06:53:33 -> 06:53:39]  value keep in mind this is the only
[06:53:37 -> 06:53:41]  thing controlling this is the idx which
[06:53:39 -> 06:53:43]  we already highlighted this is going to
[06:53:41 -> 06:53:45]  make it go downwards and then the thread
[06:53:43 -> 06:53:47]  column so thread column as I mentioned
[06:53:45 -> 06:53:48]  before again we're kind of just going
[06:53:47 -> 06:53:50]  like starting with the visual example
[06:53:48 -> 06:53:53]  and then going into the code and then
[06:53:50 -> 06:53:55]  connecting that to our visual example so
[06:53:53 -> 06:53:57]  thread column is that that horizontal
[06:53:55 -> 06:53:59]  offset and that that like each thread is
[06:53:57 -> 06:54:01]  just going to get a different horizontal
[06:53:59 -> 06:54:03]  offset right so it depends on which
[06:54:01 -> 06:54:07]  thread we're at um
[06:54:03 -> 06:54:09]  and then we go back to here um and we
[06:54:07 -> 06:54:10]  have this this thread row right so
[06:54:09 -> 06:54:12]  that's just going to depend on which
[06:54:10 -> 06:54:15]  block we're at so in this case we're not
[06:54:12 -> 06:54:17]  at block uh we're not at sorry we're not
[06:54:15 -> 06:54:20]  at thread row zero we're at thread Row
[06:54:17 -> 06:54:22]  one so it's going to Traverse TM uh
[06:54:20 -> 06:54:24]  layers down which in this case is like
[06:54:22 -> 06:54:26]  maybe four or eight or whatever whatever
[06:54:24 -> 06:54:28]  number we pick right um and then it's
[06:54:26 -> 06:54:29]  going to end up there and then the rest
[06:54:28 -> 06:54:31]  of the math is going to ensure that we
[06:54:29 -> 06:54:34]  get to the correct position with respect
[06:54:31 -> 06:54:34]  to resid and
[06:54:35 -> 06:54:40]  idx some other things you want to pay
[06:54:37 -> 06:54:43]  attention to here are how these actually
[06:54:40 -> 06:54:46]  uh are coest in memory right so in
[06:54:43 -> 06:54:48]  memory um keep in mind when we're
[06:54:46 -> 06:54:50]  loading these columns in when we're when
[06:54:48 -> 06:54:51]  we're loading these little column bits
[06:54:50 -> 06:54:54]  in in uh in
[06:54:51 -> 06:54:55]  memory we're loading them as if they're
[06:54:54 -> 06:54:58]  like adjacent next to each other right
[06:54:55 -> 06:55:00]  so this like thread zero is going to be
[06:54:58 -> 06:55:01]  adjacent to to thread thread column one
[06:55:00 -> 06:55:03]  thread column Z is adjacent to thread
[06:55:01 -> 06:55:05]  column one they're next next to each
[06:55:03 -> 06:55:08]  other thread column two three four five
[06:55:05 -> 06:55:10]  right so when we actually load this on
[06:55:08 -> 06:55:12]  the level of threads that memory access
[06:55:10 -> 06:55:14]  is going to be CEST it's going to be
[06:55:12 -> 06:55:16]  combined we're not going to have to you
[06:55:14 -> 06:55:18]  know consider this stride and then like
[06:55:16 -> 06:55:19]  oh we need to get two memory accesses to
[06:55:18 -> 06:55:22]  get both of these it's like no you can
[06:55:19 -> 06:55:27]  actually fit um like a bunch like
[06:55:22 -> 06:55:29]  however many as you need into one right
[06:55:27 -> 06:55:32]  so technically what you're going to have
[06:55:29 -> 06:55:34]  here is you're going to have um
[06:55:32 -> 06:55:38]  since BN is
[06:55:34 -> 06:55:40]  64 BN is 64 so this this whole length
[06:55:38 -> 06:55:41]  here it's going to have two warps so
[06:55:40 -> 06:55:44]  it's going to literally going to be uh
[06:55:41 -> 06:55:48]  like two memory accesses that we need to
[06:55:44 -> 06:55:50]  do because um that like an entire warp
[06:55:48 -> 06:55:52]  that can actually make memory accesses
[06:55:50 -> 06:55:54]  really efficient when we have two that's
[06:55:52 -> 06:55:56]  that's effectively just two memory
[06:55:54 -> 06:55:58]  accesses uh that we have to worry about
[06:55:56 -> 06:56:00]  so it's really awesome that we have
[06:55:58 -> 06:56:03]  these that we have these colest right I
[06:56:00 -> 06:56:05]  mean this itself this other one isn't
[06:56:03 -> 06:56:07]  going to be coess but that's fine um
[06:56:05 -> 06:56:09]  because we you know we're we're still
[06:56:07 -> 06:56:11]  using shared memory what we do care
[06:56:09 -> 06:56:14]  about though is that these are colest
[06:56:11 -> 06:56:15]  right the column accesses are colest and
[06:56:14 -> 06:56:18]  that's going to make things really
[06:56:15 -> 06:56:21]  really fast so to iterate um we we
[06:56:18 -> 06:56:23]  essentially iterate over and we complete
[06:56:21 -> 06:56:25]  we let the threads complete uh The
[06:56:23 -> 06:56:27]  Columns uh like partially and they they
[06:56:25 -> 06:56:29]  Advance over and the and the dot
[06:56:27 -> 06:56:30]  products are going or the the dot ID EXs
[06:56:29 -> 06:56:32]  are going to evolve and they're they're
[06:56:30 -> 06:56:34]  going to close inwards and then they're
[06:56:32 -> 06:56:35]  going to complete that slowly right and
[06:56:34 -> 06:56:37]  these these individual threads are going
[06:56:35 -> 06:56:38]  to complete the columns right so they're
[06:56:37 -> 06:56:42]  going to they're going to move through
[06:56:38 -> 06:56:44]  the res idx and then the do ID XS are
[06:56:42 -> 06:56:45]  going to evolve this way so you end up
[06:56:44 -> 06:56:47]  just completing a whole column because
[06:56:45 -> 06:56:50]  these line up and this one lines up and
[06:56:47 -> 06:56:53]  so you get you get a bunch of
[06:56:50 -> 06:56:56]  these and then one of these
[06:56:53 -> 06:56:57]  right so then we sync up the threads we
[06:56:56 -> 06:56:58]  make sure that they're all caught up so
[06:56:57 -> 06:57:00]  that we can actually write out the
[06:56:58 -> 06:57:02]  results safely right this is for a
[06:57:00 -> 06:57:03]  specific block tile so when we have like
[06:57:02 -> 06:57:04]  a bunch of them in the entire thing we
[06:57:03 -> 06:57:06]  have a bunch of block tiles we're
[06:57:04 -> 06:57:08]  worrying about um we want to make sure
[06:57:06 -> 06:57:09]  that we've synced them up for this
[06:57:08 -> 06:57:10]  current block tile just for safety
[06:57:09 -> 06:57:12]  purposes right we don't want to mess
[06:57:10 -> 06:57:14]  anything up so we're just going we're
[06:57:12 -> 06:57:17]  going to be safe there and then when we
[06:57:14 -> 06:57:19]  actually write out the results it's
[06:57:17 -> 06:57:22]  going to be very similar to what we
[06:57:19 -> 06:57:25]  actually did um up here right so we
[06:57:22 -> 06:57:27]  iterate through this TM term again um
[06:57:25 -> 06:57:31]  and we're going to have thread row time
[06:57:27 -> 06:57:34]  DM plus red res idx and then times n
[06:57:31 -> 06:57:38]  right so n is in the bigger picture of
[06:57:34 -> 06:57:40]  that whole that whole C Matrix um and
[06:57:38 -> 06:57:43]  then plus the actual uh thread column
[06:57:40 -> 06:57:45]  itself which is going to be that offset
[06:57:43 -> 06:57:47]  uh and that that's the actual uh index
[06:57:45 -> 06:57:51]  that we write out and we're going to
[06:57:47 -> 06:57:52]  iterate over TM or eight indices every
[06:57:51 -> 06:57:54]  single thread so each thread is going to
[06:57:52 -> 06:57:56]  write out eight elements right instead
[06:57:54 -> 06:57:59]  of just one writing out eight
[06:57:56 -> 06:58:01]  elements um and then we're we're just
[06:57:59 -> 06:58:02]  going to keep in mind this you know this
[06:58:01 -> 06:58:05]  this thread result results we're just
[06:58:02 -> 06:58:07]  essentially each time we write out it's
[06:58:05 -> 06:58:09]  just going to populate that index so
[06:58:07 -> 06:58:12]  it's like a very easy way of just
[06:58:09 -> 06:58:13]  keeping track of uh of like which ones
[06:58:12 -> 06:58:16]  we write out at the lowest level of
[06:58:13 -> 06:58:18]  Hardware like SM and registers so it's
[06:58:16 -> 06:58:20]  just going to make our jobs easier on
[06:58:18 -> 06:58:22]  that level so we could literally just
[06:58:20 -> 06:58:23]  store them one by one it's like eight an
[06:58:22 -> 06:58:25]  array of eight elements and then we
[06:58:23 -> 06:58:26]  write out that array of eight elements
[06:58:25 -> 06:58:29]  we don't have to worry about strides or
[06:58:26 -> 06:58:32]  any of that stuff right um we multiply
[06:58:29 -> 06:58:34]  Alpha by that um for each for each one
[06:58:32 -> 06:58:36]  right for each index and then we do the
[06:58:34 -> 06:58:41]  same with beta so beta is another term
[06:58:36 -> 06:58:43]  and we have C which uh I mean c is just
[06:58:41 -> 06:58:45]  we're essentially just element or we are
[06:58:43 -> 06:58:47]  we are pointwise multiplying beta so
[06:58:45 -> 06:58:49]  it's like whatever beta is maybe like
[06:58:47 -> 06:58:53]  0.5 or three or whatever it is we're
[06:58:49 -> 06:58:55]  just multiplying each each one uh and so
[06:58:53 -> 06:58:57]  we consider the the strides and the
[06:58:55 -> 06:59:00]  offset as as well for
[06:58:57 -> 06:59:03]  that okay awesome so now we can actually
[06:59:00 -> 06:59:06]  generate or we can actually see how this
[06:59:03 -> 06:59:08]  performs uh so remember how bad our
[06:59:06 -> 06:59:11]  initial kernel was I'm going to go ahead
[06:59:08 -> 06:59:13]  and run SJ 00 just to show kuas um it's
[06:59:11 -> 06:59:16]  going to iterate up and we're going to
[06:59:13 -> 06:59:18]  get about 11.4 11.5 Tera flops of
[06:59:16 -> 06:59:20]  performance and then if we go ahead and
[06:59:18 -> 06:59:24]  run the block to one so kernel number
[06:59:20 -> 06:59:24]  four go ahead and run
[06:59:24 -> 06:59:28]  this we're going to get like actually
[06:59:26 -> 06:59:29]  quite a bit faster than the previous
[06:59:28 -> 06:59:32]  which was
[06:59:29 -> 06:59:36]  03 and this one
[06:59:32 -> 06:59:36]  uh give it a
[06:59:36 -> 06:59:42]  second this one gave us about 1 1600 uh
[06:59:40 -> 06:59:44]  1600 gig flops this one is about a 3X
[06:59:42 -> 06:59:46]  speed up of what we previously had which
[06:59:44 -> 06:59:49]  is really good so that pretty much just
[06:59:46 -> 06:59:51]  shows uh that the memory access patterns
[06:59:49 -> 06:59:54]  we use so that that coling of of memory
[06:59:51 -> 06:59:57]  access was really really useful and even
[06:59:54 -> 06:59:58]  more useful was using a single thread to
[06:59:57 -> 07:00:00]  do multiple computations to compute
[06:59:58 -> 07:00:02]  eight elements instead of just one right
[07:00:00 -> 07:00:06]  so that really really SP up our
[07:00:02 -> 07:00:07]  throughput and performance there so now
[07:00:06 -> 07:00:10]  if
[07:00:07 -> 07:00:13]  um I actually wrote a like off camera I
[07:00:10 -> 07:00:17]  wrote a separate uh I wrote a separate
[07:00:13 -> 07:00:19]  function here or not function file main.
[07:00:17 -> 07:00:22]  cuu inside of the Kel's folder so so we
[07:00:19 -> 07:00:25]  could easily like compile it um what I
[07:00:22 -> 07:00:27]  pretty much did is I just uh imported
[07:00:25 -> 07:00:29]  this so the the for essentially the
[07:00:27 -> 07:00:32]  block tiling kernel the the header for
[07:00:29 -> 07:00:34]  that right here uh I included the macro
[07:00:32 -> 07:00:37]  which we had in the previous file I
[07:00:34 -> 07:00:40]  initialized the majores so 1024 1024
[07:00:37 -> 07:00:44]  1024 um and then our previous like 64 64
[07:00:40 -> 07:00:46]  8 and 8 and then I just populated these
[07:00:44 -> 07:00:48]  um could amalik manage just use like
[07:00:46 -> 07:00:49]  this this unified memory which is like
[07:00:48 -> 07:00:52]  going to just reduce a bunch of
[07:00:49 -> 07:00:53]  boilerplate and make things uh a little
[07:00:52 -> 07:00:55]  bit sped up for us but you're you're
[07:00:53 -> 07:00:57]  going to see the the main thing that
[07:00:55 -> 07:00:58]  we're looking for in a second um I
[07:00:57 -> 07:01:00]  initialize everything properly and then
[07:00:58 -> 07:01:02]  I call this kernel uh and we're just
[07:01:00 -> 07:01:05]  trying to see like what is the actual
[07:01:02 -> 07:01:08]  code look like under the hood here so if
[07:01:05 -> 07:01:12]  uh if I go up to this command that I ran
[07:01:08 -> 07:01:14]  recently nvcc D PTX that means parallel
[07:01:12 -> 07:01:17]  thread execution parallel thread
[07:01:14 -> 07:01:19]  execution is what Cuda compiles down to
[07:01:17 -> 07:01:22]  it's the Assembly Language for parallel
[07:01:19 -> 07:01:23]  processors uh and then you have just the
[07:01:22 -> 07:01:26]  file that we're compiling and then we
[07:01:23 -> 07:01:30]  output kernel. PTX
[07:01:26 -> 07:01:32]  right or I have to get out of this um
[07:01:30 -> 07:01:36]  and then go into
[07:01:32 -> 07:01:38]  SRC kernels uh and then go and run that
[07:01:36 -> 07:01:40]  this is going to give us this kernel.
[07:01:38 -> 07:01:42]  PTX file which I'll just open here we
[07:01:40 -> 07:01:49]  bring this
[07:01:42 -> 07:01:49]  up we go into uh kernel.
[07:01:49 -> 07:01:56]  PTX we notice uh there are a lot of
[07:01:53 -> 07:01:58]  lines in this there are 308
[07:01:56 -> 07:02:01]  lines
[07:01:58 -> 07:02:04]  and it doesn't tell us anything super
[07:02:01 -> 07:02:07]  specific specific right like we can see
[07:02:04 -> 07:02:12]  um like for example like an ad an ad
[07:02:07 -> 07:02:15]  operation or uh like a fuse multiply ad
[07:02:12 -> 07:02:18]  with a with a floating Point 32 number
[07:02:15 -> 07:02:19]  um it's like I think output and then
[07:02:18 -> 07:02:21]  like multiply these and then add this
[07:02:19 -> 07:02:23]  one I can't remember the exact order but
[07:02:21 -> 07:02:25]  like fuse multiply add right you can
[07:02:23 -> 07:02:26]  find all these instructions in here
[07:02:25 -> 07:02:32]  [Music]
[07:02:26 -> 07:02:32]  um multiply right um
[07:02:33 -> 07:02:38]  yeah we have the the load instruction so
[07:02:35 -> 07:02:40]  LD so it's going to load uh a floating
[07:02:38 -> 07:02:44]  Point number into uh
[07:02:40 -> 07:02:44]  shared into the into the
[07:02:45 -> 07:02:52]  SRAM and then we leave this if I can
[07:02:48 -> 07:02:54]  exit Vim uh I have a separate one that I
[07:02:52 -> 07:02:57]  also outputed this is the Shader
[07:02:54 -> 07:03:00]  assembly so initially nvcc is going to
[07:02:57 -> 07:03:01]  compile everything down to PTX and then
[07:03:00 -> 07:03:03]  PTX is going to further compile to
[07:03:01 -> 07:03:05]  Shader assembly which is then actually
[07:03:03 -> 07:03:09]  run on the GPU Shader assembly is what
[07:03:05 -> 07:03:12]  it executes so if we compile uh you know
[07:03:09 -> 07:03:14]  Cuda binary right this actual the actual
[07:03:12 -> 07:03:16]  binary that we run and then we and then
[07:03:14 -> 07:03:18]  we uh compose this back up into the
[07:03:16 -> 07:03:21]  Shader Assembly Language that uh is
[07:03:18 -> 07:03:25]  actually executing and then we output
[07:03:21 -> 07:03:25]  that in the Cu uh the Cuda binary
[07:03:26 -> 07:03:32]  format we
[07:03:28 -> 07:03:32]  can uh
[07:03:33 -> 07:03:40]  Oh wrong one let me let me check real
[07:03:37 -> 07:03:43]  quick we go yes so we've compiled it
[07:03:40 -> 07:03:44]  into this and now we we uh we look at it
[07:03:43 -> 07:03:47]  again through the special command so
[07:03:44 -> 07:03:50]  Cuda object dump uh dump Shader assembly
[07:03:47 -> 07:03:53]  and then just open essentially open that
[07:03:50 -> 07:03:56]  and it's going to give us the exact uh
[07:03:53 -> 07:03:58]  assembly code or what we just what we
[07:03:56 -> 07:04:00]  just uh compiled right that entire
[07:03:58 -> 07:04:03]  script
[07:04:00 -> 07:04:05]  so if we look look very carefully for
[07:04:03 -> 07:04:09]  like load instructions right so if we
[07:04:05 -> 07:04:12]  look for LD
[07:04:09 -> 07:04:15]  um which I know is from the it's from
[07:04:12 -> 07:04:19]  here so PTX compiled to Shader assembly
[07:04:15 -> 07:04:21]  the S shared memory loads from from B
[07:04:19 -> 07:04:24]  shared are vectorized right B shared was
[07:04:21 -> 07:04:26]  the one where we had uh these individual
[07:04:24 -> 07:04:30]  uh columns right so when that was in
[07:04:26 -> 07:04:31]  shared memory the smm loads from shared
[07:04:30 -> 07:04:33]  are vectorized remember when we had the
[07:04:31 -> 07:04:35]  threads that were adjacent to each other
[07:04:33 -> 07:04:38]  those are what we're looking for so if
[07:04:35 -> 07:04:38]  we look for
[07:04:41 -> 07:04:50]  LDS so we have
[07:04:44 -> 07:04:55]  LDS um U 128 right LDS u32 right we have
[07:04:50 -> 07:04:57]  all these ldss here um and this is when
[07:04:55 -> 07:05:00]  they're not coest so this this LDS when
[07:04:57 -> 07:05:03]  we have the 32 that means it's it's it's
[07:05:00 -> 07:05:06]  not uh it's not cess together right
[07:05:03 -> 07:05:08]  these these um when it's when it's not
[07:05:06 -> 07:05:10]  accessing like four in a row so if you
[07:05:08 -> 07:05:14]  do a a 32-bit floating Point number 32
[07:05:10 -> 07:05:16]  bits * 4 is 128 so when we're accessing
[07:05:14 -> 07:05:19]  four in a row it's going to be 128bit
[07:05:16 -> 07:05:21]  load in uh Shader assembly so this is
[07:05:19 -> 07:05:24]  what it actually looks like when we get
[07:05:21 -> 07:05:25]  those cols memory accesses going and
[07:05:24 -> 07:05:27]  this is what it looks like when we're
[07:05:25 -> 07:05:29]  not right so we end up having to uh you
[07:05:27 -> 07:05:32]  know maybe load more so we have like two
[07:05:29 -> 07:05:34]  loads here uh but anyways that this this
[07:05:32 -> 07:05:37]  is the entire Point here just to show
[07:05:34 -> 07:05:39]  you what the actual loads look like in
[07:05:37 -> 07:05:41]  binary format that this this is what
[07:05:39 -> 07:05:44]  they look like uh and we're going to
[07:05:41 -> 07:05:47]  further optimize uh these kernels to
[07:05:44 -> 07:05:50]  perform better right fe uh I think this
[07:05:47 -> 07:05:52]  is f ffma is like fuse multiply ad
[07:05:50 -> 07:05:55]  floating Point fuse multiply ad I can't
[07:05:52 -> 07:06:02]  remember we can actually search that up
[07:05:55 -> 07:06:02]  uh to uh F FMA in uh Shader assembly
[07:06:09 -> 07:06:15]  so yeah fused fused multiply ad there's
[07:06:14 -> 07:06:17]  probably a manual somewhere which I'm
[07:06:15 -> 07:06:19]  not going to look through right now you
[07:06:17 -> 07:06:21]  can do that but uh these are the Shader
[07:06:19 -> 07:06:23]  assembly instructions that we're working
[07:06:21 -> 07:06:27]  with
[07:06:23 -> 07:06:29]  um Now we move on to 2D block tiling
[07:06:27 -> 07:06:31]  which Builds on what we've already done
[07:06:29 -> 07:06:34]  but makes it even more efficient and
[07:06:31 -> 07:06:36]  makes it even more
[07:06:34 -> 07:06:37]  performant okay absolutely give yourself
[07:06:36 -> 07:06:39]  pat on the back if you made it this far
[07:06:37 -> 07:06:40]  this been quite challenging so uh you
[07:06:39 -> 07:06:43]  know feel free to take take a coffee
[07:06:40 -> 07:06:45]  break or whatever um get some
[07:06:43 -> 07:06:49]  tea I have some tea with me right here
[07:06:45 -> 07:06:51]  so you know it's it it is a grind um but
[07:06:49 -> 07:06:54]  if we step back to just this blog post
[07:06:51 -> 07:06:56]  here we go to Kernel number five so
[07:06:54 -> 07:06:58]  increasing arithmetic intensity with 2D
[07:06:56 -> 07:06:59]  block tiling before we were just
[07:06:58 -> 07:07:01]  calculating columns right we were just
[07:06:59 -> 07:07:03]  calculating columns and now this one is
[07:07:01 -> 07:07:04]  going to calculate entire blocks it's
[07:07:03 -> 07:07:06]  going to calculate like a mini block
[07:07:04 -> 07:07:08]  inside of the big block tile right
[07:07:06 -> 07:07:11]  that's the idea so before we just had
[07:07:08 -> 07:07:13]  this idx in the in the in the the B tile
[07:07:11 -> 07:07:14]  this was going to go downwards and we
[07:07:13 -> 07:07:16]  just had a single thing that was
[07:07:14 -> 07:07:18]  iterating down and calculating you know
[07:07:16 -> 07:07:20]  a column so you have this you would just
[07:07:18 -> 07:07:22]  kind of intersect and you get this
[07:07:20 -> 07:07:26]  column filled
[07:07:22 -> 07:07:27]  out but now we have this res idx uh n
[07:07:26 -> 07:07:29]  component right so that's just that's
[07:07:27 -> 07:07:33]  just going to go and make this
[07:07:29 -> 07:07:35]  essentially cover um like like a square
[07:07:33 -> 07:07:36]  or like a row and a column right so they
[07:07:35 -> 07:07:39]  intersect and it's going to fill up like
[07:07:36 -> 07:07:40]  a square area um so we have this we have
[07:07:39 -> 07:07:43]  this term which we're going to iterate
[07:07:40 -> 07:07:45]  over and then we have this new TN term
[07:07:43 -> 07:07:49]  so TM is literally just this component
[07:07:45 -> 07:07:52]  and then TM is this component underneath
[07:07:49 -> 07:07:54]  right um so now like stepping into
[07:07:52 -> 07:07:56]  this this is kind of just what it looks
[07:07:54 -> 07:07:58]  like under the hood when we sort of
[07:07:56 -> 07:08:02]  visualize how this is how this is being
[07:07:58 -> 07:08:04]  calculated so we look at our um
[07:08:02 -> 07:08:06]  we look at our our a tile and we look at
[07:08:04 -> 07:08:10]  our B tile and it's literally just we're
[07:08:06 -> 07:08:12]  storing a column um in reg M so that's
[07:08:10 -> 07:08:14]  like an actual register memory in this
[07:08:12 -> 07:08:16]  kernel we're actually using register
[07:08:14 -> 07:08:18]  memory so we're we're occupying it a
[07:08:16 -> 07:08:21]  little bit more we're uh as you can see
[07:08:18 -> 07:08:23]  we literally populate it so reg m is
[07:08:21 -> 07:08:27]  going to be of that size and then T reg
[07:08:23 -> 07:08:29]  n is TN size right um and the total
[07:08:27 -> 07:08:31]  results is going to be the surface area
[07:08:29 -> 07:08:33]  of that total area there
[07:08:31 -> 07:08:35]  um and we just we essentially just store
[07:08:33 -> 07:08:37]  a column and a row and those will those
[07:08:35 -> 07:08:38]  will dot product through this entire
[07:08:37 -> 07:08:41]  thing and they will intersect and you'll
[07:08:38 -> 07:08:43]  get this little square right here um and
[07:08:41 -> 07:08:44]  you can we can see how that evolves at
[07:08:43 -> 07:08:49]  each step
[07:08:44 -> 07:08:50]  right so now there's my marker let's
[07:08:49 -> 07:08:55]  actually step into this
[07:08:50 -> 07:08:57]  kernel so we go back to here and we see
[07:08:55 -> 07:09:01]  um you know run sjem 2D block tiling
[07:08:57 -> 07:09:04]  right so we have this we we have a we
[07:09:01 -> 07:09:07]  have this same BK term right so BK is
[07:09:04 -> 07:09:09]  just going to be eight in this case um
[07:09:07 -> 07:09:11]  and we have TM which is also the same
[07:09:09 -> 07:09:12]  and we have this new TN term right so
[07:09:11 -> 07:09:14]  we're essentially just going to
[07:09:12 -> 07:09:16]  calculate um like this area that we fill
[07:09:14 -> 07:09:19]  out is going to contain 64 elements it's
[07:09:16 -> 07:09:20]  going to be 8 by 8 that's going to fill
[07:09:19 -> 07:09:22]  that entire thing up and we're going to
[07:09:20 -> 07:09:24]  have 64 in
[07:09:22 -> 07:09:27]  there now we have this little hacky
[07:09:24 -> 07:09:29]  situation for handling like the very
[07:09:27 -> 07:09:31]  very low like the smaller matrices that
[07:09:29 -> 07:09:32]  are going to be tested when we do a
[07:09:31 -> 07:09:34]  Benchmark so this is why we have this
[07:09:32 -> 07:09:39]  little out statement here
[07:09:34 -> 07:09:42]  um so in case we you know decide to use
[07:09:39 -> 07:09:43]  um in case we decide to use like a very
[07:09:42 -> 07:09:45]  small Matrix this is able to deal with
[07:09:43 -> 07:09:46]  that effectively but normally we're just
[07:09:45 -> 07:09:48]  going to we're just going to pay
[07:09:46 -> 07:09:51]  attention to this first if statement for
[07:09:48 -> 07:09:53]  now um so if these are bigger than
[07:09:51 -> 07:09:56]  bigger than or equal 128
[07:09:53 -> 07:09:59]  right
[07:09:56 -> 07:10:03]  um yeah so in
[07:09:59 -> 07:10:07]  here pay attention to this we have a
[07:10:03 -> 07:10:11]  grid dim which is going to have um an X
[07:10:07 -> 07:10:12]  component right and then a a y component
[07:10:11 -> 07:10:15]  right so that that's going to that's
[07:10:12 -> 07:10:16]  going to stay the same and then this
[07:10:15 -> 07:10:18]  block Dimension the the number of
[07:10:16 -> 07:10:20]  threads within a block right it's the
[07:10:18 -> 07:10:22]  that like the block itself the dimension
[07:10:20 -> 07:10:24]  of that the X component there's just
[07:10:22 -> 07:10:26]  going to be one value here and that's
[07:10:24 -> 07:10:29]  going to be um essentially the the total
[07:10:26 -> 07:10:32]  number of elements in the output tile so
[07:10:29 -> 07:10:34]  you know we have this B and this BN and
[07:10:32 -> 07:10:36]  then they then they they you sort of
[07:10:34 -> 07:10:39]  just get this this filled out area of C
[07:10:36 -> 07:10:42]  the C tile that is going to be of this
[07:10:39 -> 07:10:44]  of this size right and then we divide
[07:10:42 -> 07:10:47]  that by the total surface area covered
[07:10:44 -> 07:10:49]  by um by just what a thread calculate so
[07:10:47 -> 07:10:51]  a thread is going to go through those 64
[07:10:49 -> 07:10:52]  elements it's going to be 8 by8 it's
[07:10:51 -> 07:10:54]  going to go through those and they're
[07:10:52 -> 07:10:57]  going to calculate a small little mini
[07:10:54 -> 07:11:00]  grid for us um like per thread right um
[07:10:57 -> 07:11:04]  and so if we we divide the total number
[07:11:00 -> 07:11:06]  of output result results by the uh by
[07:11:04 -> 07:11:08]  the space covered by a thread we
[07:11:06 -> 07:11:10]  actually get the number of threads right
[07:11:08 -> 07:11:12]  because each little piece here is a
[07:11:10 -> 07:11:13]  thread and so if we divide this by this
[07:11:12 -> 07:11:16]  we actually get the total number of
[07:11:13 -> 07:11:18]  threads within that entire SE tile um so
[07:11:16 -> 07:11:22]  if we do this math
[07:11:18 -> 07:11:22]  here we're going to get so
[07:11:23 -> 07:11:26]  128
[07:11:27 -> 07:11:32]  times8 and then divide this by
[07:11:31 -> 07:11:37]  8
[07:11:32 -> 07:11:40]  by8 and this simplifies to well 128 is 2
[07:11:37 -> 07:11:42]  to 7 right because remember like if you
[07:11:40 -> 07:11:44]  if you know like int 8 Precision it's
[07:11:42 -> 07:11:47]  it's like it's like an image essentially
[07:11:44 -> 07:11:51]  like number of number of like uh RGB
[07:11:47 -> 07:11:54]  values in a single U like in a single
[07:11:51 -> 07:11:56]  Pixel so you have like RGB and each of
[07:11:54 -> 07:11:58]  those is like Zer to 255 so that that's
[07:11:56 -> 07:12:00]  like how you can go down to what like uh
[07:11:58 -> 07:12:02]  128 is 2 to the 7 cuz it's like in 7
[07:12:00 -> 07:12:07]  instead 8 that's how I made that
[07:12:02 -> 07:12:07]  Association but we have this 2
[07:12:08 -> 07:12:16]  7
[07:12:10 -> 07:12:20]  7/ 2 3 * 2 3 and what you end up
[07:12:16 -> 07:12:20]  with is 2
[07:12:22 -> 07:12:29]  14
[07:12:24 -> 07:12:33]  / 2 6 and we do our exponent laws and we
[07:12:29 -> 07:12:36]  get eight right so 14 - 6 is 8 so two
[07:12:33 -> 07:12:40]  the8 that means we have
[07:12:36 -> 07:12:45]  256 my marker works
[07:12:40 -> 07:12:45]  properly 256 threads
[07:12:47 -> 07:12:53]  right 256 threads my writing is messy
[07:12:50 -> 07:12:55]  ignore that um awesome so we know the
[07:12:53 -> 07:12:56]  number of threads that we're calculating
[07:12:55 -> 07:12:58]  now this is going to help us when we
[07:12:56 -> 07:13:00]  actually jump into this kernel here so
[07:12:58 -> 07:13:03]  you know we have our our BM or BN BK TM
[07:13:00 -> 07:13:03]  and TM again
[07:13:04 -> 07:13:10]  right there's a lot happening here let's
[07:13:06 -> 07:13:12]  break this down so to start we have our
[07:13:10 -> 07:13:14]  our row which is going to be the the
[07:13:12 -> 07:13:17]  typical y component we've already went
[07:13:14 -> 07:13:19]  over this the total results in a block
[07:13:17 -> 07:13:22]  tile are as we already said in the in
[07:13:19 -> 07:13:24]  the runner file uh this is just
[07:13:22 -> 07:13:26]  essentially the the the whole the whole
[07:13:24 -> 07:13:28]  seed Matrix so you have the M Dimension
[07:13:26 -> 07:13:29]  then you have the N Dimension you
[07:13:28 -> 07:13:30]  multiply those and you get total area
[07:13:29 -> 07:13:32]  right that's the number of results we're
[07:13:30 -> 07:13:36]  going to get calculate and then we get
[07:13:32 -> 07:13:38]  the number of threads per that entire
[07:13:36 -> 07:13:41]  thing so how many individual squares are
[07:13:38 -> 07:13:43]  there that a thread is occupying right
[07:13:41 -> 07:13:45]  and so that the square surface area is
[07:13:43 -> 07:13:46]  this which we already calculated and so
[07:13:45 -> 07:13:49]  this number of threads per block tile
[07:13:46 -> 07:13:51]  should equal 256 right and then we just
[07:13:49 -> 07:13:53]  assert that down here so we want to make
[07:13:51 -> 07:13:55]  sure that 256 value is equal to what we
[07:13:53 -> 07:13:57]  just calculated which is going to hold
[07:13:55 -> 07:13:59]  true of course and then it won't it'll
[07:13:57 -> 07:14:03]  actually continue with executing right
[07:13:59 -> 07:14:05]  um let go down a little bit further
[07:14:03 -> 07:14:06]  um don't worry about these right now
[07:14:05 -> 07:14:10]  we're going to get into those in a
[07:14:06 -> 07:14:12]  second so we do the the classical just
[07:14:10 -> 07:14:15]  allocate shared memory we advance the
[07:14:12 -> 07:14:17]  block tiles based on um you know where
[07:14:15 -> 07:14:20]  they're supposed to be at in
[07:14:17 -> 07:14:25]  the uh in in the in the current block
[07:14:20 -> 07:14:25]  right or or I guess in in in the current
[07:14:26 -> 07:14:30]  thread this is where a lot of the magic
[07:14:28 -> 07:14:32]  happens right here so this this part is
[07:14:30 -> 07:14:34]  crucial pay attention to let's first go
[07:14:32 -> 07:14:36]  over the easy stuff so thread results is
[07:14:34 -> 07:14:38]  just like how big is that that little
[07:14:36 -> 07:14:40]  square that thread is going to calculate
[07:14:38 -> 07:14:42]  that's just going to be um this is going
[07:14:40 -> 07:14:45]  to be 8 by 8
[07:14:42 -> 07:14:49]  right we have this reg M term which we
[07:14:45 -> 07:14:51]  saw in the blog post so reg m is this
[07:14:49 -> 07:14:55]  and reg n is
[07:14:51 -> 07:14:59]  that TM and TN right that's just going
[07:14:55 -> 07:15:01]  to be the the little the I guess the the
[07:14:59 -> 07:15:04]  iterations of the dot product so each
[07:15:01 -> 07:15:06]  dot at each iteration of the of idx it's
[07:15:04 -> 07:15:07]  going to store those in registers and
[07:15:06 -> 07:15:09]  it's going to calculate them really
[07:15:07 -> 07:15:10]  really fast right it's going to be like
[07:15:09 -> 07:15:12]  it's going to be like eight it's going
[07:15:10 -> 07:15:16]  to be like 8 by one and then a 1 by 8
[07:15:12 -> 07:15:16]  and it's just going to evolve that
[07:15:16 -> 07:15:20]  way now we look up at
[07:15:19 -> 07:15:23]  these
[07:15:20 -> 07:15:26]  so in a row a so we just essentially
[07:15:23 -> 07:15:27]  have the thread index and we divide that
[07:15:26 -> 07:15:30]  by BK
[07:15:27 -> 07:15:32]  right in a row same idea but we use the
[07:15:30 -> 07:15:33]  mod oper
[07:15:32 -> 07:15:35]  then we have this new term called a
[07:15:33 -> 07:15:36]  stride right and now we're actually
[07:15:35 -> 07:15:38]  going to do the math we're going to do
[07:15:36 -> 07:15:42]  the math for a stride
[07:15:38 -> 07:15:42]  here so so stride
[07:15:50 -> 07:15:58]  a stride a is going to be number of
[07:15:53 -> 07:15:58]  threads per block tile which is 256
[07:16:01 -> 07:16:07]  divid by BK which in this case
[07:16:08 -> 07:16:14]  is8 so here if you divide this out you
[07:16:11 -> 07:16:14]  end up getting
[07:16:14 -> 07:16:20]  32 so our stride a is going to be 32
[07:16:21 -> 07:16:27]  right my marker is a little weird let's
[07:16:24 -> 07:16:29]  pay attention to that term
[07:16:27 -> 07:16:32]  um and then we have this other stride
[07:16:29 -> 07:16:34]  down here stride B is the same thing but
[07:16:32 -> 07:16:36]  instead of dividing by BK we divide by
[07:16:34 -> 07:16:41]  BN so BN is actually
[07:16:36 -> 07:16:43]  128 so if we divide 256
[07:16:41 -> 07:16:46]  right
[07:16:43 -> 07:16:46]  128 the answer is
[07:16:47 -> 07:16:50]  two now these are going to be important
[07:16:49 -> 07:16:53]  these are going to be very important as
[07:16:50 -> 07:16:55]  we sort of Step through this so let's
[07:16:53 -> 07:16:58]  jump down to this actual uh loop here
[07:16:55 -> 07:17:00]  now so inside of a shared look at how we
[07:16:58 -> 07:17:02]  Index this start starts here and ends
[07:17:00 -> 07:17:05]  there we have this first thing in the
[07:17:02 -> 07:17:06]  brackets which is uh inner row a so it's
[07:17:05 -> 07:17:10]  going to be a certain row that we're
[07:17:06 -> 07:17:13]  looking at plus a load offset right so
[07:17:10 -> 07:17:16]  the load offset is going to iterate up
[07:17:13 -> 07:17:19]  to BM which in this case is
[07:17:16 -> 07:17:21]  128 so you can think of it as us having
[07:17:19 -> 07:17:25]  this
[07:17:21 -> 07:17:25]  um this is the a
[07:17:26 -> 07:17:32]  tile
[07:17:28 -> 07:17:36]  right and so this is going to
[07:17:32 -> 07:17:36]  be this going to be
[07:17:36 -> 07:17:42]  BM and that's going to be BK right so
[07:17:40 -> 07:17:45]  inside of here we're going to iterate up
[07:17:42 -> 07:17:50]  to BM in strides of stride a now stride
[07:17:45 -> 07:17:52]  a is size 32 right stri a is 32 so if we
[07:17:50 -> 07:17:53]  actually look at how many times it's
[07:17:52 -> 07:17:55]  going to stride through this until it
[07:17:53 -> 07:17:59]  reaches the end it's going to start at
[07:17:55 -> 07:18:02]  zero and then it's going to go down to
[07:17:59 -> 07:18:02]  32 I can
[07:18:04 -> 07:18:07]  and then it's going to go down to
[07:18:08 -> 07:18:14]  64 and then it's going to go down to uh
[07:18:12 -> 07:18:14]  I believe
[07:18:17 -> 07:18:20]  96 and then it's going to stop right
[07:18:19 -> 07:18:22]  it's going to it's it's not going to
[07:18:20 -> 07:18:24]  actually hit this it's going to stop
[07:18:22 -> 07:18:26]  there so we're going to have this this
[07:18:24 -> 07:18:27]  initial offet of zero and then it's
[07:18:26 -> 07:18:29]  going to bump up to 32 in the next
[07:18:27 -> 07:18:33]  iteration and it's going to jump to 64
[07:18:29 -> 07:18:35]  and then 96 so notice the actual area
[07:18:33 -> 07:18:37]  that we have to fill in here right
[07:18:35 -> 07:18:38]  notice the actual area we have to fill
[07:18:37 -> 07:18:41]  in so this
[07:18:38 -> 07:18:41]  area
[07:18:44 -> 07:18:49]  is
[07:18:46 -> 07:18:52]  32 and then this BK we already know is
[07:18:49 -> 07:18:52]  eight
[07:18:54 -> 07:18:58]  right and it's going to be the same for
[07:18:56 -> 07:19:00]  this and same for that and same for
[07:18:58 -> 07:19:04]  that so this is actually where where
[07:19:00 -> 07:19:07]  some really cool stuff comes in now this
[07:19:04 -> 07:19:11]  this inner row a here this inner row a
[07:19:07 -> 07:19:13]  is calculated as the thread idx divided
[07:19:11 -> 07:19:16]  BK
[07:19:13 -> 07:19:21]  right so this value if we do the math
[07:19:16 -> 07:19:21]  here idx is say Max is out at
[07:19:22 -> 07:19:32]  256 my marker I need to get
[07:19:26 -> 07:19:32]  a 256 ID BK which is a right
[07:19:32 -> 07:19:37]  now this number is going to max out at
[07:19:35 -> 07:19:40]  32 and that's going to be our row right
[07:19:37 -> 07:19:43]  so remember this if it's
[07:19:40 -> 07:19:45]  32 this number this this inner row which
[07:19:43 -> 07:19:48]  is based off of the thread itself is
[07:19:45 -> 07:19:51]  going to max out at 32 now this inner
[07:19:48 -> 07:19:54]  column on the other hand is going to do
[07:19:51 -> 07:19:56]  mod BK right so when we do this it's
[07:19:54 -> 07:19:59]  essentially going to it's going to go
[07:19:56 -> 07:20:00]  like 0 1 2 3 all the way up to eight and
[07:19:59 -> 07:20:01]  then once it hits eight again it's it's
[07:20:00 -> 07:20:04]  going to jump rack zero and it's going
[07:20:01 -> 07:20:06]  to reset so it's going to be like 0
[07:20:04 -> 07:20:08]  through 8 and then reset 0 through 8 and
[07:20:06 -> 07:20:11]  reset right and then we're going to have
[07:20:08 -> 07:20:13]  these these rows uh the row index that
[07:20:11 -> 07:20:15]  goes from 0 to 32 right so every time we
[07:20:13 -> 07:20:17]  hit the next eight it's going to it's
[07:20:15 -> 07:20:19]  going to bump up one
[07:20:17 -> 07:20:22]  right and so this actually makes a lot
[07:20:19 -> 07:20:24]  of sense so inside of here we have this
[07:20:22 -> 07:20:27]  load offset which is going to be strided
[07:20:24 -> 07:20:30]  by this much this 32 here and then we
[07:20:27 -> 07:20:32]  add whatever this row offset is so we
[07:20:30 -> 07:20:34]  have the row offset plus whatever that
[07:20:32 -> 07:20:37]  number is we can effectively populate
[07:20:34 -> 07:20:39]  this entire area here so it's like the
[07:20:37 -> 07:20:42]  zero or the or say it's like I don't
[07:20:39 -> 07:20:44]  know 64 and then plus whichever row it's
[07:20:42 -> 07:20:48]  at from the thread index itself which we
[07:20:44 -> 07:20:52]  which we calculate up here
[07:20:48 -> 07:20:53]  um it's going to be that times the K
[07:20:52 -> 07:20:55]  Dimension so it's that's why it's going
[07:20:53 -> 07:20:57]  to stride over as many rows as it needs
[07:20:55 -> 07:21:00]  to and it's going to end up at some
[07:20:57 -> 07:21:02]  certain index and then once we multiply
[07:21:00 -> 07:21:04]  that we know where it's at vertically
[07:21:02 -> 07:21:07]  and then we simply add the inner column
[07:21:04 -> 07:21:09]  index to that so the inner column index
[07:21:07 -> 07:21:11]  like I said before is going to cap out
[07:21:09 -> 07:21:14]  at 8 right so it's going to cap out at 8
[07:21:11 -> 07:21:17]  and it's going to land somewhere here so
[07:21:14 -> 07:21:21]  by doing this we only actually iterate
[07:21:17 -> 07:21:24]  over this four times and each thread
[07:21:21 -> 07:21:28]  spanning you know 32 * 8 CU if you
[07:21:24 -> 07:21:32]  actually do 32 * 8 that's
[07:21:28 -> 07:21:32]  like 2 5
[07:21:33 -> 07:21:42]  2 5 is 32 and then uh two uh 2 the 3 is
[07:21:38 -> 07:21:46]  8 so 5 + 3 with our exponent laws that's
[07:21:42 -> 07:21:48]  2 to the 8 which is int 8 maps to 256
[07:21:46 -> 07:21:49]  right so that that's how I kind of make
[07:21:48 -> 07:21:52]  those associations you don't have to
[07:21:49 -> 07:21:55]  follow along but 8 * 32 is
[07:21:52 -> 07:21:57]  256 look how awesome that is each thread
[07:21:55 -> 07:22:00]  is going to occupy an individual spot in
[07:21:57 -> 07:22:02]  here so we're going to load this Chunk
[07:22:00 -> 07:22:05]  in each thread is going to take a little
[07:22:02 -> 07:22:06]  spot in there 32 * 8 and then we load in
[07:22:05 -> 07:22:09]  the next one each thread takes its own
[07:22:06 -> 07:22:11]  spot so it's like one operation for
[07:22:09 -> 07:22:12]  every single iteration right so we don't
[07:22:11 -> 07:22:16]  have to go sequentially through it each
[07:22:12 -> 07:22:18]  thread is just boom done it's one
[07:22:16 -> 07:22:20]  instruction same for this same for this
[07:22:18 -> 07:22:22]  same for
[07:22:20 -> 07:22:24]  this and then we just index accordingly
[07:22:22 -> 07:22:26]  we just kind of adjust it so instead of
[07:22:24 -> 07:22:28]  BK being the being the stride over you
[07:22:26 -> 07:22:30]  know we do we times K so it's able to
[07:22:28 -> 07:22:32]  stride over the entire thing when we're
[07:22:30 -> 07:22:34]  actually loading from the GPU vram um
[07:22:32 -> 07:22:36]  because it's it's actually bigger right
[07:22:34 -> 07:22:39]  so the these matrices are much bigger
[07:22:36 -> 07:22:40]  when we're when they aren't um tiles yet
[07:22:39 -> 07:22:43]  so we have to stride like the whole K
[07:22:40 -> 07:22:45]  distance instead of just the the the the
[07:22:43 -> 07:22:48]  tile K
[07:22:45 -> 07:22:53]  distance and then same idea applies to
[07:22:48 -> 07:22:55]  um BS right the the the not BS
[07:22:53 -> 07:22:59]  bshar
[07:22:55 -> 07:22:59]  not not that
[07:23:01 -> 07:23:06]  if I get up for a
[07:23:03 -> 07:23:08]  second and look at what BS looks like
[07:23:06 -> 07:23:08]  it's literally
[07:23:10 -> 07:23:15]  this let me actually get a different
[07:23:12 -> 07:23:15]  marker
[07:23:17 -> 07:23:23]  terrible it's going to be like this go
[07:23:20 -> 07:23:27]  down this way right that's what that's
[07:23:23 -> 07:23:30]  what BS is going to look like now if we
[07:23:27 -> 07:23:32]  look at if we look at these values
[07:23:30 -> 07:23:36]  inside we have this load offset which is
[07:23:32 -> 07:23:37]  going to iterate up to um up to up to BK
[07:23:36 -> 07:23:39]  right and
[07:23:37 -> 07:23:43]  BK is
[07:23:39 -> 07:23:46]  this remember it's not it's not this
[07:23:43 -> 07:23:49]  part anymore it's it's the side and then
[07:23:46 -> 07:23:52]  this part is BN right because it's K byn
[07:23:49 -> 07:23:55]  as the B Matrix
[07:23:52 -> 07:23:57]  so when we when we iterate in these
[07:23:55 -> 07:24:02]  strides it's actually going to end up
[07:23:57 -> 07:24:04]  being um well BN in this case is 128 so
[07:24:02 -> 07:24:08]  it's going to be this uh this stride
[07:24:04 -> 07:24:12]  value is going to be um or stride B
[07:24:08 -> 07:24:16]  sorry is going to be 256 / 128 which is
[07:24:12 -> 07:24:20]  two right as we calculated um down
[07:24:16 -> 07:24:23]  here so it's actually going to stride um
[07:24:20 -> 07:24:23]  it's actually going to
[07:24:23 -> 07:24:30]  stride it's going to stride two right
[07:24:26 -> 07:24:32]  and two times the length of this is
[07:24:30 -> 07:24:34]  actually a quarter right so if you have
[07:24:32 -> 07:24:37]  eight values you have eight different
[07:24:34 -> 07:24:40]  eight different rows you split it
[07:24:37 -> 07:24:42]  once now you're in half and there's four
[07:24:40 -> 07:24:45]  rows here and four rows there now if you
[07:24:42 -> 07:24:45]  split it in half
[07:24:46 -> 07:24:50]  again then you have two and two and two
[07:24:49 -> 07:24:53]  and two all that that up adds up to
[07:24:50 -> 07:24:55]  eight so when we actually stride we're
[07:24:53 -> 07:24:56]  just going a quarter of the way down
[07:24:55 -> 07:24:59]  same thing in here we go a quarter of
[07:24:56 -> 07:25:01]  the way quarter quarter right same idea
[07:24:59 -> 07:25:03]  applies here so whatever this stride
[07:25:01 -> 07:25:05]  value is it's going to start at zero and
[07:25:03 -> 07:25:09]  it's going to go up to six because it's
[07:25:05 -> 07:25:12]  going to it's going to cap out at
[07:25:09 -> 07:25:15]  um it's going to cap up at B whatever BK
[07:25:12 -> 07:25:18]  was in this case is eight and it's just
[07:25:15 -> 07:25:22]  going to stop so it's going to go 0 0 2
[07:25:18 -> 07:25:26]  4 6 and it's going to stop right then we
[07:25:22 -> 07:25:29]  have this inner row b which is from here
[07:25:26 -> 07:25:32]  and this is just the same idea so inside
[07:25:29 -> 07:25:35]  of this we have the thread idx um in
[07:25:32 -> 07:25:37]  this case it it's just going to max out
[07:25:35 -> 07:25:41]  at
[07:25:37 -> 07:25:41]  256 and we divide that
[07:25:41 -> 07:25:47]  number we divide that number by BN which
[07:25:44 -> 07:25:47]  in this case is
[07:25:49 -> 07:25:55]  128 and the column is the same idea it's
[07:25:52 -> 07:25:56]  going to go zero all the way up to 128
[07:25:55 -> 07:25:57]  and then it's going to reset right so
[07:25:56 -> 07:26:01]  it's not just going to iterate every
[07:25:57 -> 07:26:04]  time we stri 128 it's going to to uh
[07:26:01 -> 07:26:09]  it's going to go up it's going to
[07:26:04 -> 07:26:12]  essentially one mod 128 is one 127 28 is
[07:26:09 -> 07:26:15]  127 right so that's that's literally all
[07:26:12 -> 07:26:15]  it's going to do here
[07:26:15 -> 07:26:20]  um in case you haven't noticed already
[07:26:18 -> 07:26:21]  this is the same idea as here except
[07:26:20 -> 07:26:23]  we're just dealing with kind of like a a
[07:26:21 -> 07:26:25]  more like very stretched out thing
[07:26:23 -> 07:26:29]  instead of instead of like these nice to
[07:26:25 -> 07:26:31]  look at I candy looking blocks right so
[07:26:29 -> 07:26:33]  the idea here is is that we just go
[07:26:31 -> 07:26:37]  however much we need to in here CU this
[07:26:33 -> 07:26:39]  this total thing is like 128 and 128
[07:26:37 -> 07:26:40]  that totals up to 256 so it's
[07:26:39 -> 07:26:43]  essentially each thread gets its own
[07:26:40 -> 07:26:45]  little space in there one thread two
[07:26:43 -> 07:26:46]  thread three thread four
[07:26:45 -> 07:26:48]  thread right they all get their own
[07:26:46 -> 07:26:51]  little piece in there and they're able
[07:26:48 -> 07:26:52]  to load in quarters downwards right that
[07:26:51 -> 07:26:54]  that's that's essentially how we're
[07:26:52 -> 07:26:57]  loading into Shar memory so just given
[07:26:54 -> 07:26:59]  this new context we're just loading um
[07:26:57 -> 07:27:01]  you know we're just being clever about
[07:26:59 -> 07:27:04]  how we load
[07:27:01 -> 07:27:05]  things so given that we understand how
[07:27:04 -> 07:27:06]  it's everything is loaded into share
[07:27:05 -> 07:27:08]  memory we can go and jump into the next
[07:27:06 -> 07:27:09]  part here and this part is like where
[07:27:08 -> 07:27:11]  things get a little it's a little funny
[07:27:09 -> 07:27:13]  it's not actually as as intuitive and
[07:27:11 -> 07:27:15]  bad as this part um but it is still a
[07:27:13 -> 07:27:20]  little bit weird with the indexing part
[07:27:15 -> 07:27:23]  so if we jump into this it's we iterate
[07:27:20 -> 07:27:25]  over this idx right and this idx going
[07:27:23 -> 07:27:28]  back to this is literally just it's it's
[07:27:25 -> 07:27:29]  going to evolve right so idx index0 1 2
[07:27:28 -> 07:27:30]  and three right they're going they're
[07:27:29 -> 07:27:32]  going inwards like this
[07:27:30 -> 07:27:34]  uh one thread is in charge of this
[07:27:32 -> 07:27:36]  little block at the center um and the
[07:27:34 -> 07:27:40]  threat the each thread is is responsible
[07:27:36 -> 07:27:44]  for uh loading in a column and a row
[07:27:40 -> 07:27:47]  right one thread takes care of that
[07:27:44 -> 07:27:49]  um now if we look inside of here we load
[07:27:47 -> 07:27:51]  into the registers right so the
[07:27:49 -> 07:27:52]  registers are the extremely fast pieces
[07:27:51 -> 07:27:56]  of storage that are literally right next
[07:27:52 -> 07:27:58]  to the core in in the in the GPU and so
[07:27:56 -> 07:28:01]  we have this reg M right which is going
[07:27:58 -> 07:28:03]  to load that column in so how do we load
[07:28:01 -> 07:28:06]  this we have to look at this thread row
[07:28:03 -> 07:28:08]  right and to understand everything else
[07:28:06 -> 07:28:10]  let's go look at thread row here so
[07:28:08 -> 07:28:14]  where is this thread row so it's the
[07:28:10 -> 07:28:18]  thread ID x.x so this could be a maximum
[07:28:14 -> 07:28:23]  of you know
[07:28:18 -> 07:28:29]  256 and then we divide that by BN / TN
[07:28:23 -> 07:28:35]  right so BN is 128 and TN is 8 so you
[07:28:29 -> 07:28:39]  divide 128 by 8 and that's 16 so we have
[07:28:35 -> 07:28:41]  256 divided or 0 to 255 and then
[07:28:39 -> 07:28:47]  whichever one of those it is divide that
[07:28:41 -> 07:28:49]  by 16 right so inside of this we end up
[07:28:47 -> 07:28:51]  having 16 different numbers that could
[07:28:49 -> 07:28:54]  we could be through right so if we look
[07:28:51 -> 07:28:56]  at this original atile Here There are 16
[07:28:54 -> 07:28:59]  different
[07:28:56 -> 07:29:01]  um There are 16 different rows we could
[07:28:59 -> 07:29:03]  be right and these are just offsets keep
[07:29:01 -> 07:29:06]  in mind we're loading in columns that
[07:29:03 -> 07:29:10]  are eight elements long so we have like
[07:29:06 -> 07:29:11]  16 16 16 all the way down uh we have
[07:29:10 -> 07:29:13]  this eight
[07:29:11 -> 07:29:17]  times um
[07:29:13 -> 07:29:21]  and each of these or no sorry 16 times
[07:29:17 -> 07:29:23]  um each each each little column here
[07:29:21 -> 07:29:26]  this little like colored in area you
[07:29:23 -> 07:29:28]  could say that is going to be eight long
[07:29:26 -> 07:29:32]  right so we essentially are loading in
[07:29:28 -> 07:29:36]  like we go eight that's like that's like
[07:29:32 -> 07:29:38]  a single um that is a single column and
[07:29:36 -> 07:29:39]  then whichever one comes next it's like
[07:29:38 -> 07:29:42]  this is another eight and we go all the
[07:29:39 -> 07:29:46]  way down 16 times and that multiplies up
[07:29:42 -> 07:29:49]  to the total length of M which is 128 so
[07:29:46 -> 07:29:51]  the math is mathing um the same thing
[07:29:49 -> 07:29:52]  applies to thread column right I mean
[07:29:51 -> 07:29:54]  these are these are square matrices so
[07:29:52 -> 07:29:58]  it's not actually too bad to deal with
[07:29:54 -> 07:30:00]  this um so same idea here um and we end
[07:29:58 -> 07:30:02]  up with some range between 0 and 50
[07:30:00 -> 07:30:08]  right so if we go back
[07:30:02 -> 07:30:13]  down thread row time TM so TM is
[07:30:08 -> 07:30:16]  uh TM is which uh like TM is essentially
[07:30:13 -> 07:30:18]  eight right so it's going to be
[07:30:16 -> 07:30:21]  whichever thread row we're at so which
[07:30:18 -> 07:30:22]  whichever one of these out of um out of
[07:30:21 -> 07:30:28]  out of 16 are we
[07:30:22 -> 07:30:31]  at and then times um times
[07:30:28 -> 07:30:33]  TM plus I and I is going to be that
[07:30:31 -> 07:30:35]  little iterator here that goes up to TM
[07:30:33 -> 07:30:37]  right TM is like that the length of that
[07:30:35 -> 07:30:39]  column or the like yeah you could say
[07:30:37 -> 07:30:41]  the height or the the length of that
[07:30:39 -> 07:30:43]  column it's going to iterate up to I and
[07:30:41 -> 07:30:46]  it's going to put that into the register
[07:30:43 -> 07:30:48]  M right we multiply this whole thing by
[07:30:46 -> 07:30:50]  BK so that we get this offset going
[07:30:48 -> 07:30:52]  because we have to go through this k
[07:30:50 -> 07:30:53]  thing and reset every single time we
[07:30:52 -> 07:30:56]  want to get a new column you can't just
[07:30:53 -> 07:30:58]  go directly down you have to stride one
[07:30:56 -> 07:31:01]  entire length over to get to the next
[07:30:58 -> 07:31:03]  one and then we add idx to this so This
[07:31:01 -> 07:31:06]  actually becomes very intuitive when we
[07:31:03 -> 07:31:08]  look at it from a glance so hopefully
[07:31:06 -> 07:31:12]  this is sort of hopefully this is sort
[07:31:08 -> 07:31:13]  of making sense in your head um but this
[07:31:12 -> 07:31:16]  idx is just going to evolve us it's
[07:31:13 -> 07:31:17]  going to be that that horizontal offset
[07:31:16 -> 07:31:18]  and then this is the vertical offset
[07:31:17 -> 07:31:21]  right this whole portion here that's the
[07:31:18 -> 07:31:24]  vertical offset
[07:31:21 -> 07:31:26]  um and then we apply the same concept to
[07:31:24 -> 07:31:28]  loading into register n right so let
[07:31:26 -> 07:31:30]  register n isn't as bad um because we're
[07:31:28 -> 07:31:33]  actually loading uh we're actually
[07:31:30 -> 07:31:35]  loading horizontally so it's going to be
[07:31:33 -> 07:31:39]  the idx which by the way idx is going
[07:31:35 -> 07:31:41]  downwards now so idx times BN BN is the
[07:31:39 -> 07:31:46]  length of it right so you're going to go
[07:31:41 -> 07:31:48]  down um whichever idx you're at
[07:31:46 -> 07:31:51]  um you're going to go down that many
[07:31:48 -> 07:31:54]  layers and then you're going to do uh
[07:31:51 -> 07:31:56]  whichever whichever thread column you're
[07:31:54 -> 07:31:58]  at so each thread is going to have its
[07:31:56 -> 07:31:59]  own uh it's going to have its own little
[07:31:58 -> 07:32:01]  section of that right it's going to have
[07:31:59 -> 07:32:03]  it own job cuz the thread is like
[07:32:01 -> 07:32:06]  loading a specific a specific Square in
[07:32:03 -> 07:32:10]  that tile so thread here thread here
[07:32:06 -> 07:32:12]  thread here thread here it's like
[07:32:10 -> 07:32:13]  yeah each thread has its own thing so
[07:32:12 -> 07:32:16]  we're just worrying about a single
[07:32:13 -> 07:32:19]  thread
[07:32:16 -> 07:32:22]  um thread thread is like you know thread
[07:32:19 -> 07:32:24]  one is like here and then thread 255 is
[07:32:22 -> 07:32:25]  or thread zero is here and then thread
[07:32:24 -> 07:32:27]  255 is here because it goes to like the
[07:32:25 -> 07:32:30]  very edge and then the very edge and
[07:32:27 -> 07:32:31]  then they intersect at 256 right that
[07:32:30 -> 07:32:33]  that's how that's how I'm sort of
[07:32:31 -> 07:32:34]  visualizing this now we have this extra
[07:32:33 -> 07:32:36]  I term which is literally just the
[07:32:34 -> 07:32:38]  horizontal offset so when you when
[07:32:36 -> 07:32:40]  you've like looped around how many
[07:32:38 -> 07:32:42]  whatever amount of do ID EXs you need
[07:32:40 -> 07:32:44]  then you have that additional I which is
[07:32:42 -> 07:32:45]  going to be the offset so you're going
[07:32:44 -> 07:32:46]  to load in the first first element in
[07:32:45 -> 07:32:48]  the row then the second then the third
[07:32:46 -> 07:32:49]  then the fourth and then the same
[07:32:48 -> 07:32:51]  applies for this column up here which we
[07:32:49 -> 07:32:53]  already did it's just going to do 1 2 3
[07:32:51 -> 07:32:55]  4 all the way down to eight right and
[07:32:53 -> 07:32:57]  it's going to store this out in like a
[07:32:55 -> 07:32:59]  register memory like a line and this
[07:32:57 -> 07:33:02]  allows us to easily do product that
[07:32:59 -> 07:33:05]  right so when we drop down to here um
[07:33:02 -> 07:33:08]  this is the entire Loop yellow to Yellow
[07:33:05 -> 07:33:11]  um we start off with the the M component
[07:33:08 -> 07:33:13]  right so thread m is this part and then
[07:33:11 -> 07:33:16]  thread n is this part so this this n
[07:33:13 -> 07:33:19]  component is actually inside of it now
[07:33:16 -> 07:33:22]  the thread result is calculated as res
[07:33:19 -> 07:33:26]  idx M so whichever however much it is
[07:33:22 -> 07:33:28]  through uh TM right that times TN which
[07:33:26 -> 07:33:29]  is the which is the horizontal stride
[07:33:28 -> 07:33:32]  that you need to do to get to the next
[07:33:29 -> 07:33:35]  the next um the next row
[07:33:32 -> 07:33:40]  right and then you have this TN as well
[07:33:35 -> 07:33:40]  so TN is how how far along you are so
[07:33:41 -> 07:33:45]  it's or sorry res idx is how far along
[07:33:43 -> 07:33:48]  you are uh so this is going to be your
[07:33:45 -> 07:33:50]  your vertical stride and then this is
[07:33:48 -> 07:33:51]  going to be the horizontal offset right
[07:33:50 -> 07:33:53]  so that's how that's we mean we're
[07:33:51 -> 07:33:55]  storing it in linear memory but this is
[07:33:53 -> 07:34:01]  this is how we're going to index into it
[07:33:55 -> 07:34:01]  right and then we do whatever that is um
[07:34:02 -> 07:34:06]  and this keep in mind this is an
[07:34:03 -> 07:34:08]  individual um this an individual grid
[07:34:06 -> 07:34:11]  right say you're you're looking at a
[07:34:08 -> 07:34:13]  point within that grid you're doing um
[07:34:11 -> 07:34:15]  essentially a DOT product across those
[07:34:13 -> 07:34:18]  so you have to iterate through you know
[07:34:15 -> 07:34:19]  eight and then eight again so it's 64
[07:34:18 -> 07:34:22]  iterations you have to go through
[07:34:19 -> 07:34:24]  filling up that entire tile um and then
[07:34:22 -> 07:34:27]  you just use you know as you would
[07:34:24 -> 07:34:29]  expect that that value whatever it is so
[07:34:27 -> 07:34:31]  you're just essentially just crossing
[07:34:29 -> 07:34:33]  them you're finding where they intersect
[07:34:31 -> 07:34:36]  and then you're setting whatever this is
[07:34:33 -> 07:34:39]  in that so it end is just becoming this
[07:34:36 -> 07:34:42]  entire uh Matrix light out it's like
[07:34:39 -> 07:34:43]  instead of being uh loaded like this you
[07:34:42 -> 07:34:46]  just take this row and you attach it to
[07:34:43 -> 07:34:47]  the end and then this attach it to there
[07:34:46 -> 07:34:50]  and then this one it's even further
[07:34:47 -> 07:34:51]  right that's all we're doing there so
[07:34:50 -> 07:34:54]  it's like literally as you would imagine
[07:34:51 -> 07:34:56]  in your head this is how it's working um
[07:34:54 -> 07:34:57]  it's just it's just important to
[07:34:56 -> 07:34:59]  actually highlight like what the
[07:34:57 -> 07:35:01]  indexing is actually doing instead of
[07:34:59 -> 07:35:02]  just trusting that it works it's really
[07:35:01 -> 07:35:04]  important to actually dig deep into what
[07:35:02 -> 07:35:05]  this is doing under the hood so I
[07:35:04 -> 07:35:07]  encourage you if this doesn't entirely
[07:35:05 -> 07:35:09]  make sense it's very intuitive I
[07:35:07 -> 07:35:11]  encourage you to test it with your own
[07:35:09 -> 07:35:12]  examples so even just like get a piece
[07:35:11 -> 07:35:14]  of paper write down on a whiteboard
[07:35:12 -> 07:35:15]  whatever you need to do um and just and
[07:35:14 -> 07:35:17]  just sort of write this out and try to
[07:35:15 -> 07:35:20]  visualize it through each step right so
[07:35:17 -> 07:35:21]  you can even set for example TM to and
[07:35:20 -> 07:35:23]  TN to four right you can make it much
[07:35:21 -> 07:35:25]  easier on yourself you don't have to go
[07:35:23 -> 07:35:26]  to the full extent that we're using with
[07:35:25 -> 07:35:28]  you know our parameters like eight and
[07:35:26 -> 07:35:30]  two 128 here you don't have to even go
[07:35:28 -> 07:35:32]  that far you can be very simple about
[07:35:30 -> 07:35:35]  how you exercise that
[07:35:32 -> 07:35:38]  um but yeah so this is actually how we
[07:35:35 -> 07:35:41]  calculate the the individual thread tile
[07:35:38 -> 07:35:43]  so the This Thread tile this little 2D
[07:35:41 -> 07:35:45]  thing inside of the bigger block tile
[07:35:43 -> 07:35:48]  and we calculate one of those per thread
[07:35:45 -> 07:35:53]  right so 256 threads are laid out um so
[07:35:48 -> 07:35:56]  it's it's like 0 to 16 0 to I think
[07:35:53 -> 07:35:58]  240 and then over here it's 256 that
[07:35:56 -> 07:36:00]  that it might be like shifted based on
[07:35:58 -> 07:36:02]  how you're seeing that the picture in my
[07:36:00 -> 07:36:05]  hand waving but that's the idea is you
[07:36:02 -> 07:36:08]  go from from0
[07:36:05 -> 07:36:10]  to 256 right let me just make sure
[07:36:08 -> 07:36:12]  everything is uh synced up you know we
[07:36:10 -> 07:36:14]  make sure that all these are done um you
[07:36:12 -> 07:36:16]  know as we're iterating through all
[07:36:14 -> 07:36:18]  these all of these block tiles right we
[07:36:16 -> 07:36:21]  have to go like on the on the bigger
[07:36:18 -> 07:36:24]  matrices A and B we actually have to
[07:36:21 -> 07:36:26]  take these tiles and we have to uh move
[07:36:24 -> 07:36:27]  them closer together right so this is
[07:36:26 -> 07:36:29]  what this whole Loop is doing we want to
[07:36:27 -> 07:36:34]  make sure everything is synced up both
[07:36:29 -> 07:36:36]  after the uh shared memory um population
[07:36:34 -> 07:36:38]  so after we we we populate those we want
[07:36:36 -> 07:36:39]  to sync everything and then once we've
[07:36:38 -> 07:36:42]  written once we've written all the
[07:36:39 -> 07:36:43]  results here we also want to sync
[07:36:42 -> 07:36:45]  everything up make sure all the threads
[07:36:43 -> 07:36:48]  are caught up before we you know evolve
[07:36:45 -> 07:36:51]  to the next one and mess with stuff um
[07:36:48 -> 07:36:52]  so then we have this write out um and
[07:36:51 -> 07:36:56]  this isn't actually too bad this part's
[07:36:52 -> 07:36:58]  pretty good so inside of here we iterate
[07:36:56 -> 07:37:00]  over the same things that we did here
[07:36:58 -> 07:37:02]  when we were actually calculating uh
[07:37:00 -> 07:37:05]  when we were we were multiplying those
[07:37:02 -> 07:37:07]  those thread rows and columns those
[07:37:05 -> 07:37:11]  little thread
[07:37:07 -> 07:37:12]  tiles and if we just step through this
[07:37:11 -> 07:37:14]  this is actually going to seem a little
[07:37:12 -> 07:37:17]  bit weird at first but if we scroll up
[07:37:14 -> 07:37:20]  remember that we advance everything to
[07:37:17 -> 07:37:23]  the starting position given this thread
[07:37:20 -> 07:37:24]  right or given this block rather so we
[07:37:23 -> 07:37:27]  have these initial terms which are the
[07:37:24 -> 07:37:30]  blocks these are which tiles we which
[07:37:27 -> 07:37:31]  tile we actually care about within C
[07:37:30 -> 07:37:32]  and this is already stored here so we
[07:37:31 -> 07:37:34]  already know which tile we actually want
[07:37:32 -> 07:37:36]  to worry about and the the memory
[07:37:34 -> 07:37:38]  address has gone through it's skipped a
[07:37:36 -> 07:37:41]  bunch of
[07:37:38 -> 07:37:42]  spaces through just like integer
[07:37:41 -> 07:37:43]  operations it's been multiplied and
[07:37:42 -> 07:37:45]  added up to the point where it's where
[07:37:43 -> 07:37:47]  we want it and then we just do
[07:37:45 -> 07:37:50]  everything from there so we can if we
[07:37:47 -> 07:37:51]  stride like an entire length K it'll go
[07:37:50 -> 07:37:53]  here and then the remainder of it and
[07:37:51 -> 07:37:56]  it'll end up back to where it is but
[07:37:53 -> 07:37:57]  just like one one element lower right so
[07:37:56 -> 07:38:00]  that that's really all we're working
[07:37:57 -> 07:38:02]  with here now if we scroll back down
[07:38:00 -> 07:38:05]  it's literally just like this end term
[07:38:02 -> 07:38:07]  that's that's like the end term um that
[07:38:05 -> 07:38:08]  that's the that's the horizontal part
[07:38:07 -> 07:38:11]  right so that's the part that we're
[07:38:08 -> 07:38:12]  actually striding over so if we have um
[07:38:11 -> 07:38:17]  we're looking at the C
[07:38:12 -> 07:38:20]  Matrix we have thread row times thread M
[07:38:17 -> 07:38:23]  right or or TM which is
[07:38:20 -> 07:38:26]  8 plus the res index remember we it's
[07:38:23 -> 07:38:27]  the same same idea as as uh as what we
[07:38:26 -> 07:38:30]  did up
[07:38:27 -> 07:38:32]  there and so
[07:38:30 -> 07:38:35]  when we are
[07:38:32 -> 07:38:38]  um it's essentially the same as this
[07:38:35 -> 07:38:41]  except we're doing I instead so you know
[07:38:38 -> 07:38:44]  I is you could think of i as like up to
[07:38:41 -> 07:38:45]  um you know TM it's like the same idea
[07:38:44 -> 07:38:49]  we're just iterating through that that's
[07:38:45 -> 07:38:52]  going to be the um offset inside of that
[07:38:49 -> 07:38:56]  tile right so like relative to
[07:38:52 -> 07:38:58]  um relative to the actual SE tow that
[07:38:56 -> 07:39:01]  we're working on this is going to be
[07:38:58 -> 07:39:02]  like the relative off set right um so
[07:39:01 -> 07:39:05]  that's going to be
[07:39:02 -> 07:39:09]  downwards time n right so that's going
[07:39:05 -> 07:39:11]  to give us our our downwards um our
[07:39:09 -> 07:39:15]  downwards movement and then to progress
[07:39:11 -> 07:39:18]  sideways we have the red column time TN
[07:39:15 -> 07:39:21]  right so an individual thread um
[07:39:18 -> 07:39:24]  individual thread
[07:39:21 -> 07:39:26]  times times that uh that TN length of
[07:39:24 -> 07:39:29]  eight right so essentially we're going
[07:39:26 -> 07:39:32]  to have a bunch of uh a bunch of threads
[07:39:29 -> 07:39:34]  occupying like a square and those
[07:39:32 -> 07:39:37]  threads are going
[07:39:34 -> 07:39:38]  to um or no not not the threads
[07:39:37 -> 07:39:40]  occupying a square but they're going to
[07:39:38 -> 07:39:42]  occupy the whole thing and then threads
[07:39:40 -> 07:39:45]  are each thread is going to iterate
[07:39:42 -> 07:39:46]  through that TM and TN right and so we
[07:39:45 -> 07:39:49]  have this this vertical offset we have
[07:39:46 -> 07:39:51]  the horizontal offset and then plus that
[07:39:49 -> 07:39:53]  additional little kick to the right
[07:39:51 -> 07:39:54]  we're going to iter this is how much we
[07:39:53 -> 07:39:57]  need to actually like stride over like
[07:39:54 -> 07:39:59]  how big steps we we take um and then the
[07:39:57 -> 07:40:01]  initial just like inside of that inside
[07:39:59 -> 07:40:02]  of like one of these steps it's like how
[07:40:01 -> 07:40:05]  much do you actually go forward right
[07:40:02 -> 07:40:07]  how much do you do you add to
[07:40:05 -> 07:40:10]  it and then we just use what we've
[07:40:07 -> 07:40:12]  already computed so thread results um
[07:40:10 -> 07:40:15]  same indexing scheme here this should be
[07:40:12 -> 07:40:17]  fairly intuitive um we just multiply
[07:40:15 -> 07:40:19]  this element wise know for each
[07:40:17 -> 07:40:22]  iteration in the loop and then we have
[07:40:19 -> 07:40:24]  our beta times you know element wise
[07:40:22 -> 07:40:27]  this which is literally the same same
[07:40:24 -> 07:40:29]  indexing scheme that we use here so
[07:40:27 -> 07:40:31]  hopefully the the block tent kernel
[07:40:29 -> 07:40:33]  sense now we're going to jump into well
[07:40:31 -> 07:40:36]  actually before we actually jump into
[07:40:33 -> 07:40:41]  the uh vectorized kernel I want to run
[07:40:36 -> 07:40:44]  this so if we go sjem we go sjem 04
[07:40:41 -> 07:40:48]  right this is this is block ping
[07:40:44 -> 07:40:50]  normal this is regular 1D block tiling
[07:40:48 -> 07:40:55]  right then if we step this up to number
[07:40:50 -> 07:40:58]  five look at this 4800 gig flops on 496
[07:40:55 -> 07:41:00]  right that's decent but if we step it up
[07:40:58 -> 07:41:03]  to five
[07:41:00 -> 07:41:09]  we double
[07:41:03 -> 07:41:14]  it so if we do uh python we go at
[07:41:09 -> 07:41:14]  9162 /
[07:41:14 -> 07:41:21]  4873 got about 1.9x speed up right which
[07:41:17 -> 07:41:23]  is really good now we compare to the
[07:41:21 -> 07:41:24]  results
[07:41:23 -> 07:41:30]  here
[07:41:24 -> 07:41:32]  um so about about this time 1.9 roughly
[07:41:30 -> 07:41:33]  is it's pretty close to 70 right so I
[07:41:32 -> 07:41:36]  mean we're we're essentially getting the
[07:41:33 -> 07:41:39]  same results here uh so everything is
[07:41:36 -> 07:41:42]  working out we just doubled the speed by
[07:41:39 -> 07:41:44]  using 2D block tiling instead of 1D and
[07:41:42 -> 07:41:46]  now we're already about 2/3 of the way
[07:41:44 -> 07:41:49]  to kuo Performance we're actually really
[07:41:46 -> 07:41:51]  really high up there so now let's go
[07:41:49 -> 07:41:52]  ahead and continue with vectorized
[07:41:51 -> 07:41:55]  memory access which is going to give us
[07:41:52 -> 07:41:58]  an additional little performance boost
[07:41:55 -> 07:41:59]  okay awesome so now we have this new
[07:41:58 -> 07:42:02]  colel to worry about
[07:41:59 -> 07:42:07]  number six on vectorizing the shared
[07:42:02 -> 07:42:10]  memory loads so if we look inside of
[07:42:07 -> 07:42:12]  here essentially we do this we
[07:42:10 -> 07:42:16]  everything in here Remains the
[07:42:12 -> 07:42:19]  Same except we have this new float 4
[07:42:16 -> 07:42:20]  type right so notice how we have this
[07:42:19 -> 07:42:22]  this float 4 I'm going to just highlight
[07:42:20 -> 07:42:25]  that and see where it shows up so it
[07:42:22 -> 07:42:28]  shows up here when we're loading
[07:42:25 -> 07:42:30]  into uh when we are loading into shared
[07:42:28 -> 07:42:32]  memory right so a shared and B shared
[07:42:30 -> 07:42:35]  right this is where it comes up as well
[07:42:32 -> 07:42:37]  as when we're writing out the results so
[07:42:35 -> 07:42:40]  when we're going in from Global vram
[07:42:37 -> 07:42:42]  into shared memory we use float for
[07:42:40 -> 07:42:46]  loads and then when we're writing out
[07:42:42 -> 07:42:47]  from registers we load uh we load with
[07:42:46 -> 07:42:50]  float FL as well right this is what's
[07:42:47 -> 07:42:52]  happening here now before diving in
[07:42:50 -> 07:42:53]  right into this I want to go over and
[07:42:52 -> 07:42:55]  review just like what the heck this
[07:42:53 -> 07:42:56]  float board does there's a lot of terms
[07:42:55 -> 07:42:58]  like reinterpret cast and all this all
[07:42:56 -> 07:43:00]  this weird symbols and stuff so let's
[07:42:58 -> 07:43:02]  just like go and clarify what this all
[07:43:00 -> 07:43:04]  means I wrote a separate file here
[07:43:02 -> 07:43:05]  called float for. cuu you can write the
[07:43:04 -> 07:43:08]  same thing but I'm just going to go over
[07:43:05 -> 07:43:12]  this kind of Step by Step so we have an
[07:43:08 -> 07:43:15]  array length n right 1 3 4 all
[07:43:12 -> 07:43:18]  floats we have a host input host output
[07:43:15 -> 07:43:22]  we initialize device input and output we
[07:43:18 -> 07:43:24]  K like those with n * size of float we
[07:43:22 -> 07:43:26]  kud M Copy that from host to
[07:43:24 -> 07:43:29]  device host to
[07:43:26 -> 07:43:31]  device we we you launch this with a
[07:43:29 -> 07:43:33]  uh with a grid size of one there's a
[07:43:31 -> 07:43:34]  single block and within that block
[07:43:33 -> 07:43:37]  there's a single thread so it's just one
[07:43:34 -> 07:43:39]  thread that's actually being used here
[07:43:37 -> 07:43:41]  now we run this and then we copy back
[07:43:39 -> 07:43:45]  and then we display these based on their
[07:43:41 -> 07:43:47]  indices right so 0 1 2 3 then 0 1 2 3
[07:43:45 -> 07:43:49]  the inputs and the outputs and now what
[07:43:47 -> 07:43:51]  actually happens inside of here is what
[07:43:49 -> 07:43:52]  we want to pay attention to so we pass
[07:43:51 -> 07:43:55]  this this device input and the device
[07:43:52 -> 07:43:59]  output in right these are Pointers two
[07:43:55 -> 07:44:01]  arrays um output as well now we have idx
[07:43:59 -> 07:44:03]  which thread index in this case is going
[07:44:01 -> 07:44:06]  to be zero right it goes from like zero
[07:44:03 -> 07:44:09]  up to whatever the length is minus one
[07:44:06 -> 07:44:12]  and so this is just going to be zero so
[07:44:09 -> 07:44:13]  when we actually look at this this idx
[07:44:12 -> 07:44:16]  pay attention don't don't worry about
[07:44:13 -> 07:44:21]  this yet this idx is going to be Time 4
[07:44:16 -> 07:44:24]  so 0 * 4 0 * 4 0 * 4 0 * 4 it's just
[07:44:21 -> 07:44:27]  going to be 0 1 2 and three
[07:44:24 -> 07:44:29]  right these are going to be our x y z
[07:44:27 -> 07:44:31]  and W components now let's actually look
[07:44:29 -> 07:44:33]  at what's happening here so this new
[07:44:31 -> 07:44:35]  float 4 type it's part of the it's part
[07:44:33 -> 07:44:37]  of the Cuda runtime it's part of the
[07:44:35 -> 07:44:39]  well it's not part of the Cuda runtime
[07:44:37 -> 07:44:43]  if we actually look at this it's part of
[07:44:39 -> 07:44:45]  um x86 right Vector types we have this
[07:44:43 -> 07:44:46]  float 4 we have a bunch of different
[07:44:45 -> 07:44:49]  other Vector types that are device
[07:44:46 -> 07:44:52]  built-ins so we can't actually um we
[07:44:49 -> 07:44:53]  cannot actually uh see what these are
[07:44:52 -> 07:44:56]  under the hood they just kind of work
[07:44:53 -> 07:44:58]  for us U if I try to click on these
[07:44:56 -> 07:45:00]  right it's just like that's really all
[07:44:58 -> 07:45:03]  it is
[07:45:00 -> 07:45:06]  so there's Parts in here a lot of this
[07:45:03 -> 07:45:08]  is built in handled by the compiler Etc
[07:45:06 -> 07:45:11]  and so when we break down what's
[07:45:08 -> 07:45:13]  happening here um we notice that we have
[07:45:11 -> 07:45:16]  a few parts we have this reinterpret
[07:45:13 -> 07:45:21]  cast and Lally all this means is that
[07:45:16 -> 07:45:23]  we're going to reinterpret as this uh in
[07:45:21 -> 07:45:25]  in the actual instructions right so this
[07:45:23 -> 07:45:27]  isn't going to manipulate memory or do
[07:45:25 -> 07:45:29]  any data Transformations it's literally
[07:45:27 -> 07:45:32]  just going to uh
[07:45:29 -> 07:45:33]  reinterpret as a float 4 that's that's
[07:45:32 -> 07:45:35]  what the it's going to tell the compiler
[07:45:33 -> 07:45:39]  what to do right and this is going to be
[07:45:35 -> 07:45:42]  a pointer to float 4 right so we're
[07:45:39 -> 07:45:45]  we're transforming uh this essentially
[07:45:42 -> 07:45:48]  so this is a memory address this ENT is
[07:45:45 -> 07:45:49]  a memory address to whichever index
[07:45:48 -> 07:45:53]  we're looking at so in this
[07:45:49 -> 07:45:57]  case it's going to be you know idx * 4
[07:45:53 -> 07:45:59]  in this case this is just going to be um
[07:45:57 -> 07:46:02]  this is just going to be like idx is z
[07:45:59 -> 07:46:04]  so it's going to be 0 uh 0 * 4 that's
[07:46:02 -> 07:46:08]  just the beginning element right that's
[07:46:04 -> 07:46:10]  literally all all this is is just um
[07:46:08 -> 07:46:13]  we're we're doing input at index zero
[07:46:10 -> 07:46:14]  and then we're uh we're getting the
[07:46:13 -> 07:46:16]  memory address for that so it's the
[07:46:14 -> 07:46:18]  memory address so the first element in
[07:46:16 -> 07:46:20]  that entire array and then there's the
[07:46:18 -> 07:46:25]  following memory addresses for for the
[07:46:20 -> 07:46:28]  extra ones and then we uh we we we we
[07:46:25 -> 07:46:30]  reinterpret this as a float for pointer
[07:46:28 -> 07:46:32]  so we got the pointer the the memory
[07:46:30 -> 07:46:33]  address here and we're reinterpreting
[07:46:32 -> 07:46:35]  this as a float 4 pointer so we're just
[07:46:33 -> 07:46:38]  going from pointer as a float to pointer
[07:46:35 -> 07:46:41]  as a float 4 and the float 4 uh is just
[07:46:38 -> 07:46:43]  going to contain essentially the
[07:46:41 -> 07:46:45]  starting index plus an additional uh
[07:46:43 -> 07:46:47]  three afterwards
[07:46:45 -> 07:46:52]  right um and then we just have this
[07:46:47 -> 07:46:53]  stored as as uh index zero uh for this
[07:46:52 -> 07:46:55]  specific data type we don't want to be
[07:46:53 -> 07:46:57]  you know redundant and take up extra
[07:46:55 -> 07:46:58]  space so we're just going to index zero
[07:46:57 -> 07:47:00]  and then the compiler is going to know
[07:46:58 -> 07:47:03]  what to do with that later on um it's
[07:47:00 -> 07:47:05]  just literally just going to be thex
[07:47:03 -> 07:47:08]  component is going to be index0 Y is
[07:47:05 -> 07:47:09]  going to be index one Z is two and then
[07:47:08 -> 07:47:11]  W is three right so that's that's
[07:47:09 -> 07:47:14]  literally how we have it laid out how we
[07:47:11 -> 07:47:15]  have this thing laid out here and that's
[07:47:14 -> 07:47:17]  literally all the all the float 4 data
[07:47:15 -> 07:47:20]  type is so hope that that kind of makes
[07:47:17 -> 07:47:22]  sense to you um you know when you read
[07:47:20 -> 07:47:24]  complex uh when you read kind of like
[07:47:22 -> 07:47:26]  complicated expressions like this it's
[07:47:24 -> 07:47:27]  good to just break it down step by step
[07:47:26 -> 07:47:29]  right so you have this like you have
[07:47:27 -> 07:47:30]  this thing with its open and and it's
[07:47:29 -> 07:47:31]  closed and then you have this thing with
[07:47:30 -> 07:47:33]  it's open and it's closed then this
[07:47:31 -> 07:47:35]  thing with it's open and it's closed you
[07:47:33 -> 07:47:37]  just kind of see like you do like your
[07:47:35 -> 07:47:39]  order of operations or simplified
[07:47:37 -> 07:47:41]  however you want but yeah hopefully that
[07:47:39 -> 07:47:43]  makes sense and if we can uh I'm
[07:47:41 -> 07:47:46]  actually going
[07:47:43 -> 07:47:46]  to
[07:47:48 -> 07:47:57]  uh oh sjm CD into Source kernels and
[07:47:53 -> 07:48:00]  then if we go and compile that into
[07:47:57 -> 07:48:03]  float 4 we notice that we get everything
[07:48:00 -> 07:48:05]  as expected right so 1 2 3 4 that is the
[07:48:03 -> 07:48:08]  poost input so that's how we initialized
[07:48:05 -> 07:48:10]  it here um and then the host output
[07:48:08 -> 07:48:13]  which is um exactly how we want
[07:48:10 -> 07:48:14]  everything to be stored um that is all
[07:48:13 -> 07:48:17]  checking
[07:48:14 -> 07:48:19]  out now this next kernel is really fun
[07:48:17 -> 07:48:20]  uh it it plays around with some things
[07:48:19 -> 07:48:22]  that are usually played around with and
[07:48:20 -> 07:48:25]  toyed around with when you write really
[07:48:22 -> 07:48:29]  really uh performance optimal crud
[07:48:25 -> 07:48:31]  kernels right so the idea here is like
[07:48:29 -> 07:48:36]  remember how when I showed you before
[07:48:31 -> 07:48:38]  those uh we go back to here and uh I'm
[07:48:36 -> 07:48:42]  going to step
[07:48:38 -> 07:48:47]  out and go to SRC kernels and then we
[07:48:42 -> 07:48:47]  did the we did this one and we saw
[07:48:55 -> 07:49:03]  um we go up we saw the these um like
[07:49:00 -> 07:49:06]  this load this this load instruction the
[07:49:03 -> 07:49:09]  load. E and then there was like some
[07:49:06 -> 07:49:12]  load uh 128 so like load 32s and then
[07:49:09 -> 07:49:15]  load 128s right so that that's like the
[07:49:12 -> 07:49:17]  whole that's the whole deal here is
[07:49:15 -> 07:49:18]  we're going to try to make more of these
[07:49:17 -> 07:49:21]  load
[07:49:18 -> 07:49:26]  128s a single floating Point number is
[07:49:21 -> 07:49:29]  32 bits and so if we um if we if we make
[07:49:26 -> 07:49:33]  this like a vector type meaning
[07:49:29 -> 07:49:35]  you know we just put multiple numbers
[07:49:33 -> 07:49:37]  with each other in the same type then we
[07:49:35 -> 07:49:38]  can have more and we can load more
[07:49:37 -> 07:49:41]  things
[07:49:38 -> 07:49:43]  so let's go ahead and actually jump back
[07:49:41 -> 07:49:45]  to uh this part here and let's explain
[07:49:43 -> 07:49:47]  like what the heck we're
[07:49:45 -> 07:49:50]  doing
[07:49:47 -> 07:49:54]  so in this one we are
[07:49:50 -> 07:49:56]  essentially uh we're essentially taking
[07:49:54 -> 07:49:59]  the a tile so it's like normally
[07:49:56 -> 07:50:00]  vertical so it would be like tilted
[07:49:59 -> 07:50:03]  downwards like the bottom would go here
[07:50:00 -> 07:50:05]  and then this part would go right there
[07:50:03 -> 07:50:08]  we're just transposing it
[07:50:05 -> 07:50:10]  right and this transposing is going to
[07:50:08 -> 07:50:12]  let us cheat a little bit um we still
[07:50:10 -> 07:50:13]  get the same amount of memory in that
[07:50:12 -> 07:50:15]  shared block except we index it
[07:50:13 -> 07:50:19]  differently and this will allow us to
[07:50:15 -> 07:50:23]  coass memory accesses right so normally
[07:50:19 -> 07:50:25]  we would um if we go back up we would be
[07:50:23 -> 07:50:29]  taking these and we would be advancing
[07:50:25 -> 07:50:31]  to the right here but notice how
[07:50:29 -> 07:50:34]  um for example like when we're actually
[07:50:31 -> 07:50:36]  loading this in uh our threads are going
[07:50:34 -> 07:50:38]  to be loading in um from like top to
[07:50:36 -> 07:50:40]  bottom right so we're going to have like
[07:50:38 -> 07:50:42]  a like threads essentially organized in
[07:50:40 -> 07:50:45]  in different pieces and we're going to
[07:50:42 -> 07:50:46]  be iterating downwards to populate this
[07:50:45 -> 07:50:51]  right if you remember how we populated
[07:50:46 -> 07:50:55]  it before um these axises were not CEST
[07:50:51 -> 07:51:00]  but uh the B the B tile the B shared
[07:50:55 -> 07:51:03]  tile um was CEST because our our threads
[07:51:00 -> 07:51:05]  were uh loading horizontally so when our
[07:51:03 -> 07:51:07]  threads we need like thread one thread 2
[07:51:05 -> 07:51:08]  thread three thread four those are all
[07:51:07 -> 07:51:11]  next to each other so Cuda is going to
[07:51:08 -> 07:51:13]  go ahead and load those in um as a like
[07:51:11 -> 07:51:14]  if you have four of them adjacent to
[07:51:13 -> 07:51:16]  each other it's going to load all those
[07:51:14 -> 07:51:18]  in as a single load operation instead of
[07:51:16 -> 07:51:21]  four separate ones so that's what we're
[07:51:18 -> 07:51:23]  aiming for here and the idea here is
[07:51:21 -> 07:51:25]  instead of advancing instead of having
[07:51:23 -> 07:51:28]  this vertical tile and then taking this
[07:51:25 -> 07:51:32]  and advancing to the and like having idx
[07:51:28 -> 07:51:33]  to the right it's going to be flipped
[07:51:32 -> 07:51:35]  and we're going to iterate downward so
[07:51:33 -> 07:51:37]  our threads are going to be paired like
[07:51:35 -> 07:51:39]  this next to each other and it's going
[07:51:37 -> 07:51:41]  to iterate downward right that's the
[07:51:39 -> 07:51:44]  whole idea there um but what's even more
[07:51:41 -> 07:51:45]  important isn't even how uh they iterate
[07:51:44 -> 07:51:49]  downward that's just what the graphic
[07:51:45 -> 07:51:52]  looks like what it's more so about is
[07:51:49 -> 07:51:55]  how we actually um are able to load from
[07:51:52 -> 07:51:57]  Global into shared right so how do we
[07:51:55 -> 07:51:59]  take the a how do we take the a tile
[07:51:57 -> 07:52:02]  just on its own and then load that into
[07:51:59 -> 07:52:06]  shared and then populate it like this
[07:52:02 -> 07:52:08]  right so we pop back to here we notice a
[07:52:06 -> 07:52:11]  few things so first of all this is the
[07:52:08 -> 07:52:14]  same this is the same this is the same
[07:52:11 -> 07:52:17]  but these and these are different right
[07:52:14 -> 07:52:19]  so if we pop down here just pay
[07:52:17 -> 07:52:22]  attention to the inner rows and columns
[07:52:19 -> 07:52:23]  for now so notice here how that this
[07:52:22 -> 07:52:25]  this is actually very familiar but
[07:52:23 -> 07:52:27]  notice how we don't actually have that
[07:52:25 -> 07:52:30]  Loop so if we go back to uh block tiling
[07:52:27 -> 07:52:32]  for example this two block tiling we
[07:52:30 -> 07:52:36]  were loading uh in four Loops so we had
[07:52:32 -> 07:52:38]  um BM was 128 long and then this load
[07:52:36 -> 07:52:40]  offset would increase an increments of
[07:52:38 -> 07:52:42]  32 and it would do four different
[07:52:40 -> 07:52:44]  iterations of the four Loops totaling
[07:52:42 -> 07:52:46]  totaling eight different iterations per
[07:52:44 -> 07:52:49]  thread so each thread was doing eight
[07:52:46 -> 07:52:50]  different instructions or I guess at the
[07:52:49 -> 07:52:53]  high level eight different instructions
[07:52:50 -> 07:52:55]  for uh for moving data
[07:52:53 -> 07:52:57]  around and that's like kind of a
[07:52:55 -> 07:52:59]  bottleneck a little bit so we can
[07:52:57 -> 07:53:01]  actually speed that up and notice here
[07:52:59 -> 07:53:02]  we don't actually have any Loops it's
[07:53:01 -> 07:53:04]  literally just we store this temp
[07:53:02 -> 07:53:07]  variable um this is doing this is done
[07:53:04 -> 07:53:09]  once per thread by the way so this is
[07:53:07 -> 07:53:13]  like once per thread um but we
[07:53:09 -> 07:53:16]  essentially we go into we go into a we
[07:53:13 -> 07:53:20]  get this inner row and we essentially
[07:53:16 -> 07:53:24]  just we essentially just find um where
[07:53:20 -> 07:53:26]  exactly uh where exactly this is at and
[07:53:24 -> 07:53:29]  we're going to load this into shared
[07:53:26 -> 07:53:30]  memory as if it's transposed right
[07:53:29 -> 07:53:32]  so for
[07:53:30 -> 07:53:35]  example I have this thing written on the
[07:53:32 -> 07:53:37]  board here this is the a tile and I drew
[07:53:35 -> 07:53:39]  a little section at the bottom with 1 2
[07:53:37 -> 07:53:41]  3 and four these are four different
[07:53:39 -> 07:53:44]  values that we're going to count as a
[07:53:41 -> 07:53:46]  float four all right so when we actually
[07:53:44 -> 07:53:49]  transpose this when we do a tile then
[07:53:46 -> 07:53:52]  say t you might not be able to see all
[07:53:49 -> 07:53:54]  those just don't worry about it um we
[07:53:52 -> 07:53:55]  transposes and essentially we're like
[07:53:54 -> 07:53:59]  flipping
[07:53:55 -> 07:54:01]  over over this this dotted line here so
[07:53:59 -> 07:54:04]  these are going to flip over they're
[07:54:01 -> 07:54:08]  going to go from this
[07:54:04 -> 07:54:10]  to this right those are going to flip
[07:54:08 -> 07:54:12]  across that
[07:54:10 -> 07:54:17]  line and they're going to end up like
[07:54:12 -> 07:54:17]  that what you're going to have
[07:54:19 -> 07:54:24]  is you're going to have when these when
[07:54:21 -> 07:54:29]  these flip over they're going to be
[07:54:24 -> 07:54:29]  ordered one two
[07:54:29 -> 07:54:37]  I'm sorry bottom bottom I one two three
[07:54:39 -> 07:54:44]  four
[07:54:41 -> 07:54:47]  now don't worry too much about how these
[07:54:44 -> 07:54:49]  are in uh how these are in like a column
[07:54:47 -> 07:54:52]  format what we do care about is just
[07:54:49 -> 07:54:55]  that we're loading these in a coest
[07:54:52 -> 07:54:57]  manner right so notice how I've kind of
[07:54:55 -> 07:54:59]  Taken like half of this tile here and
[07:54:57 -> 07:55:03]  I've actually done this for a reason so
[07:54:59 -> 07:55:05]  if we if we go ahead and look back at
[07:55:03 -> 07:55:08]  what these inner rows and all this stuff
[07:55:05 -> 07:55:11]  is saying it's essentially the same as
[07:55:08 -> 07:55:14]  what we had um over over here in 2D
[07:55:11 -> 07:55:18]  block tiling except instead of just BN
[07:55:14 -> 07:55:20]  or BK it's that and then divide by four
[07:55:18 -> 07:55:23]  right so I mean it's it's four for a
[07:55:20 -> 07:55:26]  reason it's literally because we're just
[07:55:23 -> 07:55:29]  we're just um we're using the float 4
[07:55:26 -> 07:55:31]  type so if we're going to do uh thread
[07:55:29 -> 07:55:33]  idx which in this case is going to max
[07:55:31 -> 07:55:38]  out at
[07:55:33 -> 07:55:44]  20 it's going to max out at um say
[07:55:38 -> 07:55:50]  255 and then BK is 8 so it's going to be
[07:55:44 -> 07:55:52]  8 / 4 which is 255 / 2 which gives us
[07:55:50 -> 07:55:57]  the indices 0 to
[07:55:52 -> 07:55:57]  127 now this one on the other hand
[07:55:58 -> 07:56:05]  is going to be um same idea except we're
[07:56:01 -> 07:56:07]  doing the mod right so 255 mod 2 that
[07:56:05 -> 07:56:08]  means every two times it's going to
[07:56:07 -> 07:56:12]  reset at zero again so we're just going
[07:56:08 -> 07:56:14]  to have the two numbers zero and
[07:56:12 -> 07:56:17]  one essentially this indexing allows us
[07:56:14 -> 07:56:19]  to treat this as a group of four so when
[07:56:17 -> 07:56:21]  we divide that means it's shrinking to a
[07:56:19 -> 07:56:23]  fourth of its length and then when we go
[07:56:21 -> 07:56:24]  down here and multiply by four again
[07:56:23 -> 07:56:26]  which you'll kind of see the intuition
[07:56:24 -> 07:56:28]  for in a second and when we multiply by
[07:56:26 -> 07:56:29]  four again it's just going to stretch
[07:56:28 -> 07:56:32]  that back out to what it normally used
[07:56:29 -> 07:56:34]  to be so this is just considering uh
[07:56:32 -> 07:56:36]  this little Flo float for indexing
[07:56:34 -> 07:56:40]  scheme we have now if we pop down to
[07:56:36 -> 07:56:43]  this here uh we can see that this I'm
[07:56:40 -> 07:56:46]  actually purposely making this um match
[07:56:43 -> 07:56:49]  our a tile here so notice how we have
[07:56:46 -> 07:56:53]  this inner row a right and inner row a
[07:56:49 -> 07:56:57]  is between 0 and 127 so the row for this
[07:56:53 -> 07:57:00]  tile is between zero let me put this
[07:56:57 -> 07:57:00]  here for now
[07:57:02 -> 07:57:08]  Z and 127 right so you might not be able
[07:57:05 -> 07:57:11]  to see that but 0 127 this is very long
[07:57:08 -> 07:57:14]  this is like the longer side um and then
[07:57:11 -> 07:57:20]  we have 0 to one so two values for the
[07:57:14 -> 07:57:26]  for the column right column A so 0 to
[07:57:20 -> 07:57:29]  one or or one rather and all this allows
[07:57:26 -> 07:57:33]  us to do is just sort things more easily
[07:57:29 -> 07:57:38]  so notice how we have like uh we have
[07:57:33 -> 07:57:40]  this thing that's normally of size
[07:57:38 -> 07:57:43]  128 by
[07:57:40 -> 07:57:47]  8 and 128 by 8 if we actually do the
[07:57:43 -> 07:57:47]  multiplication for that
[07:57:48 -> 07:57:52]  um that's 1,24
[07:57:54 -> 07:58:00]  right so
[07:57:56 -> 07:58:03]  1,24 and and then reduce this number we
[07:58:00 -> 07:58:06]  divide this one by four
[07:58:03 -> 07:58:10]  right that actually reduces the whole
[07:58:06 -> 07:58:10]  thing from 1024 to
[07:58:10 -> 07:58:18]  256 guess how many threads we have 256
[07:58:14 -> 07:58:21]  it works out perfectly so each thread is
[07:58:18 -> 07:58:24]  essentially taking its own little one of
[07:58:21 -> 07:58:26]  these right so this is going this is
[07:58:24 -> 07:58:29]  spanning down 127 0 to 127 and this is
[07:58:26 -> 07:58:31]  going 01 and which just like split in
[07:58:29 -> 07:58:36]  half right so you have you have like 1 2
[07:58:31 -> 07:58:37]  3 4 5 6 7 right eight and we're just
[07:58:36 -> 07:58:39]  like splitting this in half and then
[07:58:37 -> 07:58:42]  it's it's spanning the length uh
[07:58:39 -> 07:58:45]  spanning spanning the height downwards
[07:58:42 -> 07:58:47]  right and all we have to do is just
[07:58:45 -> 07:58:49]  store this as a float so this is what we
[07:58:47 -> 07:58:52]  do here we we literally uh take the
[07:58:49 -> 07:58:54]  inner row we do that times case we need
[07:58:52 -> 07:58:56]  to you know stride around and and get
[07:58:54 -> 07:58:59]  back to the same to the same uh to the
[07:58:56 -> 07:59:01]  same column again and then we do plus
[07:58:59 -> 07:59:04]  the inner column A offset this is all in
[07:59:01 -> 07:59:06]  context of like this a tile being
[07:59:04 -> 07:59:08]  Advanced where it needs to be so we're
[07:59:06 -> 07:59:09]  literally just adding K considering that
[07:59:08 -> 07:59:11]  we're in this bigger Matrix right that's
[07:59:09 -> 07:59:13]  all we need k for everything else is it
[07:59:11 -> 07:59:15]  it just works because we're in the we're
[07:59:13 -> 07:59:18]  just dealing relative to this specific
[07:59:15 -> 07:59:21]  tile as a part of the whole bigger
[07:59:18 -> 07:59:23]  Matrix now inner column we're actually
[07:59:21 -> 07:59:27]  going to stretch this out back to four
[07:59:23 -> 07:59:28]  again because uh because we previously
[07:59:27 -> 07:59:30]  shrunk it right so we just need to
[07:59:28 -> 07:59:33]  expand that back
[07:59:30 -> 07:59:35]  again and this is just going to index um
[07:59:33 -> 07:59:39]  in the old fashion style right where we
[07:59:35 -> 07:59:42]  where we find the the vertical offset
[07:59:39 -> 07:59:46]  meaning uh meaning here and then we add
[07:59:42 -> 07:59:49]  that so vertical offset and then we plus
[07:59:46 -> 07:59:52]  to the so we like we find which row we
[07:59:49 -> 07:59:54]  want we multiply by the by the K term
[07:59:52 -> 07:59:56]  and then we get where we want to get and
[07:59:54 -> 07:59:58]  then we Plus for the horizontal offset
[07:59:56 -> 08:00:01]  right and we ex we have to multiply by
[07:59:58 -> 08:00:03]  four to like to actually uh span that
[08:00:01 -> 08:00:07]  entire length right so if you wanted to
[08:00:03 -> 08:00:08]  go to the very end of The Matrix um if
[08:00:07 -> 08:00:10]  you want to go to the very end of The
[08:00:08 -> 08:00:13]  Matrix here you'd actually have to take
[08:00:10 -> 08:00:14]  this two term or whatever that is times
[08:00:13 -> 08:00:16]  four and it would it would get you all
[08:00:14 -> 08:00:18]  the way to the end right that's that's
[08:00:16 -> 08:00:21]  the whole idea there um but if we go
[08:00:18 -> 08:00:25]  into this look at the way we store these
[08:00:21 -> 08:00:26]  right this isn't actually too bad so in
[08:00:25 -> 08:00:28]  a shared we have these four different
[08:00:26 -> 08:00:30]  they have these four different stor
[08:00:28 -> 08:00:33]  which is each four different component
[08:00:30 -> 08:00:36]  of the float 4 variable so we have this
[08:00:33 -> 08:00:40]  x y z and W term this is the first one
[08:00:36 -> 08:00:44]  index zero and then index one index 2
[08:00:40 -> 08:00:46]  Etc right so all we're doing there is
[08:00:44 -> 08:00:48]  we're looking at the inner column
[08:00:46 -> 08:00:51]  whichever column that is keep in mind we
[08:00:48 -> 08:00:52]  have this new tile of this shape we're
[08:00:51 -> 08:00:54]  doing it relative to this because this
[08:00:52 -> 08:00:58]  is how we've this is how we're storing
[08:00:54 -> 08:00:59]  everything so we're going to go um the
[08:00:58 -> 08:01:02]  inner
[08:00:59 -> 08:01:05]  column whichever that is so in this case
[08:01:02 -> 08:01:07]  the the column we have to keep in
[08:01:05 -> 08:01:10]  context how we stored column for this
[08:01:07 -> 08:01:14]  one the column in this case is which one
[08:01:10 -> 08:01:17]  of these but since we
[08:01:14 -> 08:01:21]  um since we transposed it we have to
[08:01:17 -> 08:01:23]  consider the column in the context of
[08:01:21 -> 08:01:26]  here right so it's actually a little bit
[08:01:23 -> 08:01:28]  different so instead of having row as
[08:01:26 -> 08:01:29]  this it's actually column because we
[08:01:28 -> 08:01:30]  interpreted column from this originally
[08:01:29 -> 08:01:32]  that that's kind of what's going on
[08:01:30 -> 08:01:34]  there hopefully that's hopefully that's
[08:01:32 -> 08:01:37]  easy to understand
[08:01:34 -> 08:01:38]  um and then we have this times 4 which
[08:01:37 -> 08:01:41]  is obviously we're going to stretch it
[08:01:38 -> 08:01:44]  out as we need it to and then whichever
[08:01:41 -> 08:01:47]  index we need to plus right we're
[08:01:44 -> 08:01:48]  storing this as like a as like these
[08:01:47 -> 08:01:50]  these vertical values right so instead
[08:01:48 -> 08:01:52]  of horizontally laying them out we're
[08:01:50 -> 08:01:54]  storing them vertically and this is why
[08:01:52 -> 08:01:56]  you have this extra index here this one
[08:01:54 -> 08:01:58]  this is going to be like the the the
[08:01:56 -> 08:02:01]  column Offset you could say
[08:01:58 -> 08:02:03]  um like which which sorry which yeah
[08:02:01 -> 08:02:07]  like which row are you at rather and
[08:02:03 -> 08:02:10]  then the the BM term that is just this
[08:02:07 -> 08:02:12]  and then we move it over here so it's
[08:02:10 -> 08:02:15]  going to stride across as many times as
[08:02:12 -> 08:02:18]  it needs to and then we can add the just
[08:02:15 -> 08:02:19]  inner row uh a part right so so the row
[08:02:18 -> 08:02:21]  originally came from this part and now
[08:02:19 -> 08:02:23]  we're just flipping it over to this side
[08:02:21 -> 08:02:24]  so we get the actual offset there and
[08:02:23 -> 08:02:26]  that's literally that's literally how we
[08:02:24 -> 08:02:29]  store it it's like literally that easy
[08:02:26 -> 08:02:29]  um
[08:02:30 -> 08:02:35]  so then we go further and we look at b b
[08:02:34 -> 08:02:36]  shouldn't actually be too hard I'm not
[08:02:35 -> 08:02:39]  even going to explain this it's
[08:02:36 -> 08:02:42]  literally like be we're just explicitly
[08:02:39 -> 08:02:46]  adding memory cols uh memory coling here
[08:02:42 -> 08:02:48]  like if I go back to this um what's it
[08:02:46 -> 08:02:51]  called shouldn't the compiler just be
[08:02:48 -> 08:02:53]  able to colest the second version and
[08:02:51 -> 08:02:55]  generate the 128bit loads um the and
[08:02:53 -> 08:02:57]  then the reason is that the compiler has
[08:02:55 -> 08:03:00]  no way to verify that the float B
[08:02:57 -> 08:03:02]  pointer is pass to the kernel um as as
[08:03:00 -> 08:03:04]  128 bit aligned right which would be a
[08:03:02 -> 08:03:06]  requirement for this so essentially
[08:03:04 -> 08:03:08]  we're just saying like like in the in
[08:03:06 -> 08:03:10]  the previous example where I showed all
[08:03:08 -> 08:03:11]  the Shader assembly there might have
[08:03:10 -> 08:03:13]  been some instances where it did not
[08:03:11 -> 08:03:16]  actually know that it's okay to store as
[08:03:13 -> 08:03:20]  a 128 bit right is it like a float Fort
[08:03:16 -> 08:03:22]  like an implicitly 128bit align type um
[08:03:20 -> 08:03:25]  but in this case we're explicitly
[08:03:22 -> 08:03:27]  passing reinterpret uh cast and we're
[08:03:25 -> 08:03:29]  we're explicitly setting it to to float
[08:03:27 -> 08:03:31]  for or
[08:03:29 -> 08:03:33]  128bit and that's promising the compiler
[08:03:31 -> 08:03:35]  that this is aligned right this is
[08:03:33 -> 08:03:36]  telling the compiler you can you can do
[08:03:35 -> 08:03:38]  it what you need to with this we've set
[08:03:36 -> 08:03:40]  this up properly work your
[08:03:38 -> 08:03:42]  magic and we're just helping out the
[08:03:40 -> 08:03:44]  compiler that way so that that's what's
[08:03:42 -> 08:03:47]  happening here same like it's literally
[08:03:44 -> 08:03:49]  this this identical indexing scheme um
[08:03:47 -> 08:03:51]  not really much to to talk about here
[08:03:49 -> 08:03:53]  but uh after this is all done keep in
[08:03:51 -> 08:03:56]  mind this is done once per thread right
[08:03:53 -> 08:03:58]  so we do we store and then we write
[08:03:56 -> 08:04:01]  right right right
[08:03:58 -> 08:04:04]  and it's just super fast right so don't
[08:04:01 -> 08:04:06]  don't worry about the uh don't worry
[08:04:04 -> 08:04:07]  about how we're writing this we don't
[08:04:06 -> 08:04:10]  actually have to make the rights coess
[08:04:07 -> 08:04:12]  so then the rights don't have to be um
[08:04:10 -> 08:04:14]  you know adjacent like this they can be
[08:04:12 -> 08:04:17]  uh separated like by
[08:04:14 -> 08:04:20]  rows uh and then as long as the reads
[08:04:17 -> 08:04:22]  are coess that's fine so the reads
[08:04:20 -> 08:04:24]  coming from here and then we're just
[08:04:22 -> 08:04:26]  we're just flipping it over and it's
[08:04:24 -> 08:04:28]  Landing there which is
[08:04:26 -> 08:04:30]  perfect and then we move on to actually
[08:04:28 -> 08:04:33]  calculating the results of that little
[08:04:30 -> 08:04:34]  of that little thread block the the
[08:04:33 -> 08:04:38]  essentially the thread mini tile within
[08:04:34 -> 08:04:41]  the bigger one and we iterate through
[08:04:38 -> 08:04:43]  with normal. idx right s similar scheme
[08:04:41 -> 08:04:44]  to what we had before except the
[08:04:43 -> 08:04:48]  indexing scheme is a little bit
[08:04:44 -> 08:04:50]  different um we remember how we we have
[08:04:48 -> 08:04:52]  b as like this this rectangle where it's
[08:04:50 -> 08:04:54]  like this is short and this is this is
[08:04:52 -> 08:04:56]  long right a is the same way now now
[08:04:54 -> 08:05:00]  that we've transposed it a is also like
[08:04:56 -> 08:05:03]  this so instead of a being uh like this
[08:05:00 -> 08:05:05]  it's now flipped over like this right
[08:05:03 -> 08:05:08]  and so all we have to do is just change
[08:05:05 -> 08:05:09]  up how we index it and that'll work so
[08:05:08 -> 08:05:12]  I'm not even going to go over b b is
[08:05:09 -> 08:05:15]  like very obvious understand but uh
[08:05:12 -> 08:05:19]  going over a right how do we iterate
[08:05:15 -> 08:05:24]  through this so idx right when we look
[08:05:19 -> 08:05:26]  at this we go um so whichever whichever
[08:05:24 -> 08:05:29]  index we're at we'll just disregard that
[08:05:26 -> 08:05:32]  for now um we'll just say that's zero
[08:05:29 -> 08:05:33]  maybe um and then this since this is
[08:05:32 -> 08:05:36]  multiplying by zero that's going to
[08:05:33 -> 08:05:41]  simplify to zero so it's going to
[08:05:36 -> 08:05:42]  be uh zero plus and then we'll say uh
[08:05:41 -> 08:05:47]  maybe
[08:05:42 -> 08:05:49]  the whichever thread row it's at um
[08:05:47 -> 08:05:51]  whichever thread row it's at is going
[08:05:49 -> 08:05:55]  it's going to multiply by TM which is
[08:05:51 -> 08:05:59]  eight so it's going to stride however
[08:05:55 -> 08:05:59]  much it needs to right um
[08:06:00 -> 08:06:05]  and then we simply add I to that right
[08:06:03 -> 08:06:07]  so thread row instead of being here
[08:06:05 -> 08:06:09]  thread row is actually here because it
[08:06:07 -> 08:06:11]  was previously right you can see how
[08:06:09 -> 08:06:13]  that translation works this was thread
[08:06:11 -> 08:06:15]  row this was long this is thread row now
[08:06:13 -> 08:06:17]  now this is long right so it's it's the
[08:06:15 -> 08:06:19]  same idea it's we're just we're just
[08:06:17 -> 08:06:20]  making the naming a little bit confusing
[08:06:19 -> 08:06:22]  there that that's really all it is we're
[08:06:20 -> 08:06:24]  just stay consistent with whatever this
[08:06:22 -> 08:06:28]  term means because we have to transpose
[08:06:24 -> 08:06:30]  it right um and then we just iterate
[08:06:28 -> 08:06:32]  through through um through I as we need
[08:06:30 -> 08:06:35]  to right
[08:06:32 -> 08:06:36]  so this should be intuitive I don't
[08:06:35 -> 08:06:37]  really think I need to explain that too
[08:06:36 -> 08:06:40]  much it's just like mainly paying
[08:06:37 -> 08:06:42]  attention to the uh the naming
[08:06:40 -> 08:06:44]  convention so thread Row versus thread
[08:06:42 -> 08:06:48]  column like why are those different um
[08:06:44 -> 08:06:50]  that's because you you're uh
[08:06:48 -> 08:06:55]  transposing and then you do the typical
[08:06:50 -> 08:06:58]  write out um so if we go back to this uh
[08:06:55 -> 08:07:00]  up this is all it's doing right right so
[08:06:58 -> 08:07:02]  revolving idx
[08:07:00 -> 08:07:04]  downward all of these all of these
[08:07:02 -> 08:07:06]  threads are sort of next to each other
[08:07:04 -> 08:07:10]  in memory that's why we're striding um
[08:07:06 -> 08:07:13]  TM right TM is that
[08:07:10 -> 08:07:16]  um this is TM so whichever thread row we
[08:07:13 -> 08:07:17]  are at um we're going to stride TM
[08:07:16 -> 08:07:21]  because each thread is going to take
[08:07:17 -> 08:07:21]  care of multiple of those values
[08:07:23 -> 08:07:28]  um and yeah we end up uh we end up just
[08:07:26 -> 08:07:31]  just sort of looking this a bit
[08:07:28 -> 08:07:31]  differently uh and
[08:07:32 -> 08:07:37]  then yeah just just some more visual
[08:07:35 -> 08:07:39]  examples instead of having it as like
[08:07:37 -> 08:07:44]  here and then these these like inch
[08:07:39 -> 08:07:47]  these like uh inch forward
[08:07:44 -> 08:07:48]  it's right but this is actually switched
[08:07:47 -> 08:07:50]  over so it's just kind of the
[08:07:48 -> 08:07:52]  transposing spatial reasoning you have
[08:07:50 -> 08:07:56]  to have to get through and then this all
[08:07:52 -> 08:07:58]  like is pretty straightforward um but
[08:07:56 -> 08:08:01]  yeah now we can actually pop over to the
[08:07:58 -> 08:08:02]  the write out section awesome so we're
[08:08:01 -> 08:08:04]  actually almost done we just have to
[08:08:02 -> 08:08:07]  write out the results now and we have to
[08:08:04 -> 08:08:10]  make sure that these our these are also
[08:08:07 -> 08:08:15]  uh in this Flo float for types so also
[08:08:10 -> 08:08:17]  uh colest into uh the the uh global view
[08:08:15 -> 08:08:20]  Ram right so we have the the normal um
[08:08:17 -> 08:08:22]  iterate over TM and iterate over TN
[08:08:20 -> 08:08:25]  except we change the iterators a bit
[08:08:22 -> 08:08:27]  differently so in TM the rows we're
[08:08:25 -> 08:08:29]  iterating over we're we're doing we're
[08:08:27 -> 08:08:32]  iterating up one each time so it's going
[08:08:29 -> 08:08:36]  to go one one one one one right and then
[08:08:32 -> 08:08:40]  for TN we're going plus 4 each time so
[08:08:36 -> 08:08:42]  we have eight values here and we have um
[08:08:40 -> 08:08:45]  we have two values here right so when
[08:08:42 -> 08:08:47]  you have this total 8 by8 thing you
[08:08:45 -> 08:08:49]  would normally have to do 64 values but
[08:08:47 -> 08:08:53]  because we're doing float 4 We're
[08:08:49 -> 08:08:54]  storing four of those times 16 total
[08:08:53 -> 08:08:57]  right so you can kind of do the math
[08:08:54 -> 08:09:01]  it's like four values time 16 different
[08:08:57 -> 08:09:03]  uh rights um so it's going to do four
[08:09:01 -> 08:09:04]  values each right for 16 of them and
[08:09:03 -> 08:09:08]  we're going to pop full that full 64
[08:09:04 -> 08:09:14]  thing across the TM and and TN tile
[08:09:08 -> 08:09:18]  right um so we we set this temporary
[08:09:14 -> 08:09:21]  variable as the current uh C the C the
[08:09:18 -> 08:09:24]  current C value right so this whatever
[08:09:21 -> 08:09:26]  uh whatever the the the current C is
[08:09:24 -> 08:09:28]  that we care about relative to how we've
[08:09:26 -> 08:09:29]  indexed through these right and this is
[08:09:28 -> 08:09:33]  all keeping in mind to where we are
[08:09:29 -> 08:09:36]  actually at in the entire in the entire
[08:09:33 -> 08:09:39]  C Matrix the entire C Matrix right so
[08:09:36 -> 08:09:43]  the the thread row offset times this
[08:09:39 -> 08:09:45]  plus the res idx um M offset and then
[08:09:43 -> 08:09:48]  all of that times n which is the stride
[08:09:45 -> 08:09:50]  of that and then plus our our columns
[08:09:48 -> 08:09:52]  time TN and then our res idx n this
[08:09:50 -> 08:09:54]  should all make sense this indexing
[08:09:52 -> 08:09:57]  should be like pretty much Crystal Clear
[08:09:54 -> 08:09:58]  um we've we've done very similar to this
[08:09:57 -> 08:10:02]  already
[08:09:58 -> 08:10:03]  um and a lot of the syntax is also
[08:10:02 -> 08:10:04]  similar to this float 4 we did here so
[08:10:03 -> 08:10:06]  if something doesn't make sense just
[08:10:04 -> 08:10:09]  look back at this and break things down
[08:10:06 -> 08:10:11]  again that's the easiest way to do it um
[08:10:09 -> 08:10:15]  but in here we have this we have this
[08:10:11 -> 08:10:18]  temp type right and now temp keep in
[08:10:15 -> 08:10:22]  mind this is what C originally was so we
[08:10:18 -> 08:10:25]  can actually store uh things from the
[08:10:22 -> 08:10:26]  existing uh TMP or the temp variable
[08:10:25 -> 08:10:29]  inside of it against we can actually do
[08:10:26 -> 08:10:30]  more to and then right back inside of it
[08:10:29 -> 08:10:32]  cuz we don't care about the old C result
[08:10:30 -> 08:10:34]  we just want to calculate the new one
[08:10:32 -> 08:10:36]  right so we have this thisx part which
[08:10:34 -> 08:10:39]  is the index zero so we're not going to
[08:10:36 -> 08:10:43]  add anything and then index one and then
[08:10:39 -> 08:10:45]  two and then three these are all x y z w
[08:10:43 -> 08:10:47]  respectively uh so we have this Alpha
[08:10:45 -> 08:10:50]  term the thread results which is with
[08:10:47 -> 08:10:52]  with respect to the thread results uh
[08:10:50 -> 08:10:55]  variable in the register so it's going
[08:10:52 -> 08:11:00]  to be the res index M so which which row
[08:10:55 -> 08:11:02]  are you at right and then the um and
[08:11:00 -> 08:11:04]  then TN which is which is how that spans
[08:11:02 -> 08:11:05]  so you're going to stride over and get
[08:11:04 -> 08:11:08]  to whichever row you want then you're
[08:11:05 -> 08:11:11]  going to offsite offset by this and then
[08:11:08 -> 08:11:13]  based on which Val which index you're at
[08:11:11 -> 08:11:16]  within the float 4 array within that
[08:11:13 -> 08:11:18]  specific the the float 4 window right
[08:11:16 -> 08:11:21]  you're going to add uh these to the
[08:11:18 -> 08:11:22]  actual uh memory addresses or the or the
[08:11:21 -> 08:11:26]  index themselves right store it as like
[08:11:22 -> 08:11:29]  kind of a vector type so uh this way we
[08:11:26 -> 08:11:31]  can actually get the the value that we
[08:11:29 -> 08:11:33]  want and then we simply just add that to
[08:11:31 -> 08:11:35]  the beta scaler and then multiply that
[08:11:33 -> 08:11:37]  by the existing C value as we got from
[08:11:35 -> 08:11:39]  up here right so that should be very
[08:11:37 -> 08:11:41]  straightforward um luckily this is only
[08:11:39 -> 08:11:43]  in register so the indexing isn't too
[08:11:41 -> 08:11:46]  complicated uh and then we simply just
[08:11:43 -> 08:11:49]  write back right so using the same idea
[08:11:46 -> 08:11:53]  that we did back here the same indexing
[08:11:49 -> 08:11:57]  scheme uh and we simply uh write write
[08:11:53 -> 08:11:58]  out c um on the level of threads that's
[08:11:57 -> 08:12:02]  Factor memory
[08:11:58 -> 08:12:04]  coing okay awesome so now uh we can
[08:12:02 -> 08:12:06]  actually just print out how well these
[08:12:04 -> 08:12:11]  do right so we just finished up the
[08:12:06 -> 08:12:13]  vector the V col colest memory access uh
[08:12:11 -> 08:12:15]  with vectorization kernel let's go ahead
[08:12:13 -> 08:12:17]  and print that out how well does that do
[08:12:15 -> 08:12:19]  so the last one was uh 2D block tiling
[08:12:17 -> 08:12:23]  kernel number five we go and print this
[08:12:19 -> 08:12:26]  out and we get a peak of about um 9,000
[08:12:23 -> 08:12:29]  so about about 9100 gig flops and this
[08:12:26 -> 08:12:29]  one
[08:12:30 -> 08:12:37]  about 10,800 gigaflops which is quite a
[08:12:33 -> 08:12:38]  big increase from before so that's about
[08:12:37 -> 08:12:40]  what
[08:12:38 -> 08:12:42]  roughly uh
[08:12:40 -> 08:12:45]  1,600 more Giga flops than before which
[08:12:42 -> 08:12:48]  is pretty solid right it's like a 15% 16
[08:12:45 -> 08:12:50]  whatever perc increase in performance um
[08:12:48 -> 08:12:52]  so now let's actually go ahead and print
[08:12:50 -> 08:12:54]  like some more right uh I know the the
[08:12:52 -> 08:12:57]  autot tuning one was good so we go print
[08:12:54 -> 08:12:57]  09
[08:12:58 -> 08:13:03]  this one did about 11,000 so relative to
[08:13:01 -> 08:13:05]  this one it did you know moderately
[08:13:03 -> 08:13:08]  better and then we can print out the
[08:13:05 -> 08:13:10]  last one which was kublos so kublos was
[08:13:08 -> 08:13:14]  supposed to be the fastest we can
[08:13:10 -> 08:13:17]  actually see that this
[08:13:14 -> 08:13:19]  11,496 is very close to the autotuned
[08:13:17 -> 08:13:20]  kernel right so if we actually look back
[08:13:19 -> 08:13:23]  to what we previously did which was
[08:13:20 -> 08:13:26]  number six we are very close to coup
[08:13:23 -> 08:13:29]  loss so 10,000 we'll just do 10,800
[08:13:26 -> 08:13:29]  divided by
[08:13:31 -> 08:13:36]  um 10,800 ID 11 uh
[08:13:37 -> 08:13:45]  11,496 so we get about
[08:13:40 -> 08:13:48]  94% uh kuo performance just using the uh
[08:13:45 -> 08:13:49]  vectorized uh uh like the float 4
[08:13:48 -> 08:13:52]  loading right so that's ridiculously
[08:13:49 -> 08:13:54]  good and we can still optimize further
[08:13:52 -> 08:13:56]  with these ones right so consider that
[08:13:54 -> 08:13:59]  if we were to optimize these further and
[08:13:56 -> 08:14:00]  maybe use um some additional tricks that
[08:13:59 -> 08:14:02]  you'd find in like research papers like
[08:14:00 -> 08:14:03]  this could actually get really really
[08:14:02 -> 08:14:07]  fast right
[08:14:03 -> 08:14:09]  so that's uh it's pretty cool that we
[08:14:07 -> 08:14:10]  can do that just on our own hardware and
[08:14:09 -> 08:14:13]  we can actually see it from start to
[08:14:10 -> 08:14:14]  finish and understand it intuitively um
[08:14:13 -> 08:14:17]  the reason I'm not going to go over
[08:14:14 -> 08:14:18]  anymore is because it's just more and
[08:14:17 -> 08:14:20]  more complexity to dig through as you go
[08:14:18 -> 08:14:23]  through each one um and I want to save
[08:14:20 -> 08:14:24]  some time for the last final project in
[08:14:23 -> 08:14:26]  the course which we're going to do
[08:14:24 -> 08:14:28]  shortly um but yeah I'm not going to
[08:14:26 -> 08:14:32]  cover all of these I just kind of cover
[08:14:28 -> 08:14:33]  the main ones so uh cols memory access
[08:14:32 -> 08:14:36]  from from Global just like a bump up
[08:14:33 -> 08:14:38]  from naive and then uh all the different
[08:14:36 -> 08:14:40]  tiling variants and then how we can how
[08:14:38 -> 08:14:43]  we can vectorize memory access
[08:14:40 -> 08:14:48]  right so given that um let's actually go
[08:14:43 -> 08:14:51]  ahead and print out the uh the assembly
[08:14:48 -> 08:14:55]  instructions for this specific kernel so
[08:14:51 -> 08:14:58]  if I pop out of here and go into Source
[08:14:55 -> 08:15:03]  SL kernels do
[08:14:58 -> 08:15:06]  nvcc PTX and then we
[08:15:03 -> 08:15:06]  go number
[08:15:07 -> 08:15:14]  six out and we can just go
[08:15:11 -> 08:15:16]  Um number six.
[08:15:14 -> 08:15:18]  PTX okay so instead of that uh I just
[08:15:16 -> 08:15:20]  remember that we had this main file here
[08:15:18 -> 08:15:22]  which we can reference so if I just go
[08:15:20 -> 08:15:24]  ahead and save this if I like take the
[08:15:22 -> 08:15:27]  the title of
[08:15:24 -> 08:15:30]  this and I replace
[08:15:27 -> 08:15:33]  I repl that in
[08:15:30 -> 08:15:36]  here uh and then we go ahead and compile
[08:15:33 -> 08:15:38]  we're going to get our actual uh PTX
[08:15:36 -> 08:15:42]  instructions here
[08:15:38 -> 08:15:45]  so uh it seems like we got some errors
[08:15:42 -> 08:15:50]  which is because if I actually go in
[08:15:45 -> 08:15:50]  here we have to copy this copy it
[08:15:50 -> 08:15:55]  back oh we're getting
[08:15:55 -> 08:16:00]  vim and
[08:15:57 -> 08:16:02]  and so pretty much what happened there
[08:16:00 -> 08:16:04]  was I had it imported properly but we
[08:16:02 -> 08:16:07]  just weren't including the TN at the end
[08:16:04 -> 08:16:09]  here so I just added that um and now it
[08:16:07 -> 08:16:12]  now it's successfully uh compiled so we
[08:16:09 -> 08:16:14]  get kernel. PTX we open this um and we
[08:16:12 -> 08:16:18]  can go ahead and see um let's see if we
[08:16:14 -> 08:16:20]  can find any the 128's aren't in here so
[08:16:18 -> 08:16:23]  I actually have to go in and uh and
[08:16:20 -> 08:16:27]  change this to Shader assembly we can go
[08:16:23 -> 08:16:32]  ahead and do um
[08:16:27 -> 08:16:36]  n BCC Das will do Arch is
[08:16:32 -> 08:16:39]  SM oh Arch equals sore
[08:16:36 -> 08:16:43]  86 and then we'll do Q
[08:16:39 -> 08:16:43]  binary um main
[08:16:43 -> 08:16:47]  dosc um and then out and we'll do it
[08:16:46 -> 08:16:51]  main
[08:16:47 -> 08:16:56]  main. Cuda binary and then we'll go
[08:16:51 -> 08:16:56]  ahead and C object object dump
[08:16:58 -> 08:17:03]  and we'll do main dot key binary just
[08:17:01 -> 08:17:06]  like that we go ahead and move this
[08:17:03 -> 08:17:13]  up and we can see that um if we look for
[08:17:06 -> 08:17:15]  the specific ldg doe instruction ldg
[08:17:13 -> 08:17:18]  do
[08:17:15 -> 08:17:20]  128 128
[08:17:18 -> 08:17:23]  128
[08:17:20 -> 08:17:28]  1288 128
[08:17:23 -> 08:17:31]  128 and so on and so forth so how cool
[08:17:28 -> 08:17:33]  is that we can actually verify that all
[08:17:31 -> 08:17:36]  of these
[08:17:33 -> 08:17:40]  um may we can find like a 32 in here
[08:17:36 -> 08:17:40]  somewhere oh that's just going to be in
[08:17:41 -> 08:17:47]  uh that's a
[08:17:44 -> 08:17:49]  register that's a register as well
[08:17:47 -> 08:17:49]  that's a
[08:17:50 -> 08:17:55]  register so we actually do not have any
[08:17:52 -> 08:17:58]  32-bit loads any 32-bit transfers at all
[08:17:55 -> 08:18:02]  everything is perfect
[08:17:58 -> 08:18:02]  um everything is
[08:18:03 -> 08:18:09]  great how
[08:18:06 -> 08:18:11]  wonderful so now uh now that that kind
[08:18:09 -> 08:18:13]  of makes sense that we are able to
[08:18:11 -> 08:18:16]  literally see what the uh instructions
[08:18:13 -> 08:18:19]  are looking like when we provide optim
[08:18:16 -> 08:18:21]  optimizations um you know we can we can
[08:18:19 -> 08:18:23]  actually we actually feel a lot more
[08:18:21 -> 08:18:24]  confident in our ability it's not just
[08:18:23 -> 08:18:26]  like me telling you that this works and
[08:18:24 -> 08:18:28]  trusting it it's like you can actually
[08:18:26 -> 08:18:30]  see it this is what is being uh this is
[08:18:28 -> 08:18:31]  what's being run on the gpus controllers
[08:18:30 -> 08:18:34]  the little microcontrollers inside that
[08:18:31 -> 08:18:38]  issue the instructions
[08:18:34 -> 08:18:39]  right and so if we pop back to here um
[08:18:38 -> 08:18:41]  there's one little thing I wanted to
[08:18:39 -> 08:18:44]  cover it's an extra optimization which
[08:18:41 -> 08:18:48]  is using tensor cores so we go to
[08:18:44 -> 08:18:50]  programming tensor cores um you
[08:18:48 -> 08:18:53]  literally just search it's on the Nvidia
[08:18:50 -> 08:18:57]  blog programming tensor cores in Cuda 9
[08:18:53 -> 08:19:02]  so Cuda 9 um I mean we already actually
[08:18:57 -> 08:19:04]  have I go here Nvidia SMI we're on Cuda
[08:19:02 -> 08:19:07]  12.5 so that's fine we don't have to
[08:19:04 -> 08:19:09]  worry about incompatibility issues um
[08:19:07 -> 08:19:09]  but
[08:19:09 -> 08:19:15]  essentially this gives the whole
[08:19:12 -> 08:19:17]  instruction sets on how to use tensor
[08:19:15 -> 08:19:21]  cores right so
[08:19:17 -> 08:19:23]  um ca9 provides these new features these
[08:19:21 -> 08:19:25]  tensor cores will essentially like for
[08:19:23 -> 08:19:27]  example on the on the Volta architecture
[08:19:25 -> 08:19:30]  you can you can do this where it's like
[08:19:27 -> 08:19:32]  you multiply these two fp16 matrices and
[08:19:30 -> 08:19:35]  then you add another one right the the
[08:19:32 -> 08:19:38]  fuse multiply and add operation we saw
[08:19:35 -> 08:19:41]  previously um except on the level of 4x4
[08:19:38 -> 08:19:45]  tensors and these are supported in the
[08:19:41 -> 08:19:46]  in the CU loss Library um so two tensor
[08:19:45 -> 08:19:48]  two Cuda libraries that use tensor cores
[08:19:46 -> 08:19:50]  are kublos and CNN right the ones we
[08:19:48 -> 08:19:53]  ones we covered earlier kublos use
[08:19:50 -> 08:19:56]  tensor course to speed up gem Matrix
[08:19:53 -> 08:20:00]  multiply uh operations and CNN uses it
[08:19:56 -> 08:20:02]  to speed up convolutions and rnns so
[08:20:00 -> 08:20:04]  that's cool but what if we want to write
[08:20:02 -> 08:20:05]  our own what if we want to write our own
[08:20:04 -> 08:20:07]  code that isn't dependent on CU loss
[08:20:05 -> 08:20:08]  right because CU loss we could just call
[08:20:07 -> 08:20:11]  like a separate function but it might
[08:20:08 -> 08:20:13]  not be as slow or we can't like fuse the
[08:20:11 -> 08:20:15]  actual tensor core instructions with a
[08:20:13 -> 08:20:17]  specific kernel that we want to do like
[08:20:15 -> 08:20:19]  for example um flash attention right if
[08:20:17 -> 08:20:21]  we wanted to write that kublos might not
[08:20:19 -> 08:20:23]  let us do that because it's sort of
[08:20:21 -> 08:20:27]  independent we can't include those calls
[08:20:23 -> 08:20:30]  inside of kernels um so if we wanted to
[08:20:27 -> 08:20:31]  actually look at how to use that um
[08:20:30 -> 08:20:34]  where did it
[08:20:31 -> 08:20:36]  go scroll down a little bit tensor
[08:20:34 -> 08:20:39]  course and
[08:20:36 -> 08:20:42]  CNN and then um programmatic access to
[08:20:39 -> 08:20:44]  tensor cores and that's where ca 9.0
[08:20:42 -> 08:20:48]  comes in so we're good um essentially
[08:20:44 -> 08:20:52]  there's this new thing called a um the
[08:20:48 -> 08:20:55]  complete namespace is NV and then in C++
[08:20:52 -> 08:20:57]  it's like the the W the W
[08:20:55 -> 08:20:59]  MMA um
[08:20:57 -> 08:21:00]  this means warp Matrix multiply
[08:20:59 -> 08:21:02]  accumulate which I'm not going to go
[08:21:00 -> 08:21:04]  over like exactly what that means nor do
[08:21:02 -> 08:21:09]  I entirely know what's happening under
[08:21:04 -> 08:21:12]  the hood there but um that's that is the
[08:21:09 -> 08:21:14]  tensor core uh operations that we can
[08:21:12 -> 08:21:16]  call right so there's a bunch of
[08:21:14 -> 08:21:18]  examples here I'm not going to go over
[08:21:16 -> 08:21:22]  this part but uh this is just kind of
[08:21:18 -> 08:21:23]  like a nice little um you know dock for
[08:21:22 -> 08:21:26]  understanding how the heck to use tensor
[08:21:23 -> 08:21:28]  core operations and you might you might
[08:21:26 -> 08:21:29]  might even want to implement those
[08:21:28 -> 08:21:31]  inside of the block tiling kernel right
[08:21:29 -> 08:21:34]  so the ones that we just wrote you might
[08:21:31 -> 08:21:37]  want to say take like a single thread
[08:21:34 -> 08:21:38]  and have like a thread or a piece of it
[08:21:37 -> 08:21:40]  and try to like block these actual
[08:21:38 -> 08:21:42]  tensor core operations right and make
[08:21:40 -> 08:21:44]  these really really fast so that way
[08:21:42 -> 08:21:45]  it's not just so that way it's not
[08:21:44 -> 08:21:46]  iterating through eight and then
[08:21:45 -> 08:21:49]  iterating through eight more and having
[08:21:46 -> 08:21:52]  64 total operations to do but rather
[08:21:49 -> 08:21:53]  just having one right so these are
[08:21:52 -> 08:21:56]  literally going to be organized in a 3D
[08:21:53 -> 08:22:00]  structure inside of the tensor cores on
[08:21:56 -> 08:22:03]  the the GPU um and it's it's literally
[08:22:00 -> 08:22:04]  just like a a 3D thing or a 2d thing
[08:22:03 -> 08:22:07]  depending on how big Dimensions you're
[08:22:04 -> 08:22:10]  using um because you can do like batch
[08:22:07 -> 08:22:13]  by uh Time by Channel if you're using
[08:22:10 -> 08:22:16]  Transformers or you can do
[08:22:13 -> 08:22:19]  um yeah just there there is there is a
[08:22:16 -> 08:22:21]  lot going on in the hardware but you can
[08:22:19 -> 08:22:23]  actually you can capitalize on those so
[08:22:21 -> 08:22:26]  anyways I'm not going to ramble on this
[08:22:23 -> 08:22:28]  is this is tensor cores uh you have
[08:22:26 -> 08:22:30]  permission have fun with this um I
[08:22:28 -> 08:22:31]  encourage you to do so I might add some
[08:22:30 -> 08:22:33]  more to this course later on in the
[08:22:31 -> 08:22:35]  GitHub repo that'll you know sort of
[08:22:33 -> 08:22:37]  talk more about tensor course but this
[08:22:35 -> 08:22:40]  is
[08:22:37 -> 08:22:42]  it okay so we can finally take a
[08:22:40 -> 08:22:45]  breather now matrix multiplication is
[08:22:42 -> 08:22:47]  pretty much finished um there's there's
[08:22:45 -> 08:22:49]  a little bit more that we'll do later on
[08:22:47 -> 08:22:50]  but for now you can consider matrix
[08:22:49 -> 08:22:54]  multiplication finished for the next
[08:22:50 -> 08:22:58]  section or two um now we go into Triton
[08:22:54 -> 08:23:00]  which essentially takes the previous uh
[08:22:58 -> 08:23:02]  takes the previous chapter and says
[08:23:00 -> 08:23:06]  let's abstract that and make it easier
[08:23:02 -> 08:23:08]  to use um so so taking uh you know
[08:23:06 -> 08:23:10]  matrix multiplication or like tiled
[08:23:08 -> 08:23:12]  optimizations where you're tiling things
[08:23:10 -> 08:23:13]  into blocks and then you know
[08:23:12 -> 08:23:15]  multiplying them together more
[08:23:13 -> 08:23:18]  efficiently we can actually take that
[08:23:15 -> 08:23:22]  and do it in Python with much simpler
[08:23:18 -> 08:23:22]  syntax so this is Triton
[08:23:22 -> 08:23:28]  um Triton is a bit different than Cuda
[08:23:25 -> 08:23:30]  all right so before I actually go into
[08:23:28 -> 08:23:32]  the the whole design here and what
[08:23:30 -> 08:23:34]  Triton is about I want you to pay
[08:23:32 -> 08:23:36]  attention to something so if I go pip
[08:23:34 -> 08:23:40]  install
[08:23:36 -> 08:23:42]  torch you'll see that we get all of
[08:23:40 -> 08:23:44]  these all these all this Nvidia stuff
[08:23:42 -> 08:23:49]  which we've seen before and then we get
[08:23:44 -> 08:23:52]  this Triton Triton 3.0 right and Triton
[08:23:49 -> 08:23:54]  is used by pie torch under the hood or
[08:23:52 -> 08:23:57]  accelerating and making things faster
[08:23:54 -> 08:24:00]  with python syntax right Tron is also
[08:23:57 -> 08:24:02]  fast just like Cuda and so there there
[08:24:00 -> 08:24:06]  are some differences between them which
[08:24:02 -> 08:24:10]  I thought I should highlight um so if we
[08:24:06 -> 08:24:12]  pop over to the uh pop over to the
[08:24:10 -> 08:24:14]  Triton website search up Triton doc or
[08:24:12 -> 08:24:17]  Trident website or whatever uh this will
[08:24:14 -> 08:24:21]  come up they also have a GitHub as well
[08:24:17 -> 08:24:25]  so the GitHub has you know a lot of uh
[08:24:21 -> 08:24:26]  you know useful stuff um yeah just
[08:24:25 -> 08:24:29]  playing around with it so setting up
[08:24:26 -> 08:24:31]  configuring tridon doing custom things
[08:24:29 -> 08:24:33]  uh but you can just PP install tridon
[08:24:31 -> 08:24:36]  like that and it'll work the same way um
[08:24:33 -> 08:24:38]  but if you do look at at the Triton
[08:24:36 -> 08:24:41]  website you'll see a bunch of sections
[08:24:38 -> 08:24:43]  so like how do you install it um
[08:24:41 -> 08:24:46]  tutorials on how to use Triton so
[08:24:43 -> 08:24:47]  there's like a bunch here which I don't
[08:24:46 -> 08:24:51]  necessarily cover in this course but you
[08:24:47 -> 08:24:54]  can you can go over more of these uh if
[08:24:51 -> 08:24:56]  you want to um and then there's the
[08:24:54 -> 08:24:59]  important ones that we care about so
[08:24:56 -> 08:25:02]  Triton um what are like the Triton docs
[08:24:59 -> 08:25:04]  there's like jit autotune heris all that
[08:25:02 -> 08:25:07]  um just going
[08:25:04 -> 08:25:10]  through going through
[08:25:07 -> 08:25:11]  functions um and then we get into Triton
[08:25:10 -> 08:25:14]  language which is the most important
[08:25:11 -> 08:25:16]  part of all um we have a bunch of
[08:25:14 -> 08:25:18]  different operations here with tridon
[08:25:16 -> 08:25:19]  and I'll go into the whole design of
[08:25:18 -> 08:25:22]  Tron in a second here but this it just
[08:25:19 -> 08:25:25]  like makes it really easy to see that
[08:25:22 -> 08:25:27]  all of the operations are right here so
[08:25:25 -> 08:25:32]  um like the programming model uh
[08:25:27 -> 08:25:34]  operations to know creation Ops um so if
[08:25:32 -> 08:25:36]  you have like a if you have an X tensor
[08:25:34 -> 08:25:37]  and you want to make a y but you don't
[08:25:36 -> 08:25:39]  want to populate it with anything or you
[08:25:37 -> 08:25:42]  just want to make it zeros you can say
[08:25:39 -> 08:25:44]  zeros like right as a creation um shape
[08:25:42 -> 08:25:46]  manipulation linear algebra so dot
[08:25:44 -> 08:25:50]  product
[08:25:46 -> 08:25:52]  um pointer Ops uh memory pointer Ops
[08:25:50 -> 08:25:55]  bunch of things math there's a lot of
[08:25:52 -> 08:25:56]  math Ops reduction Ops so like maximum
[08:25:55 -> 08:25:58]  or you have an array and you're trying
[08:25:56 -> 08:26:00]  to find the maximum of it and you reduce
[08:25:58 -> 08:26:04]  it to just returning a single
[08:26:00 -> 08:26:06]  value uh scan and swort operations
[08:26:04 -> 08:26:08]  atomics like we went on previously
[08:26:06 -> 08:26:10]  random number generation and Etc right
[08:26:08 -> 08:26:12]  there's there's a lot here even even uh
[08:26:10 -> 08:26:14]  debugging and printing too so try and
[08:26:12 -> 08:26:16]  this is this is where all of the
[08:26:14 -> 08:26:18]  operations are that you're going to find
[08:26:16 -> 08:26:23]  um which makes this really easy to go
[08:26:18 -> 08:26:27]  through um but if we go through uh where
[08:26:23 -> 08:26:31]  is it introduction yes so the whole idea
[08:26:27 -> 08:26:36]  here and if I go to uh Triton
[08:26:31 -> 08:26:39]  paper yes this one so I'll look at this
[08:26:36 -> 08:26:41]  in a second here but this is essentially
[08:26:39 -> 08:26:43]  what Triton was inspired by is this this
[08:26:41 -> 08:26:45]  paper here for tille neural net
[08:26:43 -> 08:26:48]  computations like we went on in the
[08:26:45 -> 08:26:50]  previous chapter right um and and this
[08:26:48 -> 08:26:53]  this is the whole idea here is you have
[08:26:50 -> 08:26:55]  scalar Pro Cuda has a scalar program and
[08:26:53 -> 08:26:57]  block threads versus Trent which has a
[08:26:55 -> 08:26:58]  block program and scal threads it's like
[08:26:57 -> 08:27:01]  okay what does this mean this is super
[08:26:58 -> 08:27:03]  weird um there's a lot of like geometry
[08:27:01 -> 08:27:06]  happening here so like how do we
[08:27:03 -> 08:27:08]  actually break this down
[08:27:06 -> 08:27:12]  um
[08:27:08 -> 08:27:15]  now to clarify the reason why Cuda is a
[08:27:12 -> 08:27:17]  scale scalar program with block threads
[08:27:15 -> 08:27:19]  is because you write a kernel to operate
[08:27:17 -> 08:27:21]  at the level of threads or scalers in
[08:27:19 -> 08:27:23]  this case so that's why it's a scaler
[08:27:21 -> 08:27:27]  program each individual kernel runs on a
[08:27:23 -> 08:27:29]  thread um but you need to be aware you
[08:27:27 -> 08:27:31]  need to be implicitly aware that those
[08:27:29 -> 08:27:35]  kernels also operate on the level of
[08:27:31 -> 08:27:36]  groups too so when you have uh like for
[08:27:35 -> 08:27:39]  example shared memory like that's
[08:27:36 -> 08:27:42]  something you need to worry about right
[08:27:39 -> 08:27:45]  um Cuda
[08:27:42 -> 08:27:47]  has Cuda has blocked threads so scalar
[08:27:45 -> 08:27:49]  program it's like the actual kernel that
[08:27:47 -> 08:27:51]  you write and then block threads with
[08:27:49 -> 08:27:53]  the idea of when you actually write the
[08:27:51 -> 08:27:54]  the kernel itself which runs on a thread
[08:27:53 -> 08:27:55]  it has to be aware of of all the other
[08:27:54 -> 08:27:57]  threads too that that's kind of what I'm
[08:27:55 -> 08:28:01]  getting out there and then Triton is
[08:27:57 -> 08:28:04]  abstracted up to thread blocks so
[08:28:01 -> 08:28:06]  um this this essentially means compiler
[08:28:04 -> 08:28:09]  takes care of all these thread level
[08:28:06 -> 08:28:11]  operations uh for us so when you write
[08:28:09 -> 08:28:13]  something in Triton instead of having it
[08:28:11 -> 08:28:15]  run on each individual thread and have
[08:28:13 -> 08:28:16]  them communicate and be aware of that
[08:28:15 -> 08:28:19]  it's going to write on the level of
[08:28:16 -> 08:28:21]  blocks and all of those like thread
[08:28:19 -> 08:28:22]  level instructions and optimizations are
[08:28:21 -> 08:28:24]  going to be handled by the compiler for
[08:28:22 -> 08:28:26]  you so you don't have to actually worry
[08:28:24 -> 08:28:30]  about those um
[08:28:26 -> 08:28:32]  and then when we talk about um when we
[08:28:30 -> 08:28:32]  talk
[08:28:32 -> 08:28:38]  about when we talk about scalar threads
[08:28:36 -> 08:28:41]  it means you don't have to be uh as
[08:28:38 -> 08:28:43]  worried about them talking inter uh like
[08:28:41 -> 08:28:45]  interconnected with each other you don't
[08:28:43 -> 08:28:47]  have to be aware because Cuda actually
[08:28:45 -> 08:28:49]  handles that part so since you're
[08:28:47 -> 08:28:51]  writing on the level of blocks you don't
[08:28:49 -> 08:28:53]  actually have to worry about interthread
[08:28:51 -> 08:28:55]  Communications you just have to worry
[08:28:53 -> 08:28:57]  about what exactly the block is doing um
[08:28:55 -> 08:28:59]  write the oper out clearly for that and
[08:28:57 -> 08:29:03]  then TR will actually handle that under
[08:28:59 -> 08:29:05]  the hood so you saw how um there's
[08:29:03 -> 08:29:07]  there's very little operations in the
[08:29:05 -> 08:29:09]  tron do language section this is because
[08:29:07 -> 08:29:11]  the compiler is able to handle a lot of
[08:29:09 -> 08:29:13]  the additional operations that that
[08:29:11 -> 08:29:16]  allow for for performance increases
[08:29:13 -> 08:29:18]  right um so so that's that's the whole
[08:29:16 -> 08:29:21]  idea behind um that's the whole idea
[08:29:18 -> 08:29:25]  behind Triton and that whole uh blocked
[08:29:21 -> 08:29:27]  versus scalar uh blocked versus blocked
[08:29:25 -> 08:29:30]  and scalar thread
[08:29:27 -> 08:29:30]  philosophy
[08:29:30 -> 08:29:35]  so why can't we just skip Cuda and go
[08:29:32 -> 08:29:38]  straight to Triton why can't we do this
[08:29:35 -> 08:29:42]  well Tron is an abstraction on top of
[08:29:38 -> 08:29:43]  Cuda so you have lower level stuff that
[08:29:42 -> 08:29:46]  offers optimizations and that you can be
[08:29:43 -> 08:29:47]  explicit about to the Nvidia compiler
[08:29:46 -> 08:29:50]  the
[08:29:47 -> 08:29:53]  nbcc um and Triton takes advantage of
[08:29:50 -> 08:29:54]  those and bumps it up a few layers uh to
[08:29:53 -> 08:29:56]  something that you can sort of
[08:29:54 -> 08:29:58]  understand easily and and write less
[08:29:56 -> 08:30:00]  boiler plate code for so we still need
[08:29:58 -> 08:30:01]  to understand how Cuda Works to ensure
[08:30:00 -> 08:30:03]  that we're applying the right
[08:30:01 -> 08:30:05]  optimizations you can't naively write
[08:30:03 -> 08:30:07]  Triton code without knowing uh what's
[08:30:05 -> 08:30:11]  going on on a low level it just helps
[08:30:07 -> 08:30:13]  helps you prevent boiler plate
[08:30:11 -> 08:30:16]  um you also may want to optimize your
[08:30:13 -> 08:30:17]  own kernels in Cuda right so going back
[08:30:16 -> 08:30:19]  to what I said before you kind of need
[08:30:17 -> 08:30:21]  to know the low LEL operations uh in
[08:30:19 -> 08:30:24]  order to make sure that everything is
[08:30:21 -> 08:30:27]  working as intended right um and if you
[08:30:24 -> 08:30:30]  want to build on top of Tri or build
[08:30:27 -> 08:30:31]  things like it or abstract above Cuda
[08:30:30 -> 08:30:33]  which is shr you you need to learn Cuda
[08:30:31 -> 08:30:35]  you need to understand what is Cuda
[08:30:33 -> 08:30:37]  doing so that you can build on top of it
[08:30:35 -> 08:30:40]  so you can Leverage What it contains
[08:30:37 -> 08:30:42]  already right
[08:30:40 -> 08:30:45]  um and so if we go to the Triton paper
[08:30:42 -> 08:30:47]  here Intermediate Language and compiler
[08:30:45 -> 08:30:49]  for tile neuronet
[08:30:47 -> 08:30:51]  computations that this is It's
[08:30:49 -> 08:30:54]  essentially just
[08:30:51 -> 08:30:57]  um
[08:30:54 -> 08:31:02]  it it does everything that kublos and
[08:30:57 -> 08:31:05]  and CNN does but does it um roughly as
[08:31:02 -> 08:31:06]  fast and without a ton of boiler plate
[08:31:05 -> 08:31:08]  so that that's like the whole idea is
[08:31:06 -> 08:31:10]  you have these tiled computations that
[08:31:08 -> 08:31:12]  you have to do in kublos and cdnn and it
[08:31:10 -> 08:31:14]  takes care of those so I'm not going to
[08:31:12 -> 08:31:17]  go through this um this is like this is
[08:31:14 -> 08:31:20]  a separate course going through all of
[08:31:17 -> 08:31:22]  Triton but we're we're going to go over
[08:31:20 -> 08:31:24]  like how how do the basics work right so
[08:31:22 -> 08:31:27]  that you understand um how Triton can be
[08:31:24 -> 08:31:30]  applied um and maybe it'll give you some
[08:31:27 -> 08:31:33]  ideas as to what to do later on okay so
[08:31:30 -> 08:31:36]  now we actually go into an example of a
[08:31:33 -> 08:31:37]  vector Edition kernel in Trine all right
[08:31:36 -> 08:31:40]  so I'm going to try to break this down
[08:31:37 -> 08:31:41]  as kind of as as efficiently as possible
[08:31:40 -> 08:31:43]  uh this stuff isn't too hard it's
[08:31:41 -> 08:31:44]  supposed to be kind of a break from
[08:31:43 -> 08:31:47]  doing uh things like matrix
[08:31:44 -> 08:31:48]  multiplication so uh you you'll you'll
[08:31:47 -> 08:31:51]  probably find this you'll probably find
[08:31:48 -> 08:31:52]  this is a end up end up being you'll
[08:31:51 -> 08:31:54]  probably find this ends up being a
[08:31:52 -> 08:31:57]  breeze uh so we start off you know we
[08:31:54 -> 08:31:59]  import torch so Triton is very closely
[08:31:57 -> 08:32:02]  linked with torch uh so we we end up
[08:31:59 -> 08:32:04]  using that to to handle some other stuff
[08:32:02 -> 08:32:07]  like initialize arrays and matrices and
[08:32:04 -> 08:32:08]  stuff we import Triton the Triton
[08:32:07 -> 08:32:12]  language and then shorthand version of
[08:32:08 -> 08:32:15]  that and then we go down further um we
[08:32:12 -> 08:32:16]  initialize uh a seed so that the results
[08:32:15 -> 08:32:19]  are reproducible when we when you
[08:32:16 -> 08:32:20]  randomly initialize stuff um to ensure
[08:32:19 -> 08:32:22]  that you know we don't get errors or
[08:32:20 -> 08:32:25]  whatever it's a good practice and then
[08:32:22 -> 08:32:28]  we have a size of 2 to the 25 so if we
[08:32:25 -> 08:32:32]  go into here we go
[08:32:28 -> 08:32:35]  uh this is 33 million elements long
[08:32:32 -> 08:32:35]  right so if we go
[08:32:37 -> 08:32:44]  this 33.5 million elements
[08:32:41 -> 08:32:46]  long um and then we just initialize our
[08:32:44 -> 08:32:49]  X and Y the two two that we're going to
[08:32:46 -> 08:32:50]  add on device that are randomly randomly
[08:32:49 -> 08:32:52]  initialized uh and then we just have
[08:32:50 -> 08:32:54]  some benchmarking stuff down here don't
[08:32:52 -> 08:32:56]  worry about this this is just for
[08:32:54 -> 08:32:58]  testing and and seeing how perform which
[08:32:56 -> 08:33:00]  we which we'll see in a second here um
[08:32:58 -> 08:33:02]  but going up when we're actually adding
[08:33:00 -> 08:33:04]  them together this is when you when you
[08:33:02 -> 08:33:06]  do um when you're actually adding them
[08:33:04 -> 08:33:08]  you're going to add X and Y which are
[08:33:06 -> 08:33:10]  tensors and then you're going to return
[08:33:08 -> 08:33:14]  a tensor right so it's just X Plus y
[08:33:10 -> 08:33:17]  return output that's it very simple um
[08:33:14 -> 08:33:19]  this is essentially uh an easier way of
[08:33:17 -> 08:33:23]  doing Cuda Malik so instead of Cuda
[08:33:19 -> 08:33:25]  Malik and C or or CA c c C++ you're
[08:33:23 -> 08:33:27]  going to go tor. empty like so it's
[08:33:25 -> 08:33:29]  going to have the same shape as X but
[08:33:27 -> 08:33:31]  it's going to be populated with zeros uh
[08:33:29 -> 08:33:33]  we do an assert so we make sure all of
[08:33:31 -> 08:33:36]  our all of our tensors are on the
[08:33:33 -> 08:33:39]  device we set num elements to literally
[08:33:36 -> 08:33:41]  just num elements of the output so how
[08:33:39 -> 08:33:45]  many what is like the length of this
[08:33:41 -> 08:33:48]  array um and then we have this weird uh
[08:33:45 -> 08:33:50]  configuration of uh of like how we how
[08:33:48 -> 08:33:52]  we actually launch a kernel and how we
[08:33:50 -> 08:33:55]  Define like the the dim three and and
[08:33:52 -> 08:33:57]  the grid size and all that stuff um so
[08:33:55 -> 08:33:58]  we have a Lambda function here don't
[08:33:57 -> 08:34:01]  worry about this too much by the way
[08:33:58 -> 08:34:02]  this this you care about performance not
[08:34:01 -> 08:34:06]  not like understanding how the syntax
[08:34:02 -> 08:34:08]  Works under the hood um but this is a
[08:34:06 -> 08:34:10]  just a Lambda function and inside of
[08:34:08 -> 08:34:12]  here all like all you have to worry
[08:34:10 -> 08:34:16]  about is that we're doing a ceiling
[08:34:12 -> 08:34:18]  division of n elements so let's say n
[08:34:16 -> 08:34:21]  elements is like24 you have an array
[08:34:18 -> 08:34:22]  1024 elements and you want to add them
[08:34:21 -> 08:34:25]  together in blocks and your block size
[08:34:22 -> 08:34:28]  is 256 so what you would do is you would
[08:34:25 -> 08:34:33]  par that 1024 over four different blocks
[08:34:28 -> 08:34:35]  of size 256 right but if you have say
[08:34:33 -> 08:34:37]  1,25 elements then you want to make sure
[08:34:35 -> 08:34:38]  that you give it an extra block so that
[08:34:37 -> 08:34:39]  it doesn't miss that element right or
[08:34:38 -> 08:34:41]  else you're not going to get the right
[08:34:39 -> 08:34:44]  answer so you have to actually do a
[08:34:41 -> 08:34:46]  ceiling div um so that it rounds up to
[08:34:44 -> 08:34:48]  five blocks instead of just rounding
[08:34:46 -> 08:34:50]  back down to four because then you'll
[08:34:48 -> 08:34:52]  end up missing that extra one right uh
[08:34:50 -> 08:34:55]  so that's all we're doing here um and
[08:34:52 -> 08:34:58]  then just to launch the kernel you I
[08:34:55 -> 08:35:00]  know it's you do like this this this uh
[08:34:58 -> 08:35:02]  this function and then you index with
[08:35:00 -> 08:35:04]  this grid term and then you pass in your
[08:35:02 -> 08:35:07]  variables after it's like hm that's
[08:35:04 -> 08:35:10]  weird um but don't don't don't worry
[08:35:07 -> 08:35:13]  about this too much so we just pass in
[08:35:10 -> 08:35:15]  the grid as like that uh those like
[08:35:13 -> 08:35:18]  those like alligator symbols um that's
[08:35:15 -> 08:35:20]  that's all this is that's your kernel
[08:35:18 -> 08:35:22]  launch configuration and then you have
[08:35:20 -> 08:35:25]  the actual parameters themselves so
[08:35:22 -> 08:35:25]  instead of going
[08:35:26 -> 08:35:28]  we just
[08:35:45 -> 08:35:50]  go we just do that very so it's it's
[08:35:48 -> 08:35:52]  like a syntactical thing
[08:35:50 -> 08:35:55]  right
[08:35:52 -> 08:35:57]  um anyways that's that that that
[08:35:55 -> 08:35:59]  shouldn't be too much of a question I'm
[08:35:57 -> 08:36:01]  I'm not going over this because we care
[08:35:59 -> 08:36:03]  about performance as opposed to like
[08:36:01 -> 08:36:06]  what the heck what the heck all these
[08:36:03 -> 08:36:09]  what the heck the syntax is um but yeah
[08:36:06 -> 08:36:13]  so so going up to the actual mechanics
[08:36:09 -> 08:36:16]  of a kernel inen and comparing that sort
[08:36:13 -> 08:36:18]  of side by side with with Cuda um we
[08:36:16 -> 08:36:20]  have we have
[08:36:18 -> 08:36:22]  X we have we have to actually I forgot
[08:36:20 -> 08:36:24]  to say we add this Tron jet decorator at
[08:36:22 -> 08:36:26]  the top to say that we want it to we
[08:36:24 -> 08:36:28]  want it to be jet compiled by tridon
[08:36:26 -> 08:36:28]  just like be aware of that CU if you
[08:36:28 -> 08:36:31]  don't add this you're going to get
[08:36:28 -> 08:36:33]  errors um but for all the different
[08:36:31 -> 08:36:35]  variables you know you have your X and Y
[08:36:33 -> 08:36:37]  your output your Nom elements and then
[08:36:35 -> 08:36:40]  the block size um which you you know
[08:36:37 -> 08:36:42]  pass into here XY output output elements
[08:36:40 -> 08:36:45]  and elements block
[08:36:42 -> 08:36:48]  size now we have these as pointers
[08:36:45 -> 08:36:51]  because in memory it's going to be it's
[08:36:48 -> 08:36:54]  going to be laid out as the the first
[08:36:51 -> 08:36:56]  element in that array is going to or
[08:36:54 -> 08:36:58]  essentially this this pointer is going
[08:36:56 -> 08:37:00]  to be the first element in that array so
[08:36:58 -> 08:37:02]  you want to start from the you want to
[08:37:00 -> 08:37:04]  base it off of the start of that entire
[08:37:02 -> 08:37:05]  tensor or array and you want to continue
[08:37:04 -> 08:37:07]  from there right so that's how Triton is
[08:37:05 -> 08:37:11]  going to interpret that uh and then you
[08:37:07 -> 08:37:14]  do the same for uh y output pointer um
[08:37:11 -> 08:37:17]  and then we just continue on
[08:37:14 -> 08:37:21]  so so jumping down a little bit we see
[08:37:17 -> 08:37:24]  uh p and so P essentially says which
[08:37:21 -> 08:37:29]  which block are we add in the grid right
[08:37:24 -> 08:37:32]  um now we have to be careful about this
[08:37:29 -> 08:37:34]  so a a good way to think about which
[08:37:32 -> 08:37:36]  block we are in the grid is to actually
[08:37:34 -> 08:37:39]  go down to here and see how does this
[08:37:36 -> 08:37:42]  apply right so when we're actually in
[08:37:39 -> 08:37:46]  this array um let's say block size is 64
[08:37:42 -> 08:37:49]  and we have 256 elements in this array
[08:37:46 -> 08:37:50]  um you don't want to just say zero or
[08:37:49 -> 08:37:52]  one or two cuz those are going to be
[08:37:50 -> 08:37:54]  individual elements right when we're
[08:37:52 -> 08:37:56]  looking at the actual data cuz remember
[08:37:54 -> 08:37:59]  this is how we're actually indexing data
[08:37:56 -> 08:38:02]  we want to make sure that uh we're
[08:37:59 -> 08:38:03]  keeping this block length in mind CU
[08:38:02 -> 08:38:06]  block size is Big right so we're going
[08:38:03 -> 08:38:09]  to advance
[08:38:06 -> 08:38:11]  um block size times the number of blocks
[08:38:09 -> 08:38:13]  and then the offsets are actually what's
[08:38:11 -> 08:38:16]  important so it's like whatever that
[08:38:13 -> 08:38:20]  number is and then we're going to
[08:38:16 -> 08:38:24]  arrange an additional thing of 02 block
[08:38:20 -> 08:38:26]  size so for like say 64 to 128 you're
[08:38:24 -> 08:38:28]  going to start start the block start is
[08:38:26 -> 08:38:32]  going to be 64 and then we're going to
[08:38:28 -> 08:38:35]  arrange a bunch of different uh indices
[08:38:32 -> 08:38:38]  between 64 uh which essentially just
[08:38:35 -> 08:38:41]  going to be an array of 64 up to one up
[08:38:38 -> 08:38:43]  to 128 right and uh that's going to
[08:38:41 -> 08:38:45]  that's how we're going to index our data
[08:38:43 -> 08:38:47]  so we do Triton language. a range zero
[08:38:45 -> 08:38:48]  to block size and that's what that is
[08:38:47 -> 08:38:50]  and then we just add whatever block
[08:38:48 -> 08:38:52]  start is to that so hopefully that kind
[08:38:50 -> 08:38:54]  of like makes sense in your head now
[08:38:52 -> 08:38:57]  jumping back to P here you might want to
[08:38:54 -> 08:38:59]  pay attention attention to this term so
[08:38:57 -> 08:39:02]  in here we have axis and Builder don't
[08:38:59 -> 08:39:08]  worry about this just worry about axis
[08:39:02 -> 08:39:13]  so axis is like uh block idx dox right
[08:39:08 -> 08:39:17]  um block block like X is the same as
[08:39:13 -> 08:39:20]  axis zero a uh block idx Y is the same
[08:39:17 -> 08:39:23]  as axis one and Z is equal to three
[08:39:20 -> 08:39:26]  right so we have to keep this in mind
[08:39:23 -> 08:39:28]  when we're writing more like you know 2D
[08:39:26 -> 08:39:31]  you know 3D spatial uh kernels then we
[08:39:28 -> 08:39:33]  have to keep in mind this access term so
[08:39:31 -> 08:39:34]  luckily right now this is very simple
[08:39:33 -> 08:39:36]  but this is something we're going to
[08:39:34 -> 08:39:39]  want to keep in mind if if would you end
[08:39:36 -> 08:39:41]  up writing uh this is something you want
[08:39:39 -> 08:39:45]  to keep in mind if you end up writing uh
[08:39:41 -> 08:39:47]  more you know 2D 3D stuff um now going
[08:39:45 -> 08:39:51]  down a little bit um we have this mask
[08:39:47 -> 08:39:54]  term so mask equals
[08:39:51 -> 08:39:57]  offsets mask equals whatever the Boolean
[08:39:54 -> 08:40:01]  uh I guess the the the Boolean output of
[08:39:57 -> 08:40:04]  whatever uh of if offsets is less than
[08:40:01 -> 08:40:08]  num elements so what this essentially
[08:40:04 -> 08:40:11]  does is we have these offsets which are
[08:40:08 -> 08:40:14]  the indices in the actual arrays itself
[08:40:11 -> 08:40:16]  so the input and the output Point input
[08:40:14 -> 08:40:18]  sorry the X and Y pointers whichever
[08:40:16 -> 08:40:20]  indices we're at in those we want to
[08:40:18 -> 08:40:22]  make sure that those do not surpass uh
[08:40:20 -> 08:40:25]  we don't we want to make sure those
[08:40:22 -> 08:40:27]  don't do not equal to or uh pass num
[08:40:25 -> 08:40:29]  elements right it's going to be zero up
[08:40:27 -> 08:40:31]  to num elements minus one so we want to
[08:40:29 -> 08:40:34]  make sure that we mask everything off
[08:40:31 -> 08:40:36]  that is num elements or past that point
[08:40:34 -> 08:40:38]  right and that's what this is so mask is
[08:40:36 -> 08:40:41]  just going to be essentially an array
[08:40:38 -> 08:40:43]  it's going to do uh like it's
[08:40:41 -> 08:40:44]  essentially going to uh say you know we
[08:40:43 -> 08:40:47]  have all these offsets we have this
[08:40:44 -> 08:40:51]  giant like array in memory and we're
[08:40:47 -> 08:40:56]  going to see um if like we have a you
[08:40:51 -> 08:40:58]  know 64 65 67 122 whatever we want to
[08:40:56 -> 08:41:01]  make sure that those numbers are less
[08:40:58 -> 08:41:02]  than a numb elements right um so we want
[08:41:01 -> 08:41:04]  to essentially just making sure that
[08:41:02 -> 08:41:06]  we're not accessing something that's
[08:41:04 -> 08:41:08]  outside of uh our data structure in
[08:41:06 -> 08:41:10]  memory we have this whole Space we want
[08:41:08 -> 08:41:13]  to keep it masked to this one section
[08:41:10 -> 08:41:15]  right that's what this does and so we
[08:41:13 -> 08:41:17]  get a new array which is mask and that's
[08:41:15 -> 08:41:20]  just a bunch of essentially just a bunch
[08:41:17 -> 08:41:21]  of ones and zeros um and this this
[08:41:20 -> 08:41:25]  behaves the same way that this if
[08:41:21 -> 08:41:28]  statement does inside of an addition
[08:41:25 -> 08:41:32]  Vector Edition kernel in Cuda right same
[08:41:28 -> 08:41:35]  idea um now going down further um we
[08:41:32 -> 08:41:38]  actually load these efficiently into a
[08:41:35 -> 08:41:41]  shared memory so don't
[08:41:38 -> 08:41:43]  um don't worry about how it actually how
[08:41:41 -> 08:41:45]  Triton handles data how how it you know
[08:41:43 -> 08:41:46]  transfers data that's a that's an
[08:41:45 -> 08:41:49]  optimization that's taken care of for
[08:41:46 -> 08:41:52]  you so just assume that these are going
[08:41:49 -> 08:41:56]  to load in um like starting from the
[08:41:52 -> 08:42:00]  like the beginning of X up to
[08:41:56 -> 08:42:02]  um up to the end so it's going to
[08:42:00 -> 08:42:04]  essentially just each each point each
[08:42:02 -> 08:42:05]  point in memory it's going to each each
[08:42:04 -> 08:42:08]  data point in memory it's going to load
[08:42:05 -> 08:42:10]  those in and it's going to do a mask and
[08:42:08 -> 08:42:12]  it's going to say do we want to actually
[08:42:10 -> 08:42:14]  compute these values or not that's what
[08:42:12 -> 08:42:16]  this is doing so whichever point it
[08:42:14 -> 08:42:17]  starts at all the way up to all the
[08:42:16 -> 08:42:19]  different offsets so it's like this
[08:42:17 -> 08:42:21]  point and then it's copied and then all
[08:42:19 -> 08:42:23]  the different offsets um and then you
[08:42:21 -> 08:42:25]  have a mask applied to those as well to
[08:42:23 -> 08:42:27]  say which ones do we want want to
[08:42:25 -> 08:42:30]  compute so if these ones are like extra
[08:42:27 -> 08:42:31]  just like compute this section um that
[08:42:30 -> 08:42:33]  that's what that's what this uh load is
[08:42:31 -> 08:42:37]  going to do and it's going to
[08:42:33 -> 08:42:39]  efficiently load this into um shared
[08:42:37 -> 08:42:42]  memory so the the the fast memory on the
[08:42:39 -> 08:42:44]  GPU the one on the actual uh streaming
[08:42:42 -> 08:42:46]  multiprocessor that that's what that's
[08:42:44 -> 08:42:49]  what this is taking care of for you and
[08:42:46 -> 08:42:53]  then we do the exact same thing for y
[08:42:49 -> 08:42:55]  now try and Implement um these blockwise
[08:42:53 -> 08:42:57]  operations very efficiently so you don't
[08:42:55 -> 08:43:00]  actually need to do any advanced stuff
[08:42:57 -> 08:43:03]  you literally just um have these loaded
[08:43:00 -> 08:43:05]  onto SRAM and it'll do a uh element wise
[08:43:03 -> 08:43:08]  addition so it'll it'll go through each
[08:43:05 -> 08:43:10]  one and just add together and notice how
[08:43:08 -> 08:43:12]  this is blue instead of red that means
[08:43:10 -> 08:43:15]  like it's taking care of by uh Tron for
[08:43:12 -> 08:43:19]  us so um that's that's literally how you
[08:43:15 -> 08:43:21]  compute the output um and then we just
[08:43:19 -> 08:43:23]  store this back again with another um
[08:43:21 -> 08:43:25]  you know with another with another data
[08:43:23 -> 08:43:27]  transfer operation
[08:43:25 -> 08:43:30]  find and so that's just going to be the
[08:43:27 -> 08:43:32]  same idea as here so uh the starting
[08:43:30 -> 08:43:35]  place in memory plus you know the offset
[08:43:32 -> 08:43:37]  of all the offset indices for for that
[08:43:35 -> 08:43:40]  block
[08:43:37 -> 08:43:43]  um the output itself so we're going to
[08:43:40 -> 08:43:46]  just store the output
[08:43:43 -> 08:43:49]  um and then have this mask as we did
[08:43:46 -> 08:43:51]  here and that's how a tri kernel works
[08:43:49 -> 08:43:53]  so hopefully this makes sense feel free
[08:43:51 -> 08:43:56]  to rewatch some parts if they didn't
[08:43:53 -> 08:43:57]  entirely you know click in your head uh
[08:43:56 -> 08:44:01]  but now we're actually going to jump
[08:43:57 -> 08:44:04]  into the uh softmax Trident function
[08:44:01 -> 08:44:05]  okay so now we jump into softmax and
[08:44:04 -> 08:44:07]  instead of just jumping right into code
[08:44:05 -> 08:44:09]  I'm going to do a manual like hand
[08:44:07 -> 08:44:11]  example with this
[08:44:09 -> 08:44:13]  um
[08:44:11 -> 08:44:17]  so let
[08:44:13 -> 08:44:18]  me delete that real quick um essentially
[08:44:17 -> 08:44:20]  what we're doing here I have a C file
[08:44:18 -> 08:44:23]  we're just doing this in C to understand
[08:44:20 -> 08:44:26]  what it's doing intuitively um we have
[08:44:23 -> 08:44:31]  an array of floats 1 to three so it it
[08:44:26 -> 08:44:31]  literally just looks like this um
[08:44:31 -> 08:44:37]  x x is that and so we're going to
[08:44:34 -> 08:44:37]  calculate the soft Max of
[08:44:42 -> 08:44:45]  X
[08:44:46 -> 08:44:53]  um so how this typically goes is if we
[08:44:49 -> 08:44:57]  search this up on uh on
[08:44:53 -> 08:44:59]  Google soft Max activation function it
[08:44:57 -> 08:45:01]  looks like this right so you have this
[08:44:59 -> 08:45:04]  exponentiate this input so you have an
[08:45:01 -> 08:45:07]  input Vector um and this this this
[08:45:04 -> 08:45:09]  symbol is a softmax function and you go
[08:45:07 -> 08:45:11]  over each one uh and then you
[08:45:09 -> 08:45:13]  essentially see how much each
[08:45:11 -> 08:45:16]  exponentiated value contributes to the
[08:45:13 -> 08:45:19]  total uh the total sum of all the
[08:45:16 -> 08:45:24]  exponential all the exponentiated values
[08:45:19 -> 08:45:27]  right so if we do if we open here uh if
[08:45:24 -> 08:45:27]  I just open I
[08:45:28 -> 08:45:35]  python if we import math and go say we
[08:45:31 -> 08:45:38]  go math.exp of
[08:45:35 -> 08:45:39]  1.0 we're going to get 2.71 because
[08:45:38 -> 08:45:43]  that's what e is right it's going to be
[08:45:39 -> 08:45:46]  e to the 1 um so our first
[08:45:43 -> 08:45:48]  value is going to be
[08:45:46 -> 08:45:52]  uh
[08:45:48 -> 08:45:57]  2.71 and then the second
[08:45:52 -> 08:45:57]  one is going to be two so 7 say
[08:45:58 -> 08:46:05]  7.39 and then the third one number uh
[08:46:01 -> 08:46:05]  three is going to be 20.1
[08:46:08 -> 08:46:14]  roughly now we sum these Al together
[08:46:15 -> 08:46:22]  so it's trying to autocomplete for me
[08:46:19 -> 08:46:26]  2.71 +
[08:46:22 -> 08:46:30]  7.39 + 20.1 0 and we get
[08:46:26 -> 08:46:32]  30.2 right now in order to get the
[08:46:30 -> 08:46:34]  actual softmax output we see how much
[08:46:32 -> 08:46:36]  each of these each of these
[08:46:34 -> 08:46:39]  exponentiated values contributes to the
[08:46:36 -> 08:46:40]  exponentiated sum of all of them um well
[08:46:39 -> 08:46:42]  not exponentiated sum but when you
[08:46:40 -> 08:46:46]  exponentiate all of them you take the
[08:46:42 -> 08:46:47]  sum of those which is 32.2 or 30.2 and
[08:46:46 -> 08:46:50]  then you you see how much each
[08:46:47 -> 08:46:57]  contributes so
[08:46:50 -> 08:47:01]  2.71 / 30.2 so that about 9% roughly so
[08:46:57 -> 08:47:03]  we're going to go um
[08:47:01 -> 08:47:06]  0.09
[08:47:03 -> 08:47:10]  um and then we go
[08:47:06 -> 08:47:10]  7.39 is
[08:47:10 -> 08:47:17]  0.24 and then the last one's going to be
[08:47:13 -> 08:47:21]  67 if we go and do it
[08:47:17 -> 08:47:22]  uh if you round up it's 67 so you have
[08:47:21 -> 08:47:29]  that right and then if we add these all
[08:47:22 -> 08:47:32]  these numbers together 0.0 09 + 0.24 +
[08:47:29 -> 08:47:35]  0.67 you get 1.0 right and these are all
[08:47:32 -> 08:47:38]  rounded of course um but but but the
[08:47:35 -> 08:47:39]  point here is this is the softmax notice
[08:47:38 -> 08:47:41]  how we had this this initial
[08:47:39 -> 08:47:44]  distribution here and then we went to a
[08:47:41 -> 08:47:46]  new one which kind of just made
[08:47:44 -> 08:47:49]  everything add up to one and uh gave it
[08:47:46 -> 08:47:52]  a bit of gave it a bit of extra weight
[08:47:49 -> 08:47:55]  to the three right so notice how like a
[08:47:52 -> 08:47:59]  like three is three times that
[08:47:55 -> 08:48:04]  1.0 um the
[08:47:59 -> 08:48:06]  6.67 is way more than 3 times uh 0.09 so
[08:48:04 -> 08:48:07]  you kind of have that extra waiting for
[08:48:06 -> 08:48:10]  the bigger values right and that's what
[08:48:07 -> 08:48:12]  the softmax does um so it's going to
[08:48:10 -> 08:48:14]  like essentially find like the biggest
[08:48:12 -> 08:48:16]  number and and um put a lot of weight on
[08:48:14 -> 08:48:20]  it or highlight it the
[08:48:16 -> 08:48:23]  most um but there is a flaw with this
[08:48:20 -> 08:48:26]  approach and I'm going to explain this
[08:48:23 -> 08:48:30]  right now so essentially the flaw here
[08:48:26 -> 08:48:33]  is if you have say um if you have
[08:48:30 -> 08:48:36]  X as say
[08:48:33 -> 08:48:37]  1
[08:48:36 -> 08:48:39]  1,000
[08:48:37 -> 08:48:42]  zero
[08:48:39 -> 08:48:42]  and
[08:48:44 -> 08:48:50]  uh negative 1,000 right so some of these
[08:48:48 -> 08:48:52]  the sum of these is zero right you have
[08:48:50 -> 08:48:55]  you have a problem with that and and
[08:48:52 -> 08:48:58]  that essentially means this is is going
[08:48:55 -> 08:49:01]  to um this is going to contribute a big
[08:48:58 -> 08:49:04]  chunk of this or at least it's supposed
[08:49:01 -> 08:49:06]  to It's supposed to contribute all of it
[08:49:04 -> 08:49:09]  um but it's not going to because you
[08:49:06 -> 08:49:11]  cannot contribute like it just doesn't
[08:49:09 -> 08:49:13]  make sense how can you contribute like
[08:49:11 -> 08:49:14]  what what percentage does a th
[08:49:13 -> 08:49:17]  contribute to zero it's like Infinity
[08:49:14 -> 08:49:22]  right so if you end up doing softmax on
[08:49:17 -> 08:49:25]  this um like even if you try to math.
[08:49:22 -> 08:49:27]  exp of 1,000 you're going to get math
[08:49:25 -> 08:49:29]  out of overflow error math range error
[08:49:27 -> 08:49:31]  because it's just too big of a number
[08:49:29 -> 08:49:34]  right
[08:49:31 -> 08:49:37]  so what you can actually do is you can
[08:49:34 -> 08:49:40]  subtract by the max of of all of these
[08:49:37 -> 08:49:44]  right so we can see
[08:49:40 -> 08:49:46]  um what is the maximum uh what is the
[08:49:44 -> 08:49:49]  maximum number here so the biggest one
[08:49:46 -> 08:49:51]  is a th and so what we can do is we can
[08:49:49 -> 08:49:55]  subtract each number by that value so we
[08:49:51 -> 08:49:57]  go we can say x we'll just say like X2
[08:49:55 -> 08:49:58]  equals
[08:49:57 -> 08:50:04]  um
[08:49:58 -> 08:50:07]  0 and then 0 - 1,000
[08:50:04 -> 08:50:13]  is th000 and then 1,00 minus th000 is
[08:50:07 -> 08:50:13]  2,000 right and so if we go
[08:50:15 -> 08:50:22]  um we go math.exp of
[08:50:22 -> 08:50:29]  zero notice we get one
[08:50:25 -> 08:50:31]  math. exp of 1000 is going to give us
[08:50:29 -> 08:50:32]  well pretty close to zero it's not quite
[08:50:31 -> 08:50:34]  going to be zero but it's going to be
[08:50:32 -> 08:50:37]  very very it's going to be a very small
[08:50:34 -> 08:50:38]  number right and then of course the same
[08:50:37 -> 08:50:40]  thing with 2,000 is just going to be the
[08:50:38 -> 08:50:43]  same right it's going to be very small
[08:50:40 -> 08:50:45]  and so you notice how if we if we
[08:50:43 -> 08:50:48]  originally just if we just did this one
[08:50:45 -> 08:50:51]  normally using these big numbers would
[08:50:48 -> 08:50:55]  have actually given us errors so we can
[08:50:51 -> 08:50:56]  subtract by the max we can we can um
[08:50:55 -> 08:50:58]  you know pointwise subtract by the max
[08:50:56 -> 08:51:00]  number and then we get something that
[08:50:58 -> 08:51:01]  makes more sense so you know zero
[08:51:00 -> 08:51:05]  contributes to all of it because that's
[08:51:01 -> 08:51:08]  what the that's what the the sum
[08:51:05 -> 08:51:10]  is and so this is how we actually follow
[08:51:08 -> 08:51:12]  through in the softmax function in C so
[08:51:10 -> 08:51:15]  we have three separate for Loops here
[08:51:12 -> 08:51:17]  the first one finds the max value in the
[08:51:15 -> 08:51:19]  array so we just set the initial max
[08:51:17 -> 08:51:22]  value to be the first number and then we
[08:51:19 -> 08:51:23]  iterate through this uh starting at the
[08:51:22 -> 08:51:24]  next number we don't want to start at
[08:51:23 -> 08:51:26]  the first we don't want to start at the
[08:51:24 -> 08:51:27]  first one we started the one after it
[08:51:26 -> 08:51:29]  cuz we already we already know what this
[08:51:27 -> 08:51:31]  one is um and we essentially just
[08:51:29 -> 08:51:34]  compare if if the new one is bigger than
[08:51:31 -> 08:51:36]  the max we set the max to that one um
[08:51:34 -> 08:51:39]  and we end up with this this Max term
[08:51:36 -> 08:51:42]  which you know same as we did up here
[08:51:39 -> 08:51:45]  and then we have this we have the
[08:51:42 -> 08:51:49]  accumulation sum so we iterate through
[08:51:45 -> 08:51:52]  the length of the array and we do that
[08:51:49 -> 08:51:55]  index minus the maximum and then we we
[08:51:52 -> 08:51:57]  add that total to the sum right we we
[08:51:55 -> 08:51:58]  add whatever this was to the sum so that
[08:51:57 -> 08:52:02]  we can you know accumulate all of it and
[08:51:58 -> 08:52:05]  then we just write out
[08:52:02 -> 08:52:06]  um we just write out the same value we
[08:52:05 -> 08:52:09]  we essentially just take the input and
[08:52:06 -> 08:52:12]  then replace it one by one uh and we set
[08:52:09 -> 08:52:14]  it to uh whatever that exponentiated
[08:52:12 -> 08:52:19]  value is uh
[08:52:14 -> 08:52:21]  minus uh or sorry not minus but we we
[08:52:19 -> 08:52:23]  have that exponen exponentiated value we
[08:52:21 -> 08:52:24]  divide that by the sum uh very simple I
[08:52:23 -> 08:52:27]  probably could have explained that in in
[08:52:24 -> 08:52:29]  a bit shorter time but I just want you
[08:52:27 -> 08:52:30]  to like gain the int intuition for that
[08:52:29 -> 08:52:32]  you've probably already written the soft
[08:52:30 -> 08:52:34]  Max and Pi torch and nump I get that uh
[08:52:32 -> 08:52:36]  but just to kind of provide that review
[08:52:34 -> 08:52:39]  as to how it works under the hood and
[08:52:36 -> 08:52:42]  this uh you know numerical stability
[08:52:39 -> 08:52:46]  add-on we have um so
[08:52:42 -> 08:52:49]  now I have an example in uh Cuda as well
[08:52:46 -> 08:52:53]  so I'm not going to go over this um but
[08:52:49 -> 08:52:55]  the idea here is uh in Cuda mean yes you
[08:52:53 -> 08:52:57]  could just you could do
[08:52:55 -> 08:53:00]  um you could go over you know the length
[08:52:57 -> 08:53:01]  of one single array but in deep learning
[08:53:00 -> 08:53:02]  you're not actually trying to do that
[08:53:01 -> 08:53:04]  you're typically going to have a batch
[08:53:02 -> 08:53:05]  size and this batch size is going to
[08:53:04 -> 08:53:07]  have like a bunch of these it's going to
[08:53:05 -> 08:53:08]  be a batch of arrays that you're going
[08:53:07 -> 08:53:10]  to softmax right and you're going to do
[08:53:08 -> 08:53:13]  the softmax on them rowwise you're going
[08:53:10 -> 08:53:15]  to do here here here uh right instead of
[08:53:13 -> 08:53:17]  just like to each element so it's going
[08:53:15 -> 08:53:19]  to go through the rows and this is where
[08:53:17 -> 08:53:21]  we can actually get a speed up from Cuda
[08:53:19 -> 08:53:23]  because we can give each each of the
[08:53:21 -> 08:53:26]  threads we know we have batch size many
[08:53:23 -> 08:53:29]  threads and we give each of them uh we
[08:53:26 -> 08:53:31]  give each of them a softmax job right so
[08:53:29 -> 08:53:33]  we see in
[08:53:31 -> 08:53:36]  here
[08:53:33 -> 08:53:41]  how we see in here how we actually give
[08:53:36 -> 08:53:43]  one to each um we we we give each we
[08:53:41 -> 08:53:46]  give each thread a separate softmax drop
[08:53:43 -> 08:53:48]  that's going to go it's going to span
[08:53:46 -> 08:53:51]  1,24 iterations or goes up to the length
[08:53:48 -> 08:53:53]  of n right as we can see in these so
[08:53:51 -> 08:53:55]  like theoretically it's going to take 3
[08:53:53 -> 08:53:57]  n iteration because has to go three
[08:53:55 -> 08:53:59]  different for Loops um that's that's
[08:53:57 -> 08:54:00]  kind of how that works there so we want
[08:53:59 -> 08:54:03]  to we want to pay attention to batch
[08:54:00 -> 08:54:05]  sizes right um but there's there's an
[08:54:03 -> 08:54:07]  example in in Cuda here which you can
[08:54:05 -> 08:54:10]  look over on your own and and get an in
[08:54:07 -> 08:54:12]  intuition for the batch St Max but it's
[08:54:10 -> 08:54:16]  very similar to the one we didn't
[08:54:12 -> 08:54:19]  see now we jump into the one in Triton
[08:54:16 -> 08:54:22]  okay so now we actually go into the uh
[08:54:19 -> 08:54:23]  softmax Trion kernel itself uh a little
[08:54:22 -> 08:54:25]  bit more advanced in Vector Edition but
[08:54:23 -> 08:54:28]  we're going to go through step by step
[08:54:25 -> 08:54:30]  so we obviously import at the top I'm
[08:54:28 -> 08:54:32]  going to go sort of from the bottom here
[08:54:30 -> 08:54:34]  explain what's happening and then what
[08:54:32 -> 08:54:37]  we're going as we're calculating the the
[08:54:34 -> 08:54:41]  output um so we set our manual seed we
[08:54:37 -> 08:54:45]  make a random uh normally
[08:54:41 -> 08:54:49]  uh normally distributed random tensor of
[08:54:45 -> 08:54:52]  floats um so mean zero and variance one
[08:54:49 -> 08:54:55]  so it's a normal distribution and all of
[08:54:52 -> 08:55:00]  the uh essentially the shape is is batch
[08:54:55 -> 08:55:04]  size by um by n so B by n batch size is
[08:55:00 -> 08:55:08]  256 n is 1,24 elements long we're just
[08:55:04 -> 08:55:10]  going to softmax um those 1024 element
[08:55:08 -> 08:55:13]  rows uh and we're going to do this on
[08:55:10 -> 08:55:16]  Cuda now we're going to calculate and
[08:55:13 -> 08:55:17]  make sure that torch and TR now put the
[08:55:16 -> 08:55:20]  same thing so typically you would do
[08:55:17 -> 08:55:23]  torch. softmax or or f. soft Max or
[08:55:20 -> 08:55:26]  whatever and then you would go um Dem
[08:55:23 -> 08:55:27]  equals 1 right and then we want to do
[08:55:26 -> 08:55:30]  that same thing for Trident and then
[08:55:27 -> 08:55:31]  make sure that um you know the max value
[08:55:30 -> 08:55:33]  isn't too ridiculous we're going to
[08:55:31 -> 08:55:35]  print that out then we're going to make
[08:55:33 -> 08:55:40]  sure that this is this is all close with
[08:55:35 -> 08:55:42]  torch all close right um so we do the
[08:55:40 -> 08:55:45]  Triton Triton softmax we go
[08:55:42 -> 08:55:48]  here and in here we have uh rows and
[08:55:45 -> 08:55:53]  columns so when we print out um I'm just
[08:55:48 -> 08:55:56]  going to print out uh input shape so we
[08:55:53 -> 08:55:56]  go here
[08:55:59 -> 08:56:05]  input shape is as we want right
[08:56:05 -> 08:56:11]  um output we're just going to do a malic
[08:56:08 -> 08:56:13]  essentially right there and then uh
[08:56:11 -> 08:56:15]  instead of doing some weird like meta
[08:56:13 -> 08:56:17]  parameter uh like doing like the whole
[08:56:15 -> 08:56:19]  like Lambda function calculating we're
[08:56:17 -> 08:56:22]  just going to do Tron and then next
[08:56:19 -> 08:56:24]  power of two so what this says is it
[08:56:22 -> 08:56:26]  Returns the smallest Power of Two
[08:56:24 -> 08:56:27]  greater than or equal to n so similar to
[08:56:26 -> 08:56:29]  what we were doing before but just a
[08:56:27 -> 08:56:31]  little simpler um and then we're going
[08:56:29 -> 08:56:37]  to we're going to set block size equal
[08:56:31 -> 08:56:39]  to the minimum of 1024 and this right um
[08:56:37 -> 08:56:42]  so you can kind of see how we you can
[08:56:39 -> 08:56:46]  kind of see why we why we would do that
[08:56:42 -> 08:56:49]  now we set our grid to n rows because in
[08:56:46 -> 08:56:51]  our grid we want a block for each row
[08:56:49 -> 08:56:54]  right when we're processing in parallel
[08:56:51 -> 08:56:57]  we want we we can't uh we can't split a
[08:56:54 -> 08:56:59]  single row amongst multiple um or at
[08:56:57 -> 08:57:01]  least naively we can't split it across
[08:56:59 -> 08:57:04]  multiple threads so we' want to do it uh
[08:57:01 -> 08:57:06]  for each block so each different um each
[08:57:04 -> 08:57:08]  different row would get its own block
[08:57:06 -> 08:57:10]  essentially and that's how we launch the
[08:57:08 -> 08:57:13]  grid here um and then you could assume
[08:57:10 -> 08:57:16]  that you know why we have this um why we
[08:57:13 -> 08:57:18]  have this trailing is just that the the
[08:57:16 -> 08:57:21]  trailing Dimensions you can just assume
[08:57:18 -> 08:57:23]  are one right and so that's how like Pi
[08:57:21 -> 08:57:26]  torch shap shapes work if you do like
[08:57:23 -> 08:57:28]  batch comma it'll be like B by one and
[08:57:26 -> 08:57:30]  so it'll just be like a column Vector
[08:57:28 -> 08:57:33]  sort of you you could think of it that
[08:57:30 -> 08:57:35]  way uh and then the softmax kernel we do
[08:57:33 -> 08:57:37]  this we call this the same way as we did
[08:57:35 -> 08:57:41]  Vector ad except we add two more things
[08:57:37 -> 08:57:42]  in so we add in this x. stride this this
[08:57:41 -> 08:57:44]  is just an input parameter to what
[08:57:42 -> 08:57:48]  what's going to be upstairs here so we
[08:57:44 -> 08:57:52]  got x. stride which is the essentially
[08:57:48 -> 08:57:54]  the The Stride of of the row so we when
[08:57:52 -> 08:57:56]  we um when we go across it's like it's
[08:57:54 -> 08:57:59]  stacked in memory it's like Row one row
[08:57:56 -> 08:58:01]  two Row three row four right um but the
[08:57:59 -> 08:58:03]  stride is like how far do you have to go
[08:58:01 -> 08:58:05]  to wrap around to a new one so when we
[08:58:03 -> 08:58:07]  say we want to go to row four um you
[08:58:05 -> 08:58:09]  actually have to go okay well what is
[08:58:07 -> 08:58:11]  the what is the strride right how long
[08:58:09 -> 08:58:12]  do you have to go across to get to the
[08:58:11 -> 08:58:17]  next one it's like essentially the
[08:58:12 -> 08:58:19]  length um and that's you know we we we
[08:58:17 -> 08:58:21]  we could say n columns but we're just
[08:58:19 -> 08:58:25]  going to use the um you know objective
[08:58:21 -> 08:58:27]  attribute x. stride
[08:58:25 -> 08:58:31]  um passing and columns and then block
[08:58:27 -> 08:58:34]  size right so we go up and we see that
[08:58:31 -> 08:58:39]  we get the uh where did it
[08:58:34 -> 08:58:45]  go output X stride stride and columns
[08:58:39 -> 08:58:45]  and then block size uh output
[08:58:47 -> 08:58:53]  input stride stride and columns block
[08:58:50 -> 08:58:56]  size now we do the same we we we do the
[08:58:53 -> 08:58:59]  same like pram ID and and the the start
[08:58:56 -> 08:59:02]  pointers the same way
[08:58:59 -> 08:59:04]  um we calculate this normally so which
[08:59:02 -> 08:59:07]  block are we at right and then we have
[08:59:04 -> 08:59:10]  to advance a number of indices forward
[08:59:07 -> 08:59:13]  in memory so it's like we we essentially
[08:59:10 -> 08:59:14]  we take the initial uh input pointer so
[08:59:13 -> 08:59:19]  where does it start at where does where
[08:59:14 -> 08:59:22]  does a starting place in memory and then
[08:59:19 -> 08:59:26]  uh we add that
[08:59:22 -> 08:59:31]  to we we we take that and then we plus
[08:59:26 -> 08:59:33]  uh row idx so which row is it times the
[08:59:31 -> 08:59:36]  length of the row right so then we can
[08:59:33 -> 08:59:39]  get which row where is the starting row
[08:59:36 -> 08:59:40]  within that batch right so which row do
[08:59:39 -> 08:59:42]  we want to go to and then this whole
[08:59:40 -> 08:59:44]  thing is actually in the bigger picture
[08:59:42 -> 08:59:45]  which is the whole memory right that's
[08:59:44 -> 08:59:47]  that's what we're doing there and then
[08:59:45 -> 08:59:49]  same idea for the output uh start
[08:59:47 -> 08:59:50]  pointer so we're essentially just
[08:59:49 -> 08:59:54]  finding the place in memory where we
[08:59:50 -> 08:59:57]  start based on based on these three um
[08:59:54 -> 09:00:01]  and then we load in so uh you know we do
[08:59:57 -> 09:00:04]  the typical load so uh pointer and then
[09:00:01 -> 09:00:05]  we want to uh load in all of the indices
[09:00:04 -> 09:00:09]  so similar to how we were doing it
[09:00:05 -> 09:00:10]  before and then our mask is uh we're
[09:00:09 -> 09:00:13]  just going to make sure that this is
[09:00:10 -> 09:00:15]  smaller than n columns um right making
[09:00:13 -> 09:00:18]  sure that we don't go out of bounds and
[09:00:15 -> 09:00:19]  then this this important term other is
[09:00:18 -> 09:00:23]  is a bit critical as
[09:00:19 -> 09:00:23]  well so
[09:00:25 -> 09:00:29]  this other term means that if you have
[09:00:28 -> 09:00:33]  um for
[09:00:29 -> 09:00:36]  example if you have 1,000 elements in a
[09:00:33 -> 09:00:39]  row and the block size is24 so it's
[09:00:36 -> 09:00:40]  going to be a bit longer um then that
[09:00:39 -> 09:00:41]  means you're going to process some
[09:00:40 -> 09:00:44]  additional elements it's going to think
[09:00:41 -> 09:00:46]  that there's 24 extra ones at the end so
[09:00:44 -> 09:00:47]  what you can do I mean this is not this
[09:00:46 -> 09:00:49]  is not going to happen in our case
[09:00:47 -> 09:00:51]  because we've very objective about how
[09:00:49 -> 09:00:53]  we Define things but in some cases this
[09:00:51 -> 09:00:56]  will be important to pay attention to
[09:00:53 -> 09:00:58]  where um you actually want to explicitly
[09:00:56 -> 09:01:00]  set the edge so that it doesn't mess up
[09:00:58 -> 09:01:03]  everything else right if that was like
[09:01:00 -> 09:01:04]  one for example then in our whole
[09:01:03 -> 09:01:06]  softmax calculation that would
[09:01:04 -> 09:01:08]  contribute to it quite significantly
[09:01:06 -> 09:01:11]  because it's you know it's it's not just
[09:01:08 -> 09:01:13]  like massive sparse integers it's like a
[09:01:11 -> 09:01:16]  it's like decimal places that are around
[09:01:13 -> 09:01:18]  zero right um so it's important to do
[09:01:16 -> 09:01:20]  this when when we exponentiate when you
[09:01:18 -> 09:01:24]  do
[09:01:20 -> 09:01:30]  um if we go into uh python here
[09:01:24 -> 09:01:31]  import uh import math we go Um can just
[09:01:30 -> 09:01:36]  say
[09:01:31 -> 09:01:37]  x xal negative float and then
[09:01:36 -> 09:01:42]  INF
[09:01:37 -> 09:01:45]  right and then we go um math.exp
[09:01:42 -> 09:01:48]  X we notice that we get zero right where
[09:01:45 -> 09:01:51]  we we did math. exp of one we would get
[09:01:48 -> 09:01:52]  we would get this number so we just want
[09:01:51 -> 09:01:54]  to make sure that that is not
[09:01:52 -> 09:01:56]  contributing anything at all that that's
[09:01:54 -> 09:02:00]  all we're doing there
[09:01:56 -> 09:02:01]  um so all the other additional values if
[09:02:00 -> 09:02:03]  they happen we don't want those to
[09:02:01 -> 09:02:06]  contribute at all just kind of a safety
[09:02:03 -> 09:02:08]  thing it's good to look out for this um
[09:02:06 -> 09:02:09]  and then we just do the the the normal s
[09:02:08 -> 09:02:12]  softmax calculation which is actually
[09:02:09 -> 09:02:15]  only takes four lines um so we calculate
[09:02:12 -> 09:02:18]  the max across across you know that row
[09:02:15 -> 09:02:21]  and then we get the numerator which is
[09:02:18 -> 09:02:23]  remember we we subtract the Max from
[09:02:21 -> 09:02:25]  that uh we exponentiate this so we
[09:02:23 -> 09:02:26]  exponentiate whatever that whatever that
[09:02:25 -> 09:02:29]  result
[09:02:26 -> 09:02:31]  is and then we get the denominator which
[09:02:29 -> 09:02:33]  is a sum
[09:02:31 -> 09:02:36]  across each individual each individual
[09:02:33 -> 09:02:38]  one so there there's going to be some
[09:02:36 -> 09:02:40]  sum number and then we're going to do
[09:02:38 -> 09:02:42]  essentially this array it's going to be
[09:02:40 -> 09:02:45]  an array of elements and we're going to
[09:02:42 -> 09:02:47]  divide this by a scalar value so it's
[09:02:45 -> 09:02:49]  going to just like take this and it's
[09:02:47 -> 09:02:51]  going to divide divide divide divide
[09:02:49 -> 09:02:53]  divide right and it's and that's how we
[09:02:51 -> 09:02:56]  get our softmax output and then we'll
[09:02:53 -> 09:02:58]  see that we store this
[09:02:56 -> 09:03:01]  um similar to how we actually loaded
[09:02:58 -> 09:03:03]  things in in initially so instead of the
[09:03:01 -> 09:03:07]  row starting pointer it's the out row
[09:03:03 -> 09:03:10]  starting pointer same idea um we want to
[09:03:07 -> 09:03:12]  store a value which is the softmax
[09:03:10 -> 09:03:14]  output which is what we calculated here
[09:03:12 -> 09:03:18]  so that's the actual value uh inside of
[09:03:14 -> 09:03:20]  it um and then our mask is same as as we
[09:03:18 -> 09:03:22]  did up here so we we want to make sure
[09:03:20 -> 09:03:24]  that we're not uh doing reads and wrs
[09:03:22 -> 09:03:26]  out of bounds right and that's that's
[09:03:24 -> 09:03:30]  pretty much the entire softx calculation
[09:03:26 -> 09:03:31]  in Triton so you know I encourage you to
[09:03:30 -> 09:03:34]  uh you know play around with this you
[09:03:31 -> 09:03:36]  can do you can actually do
[09:03:34 -> 09:03:43]  um you can actually
[09:03:36 -> 09:03:47]  go uh t. device print and then we can go
[09:03:43 -> 09:03:48]  um like P and then just put like row idx
[09:03:47 -> 09:03:53]  or we could
[09:03:48 -> 09:03:53]  do row idx like this
[09:03:56 -> 09:04:01]  and we can actually print that
[09:03:58 -> 09:04:04]  out so you know maximum difference
[09:04:01 -> 09:04:06]  between TR TR pytorch and Tron results
[09:04:04 -> 09:04:09]  is is very small results are close is
[09:04:06 -> 09:04:12]  true um and then we can see each each
[09:04:09 -> 09:04:14]  individual one of these so like P ID is
[09:04:12 -> 09:04:18]  you know 255 this is in the X Dimension
[09:04:14 -> 09:04:20]  right so if you had like a you had like
[09:04:18 -> 09:04:24]  a
[09:04:20 -> 09:04:27]  um this is Axis so we do axis Z same
[09:04:24 -> 09:04:30]  thing we access of one then you know
[09:04:27 -> 09:04:33]  this would be on this on this spot
[09:04:30 -> 09:04:36]  instead of instead of X
[09:04:33 -> 09:04:39]  right so yeah we we print out whatever
[09:04:36 -> 09:04:40]  that value is and we get uh we we can
[09:04:39 -> 09:04:44]  print stuff out in the terminal if I do
[09:04:40 -> 09:04:44]  like normal just like
[09:04:45 -> 09:04:50]  save now it might be a good idea to
[09:04:47 -> 09:04:52]  actually dig into pytorch or not even
[09:04:50 -> 09:04:54]  dig into it but rather understand how
[09:04:52 -> 09:04:56]  you can add on top of it make things a
[09:04:54 -> 09:04:59]  little bit faster uh for your own custom
[09:04:56 -> 09:05:01]  use cases and whatnot so we just
[09:04:59 -> 09:05:02]  finished off the Trident chapter uh I
[09:05:01 -> 09:05:04]  hope you enjoyed that but now we're
[09:05:02 -> 09:05:07]  going to go a little bit more into
[09:05:04 -> 09:05:10]  Python and more on the torch side so
[09:05:07 -> 09:05:14]  I've written a few files here I have
[09:05:10 -> 09:05:16]  some descriptions about what's going on
[09:05:14 -> 09:05:19]  uh what's going on with all these like
[09:05:16 -> 09:05:21]  different types and names and stuff in
[09:05:19 -> 09:05:22]  uh in the readme as well as some
[09:05:21 -> 09:05:24]  intuitive examples of what we're going
[09:05:22 -> 09:05:27]  to be looking at
[09:05:24 -> 09:05:30]  I wrote a setup script for just
[09:05:27 -> 09:05:31]  compiling a separate Pythor extension uh
[09:05:30 -> 09:05:34]  a separate function that we're going to
[09:05:31 -> 09:05:38]  use to do a polinomial it's very simple
[09:05:34 -> 09:05:41]  operation um and then uh a Cuda script
[09:05:38 -> 09:05:43]  that is going to uh compile and bind
[09:05:41 -> 09:05:46]  into pytorch so that we can actually use
[09:05:43 -> 09:05:49]  it um in Python and then the python
[09:05:46 -> 09:05:52]  script itself which we Benchmark against
[09:05:49 -> 09:05:55]  uh naive pytorch so just all we're
[09:05:52 -> 09:05:58]  really doing here is x^2 + x + 1 that's
[09:05:55 -> 09:06:00]  all we're doing so going here we have
[09:05:58 -> 09:06:02]  this include and this is giving us
[09:06:00 -> 09:06:04]  errors but don't worry about this uh
[09:06:02 -> 09:06:06]  when we set up it's going to actually uh
[09:06:04 -> 09:06:09]  it's going to handle this properly so we
[09:06:06 -> 09:06:11]  don't actually need um a dedicated
[09:06:09 -> 09:06:14]  include file or or any arguments for
[09:06:11 -> 09:06:17]  this we don't need to specify where the
[09:06:14 -> 09:06:20]  extension. H file is um but going into
[09:06:17 -> 09:06:22]  the actual kernel itself um so just like
[09:06:20 -> 09:06:24]  top to bottom we have this new thing
[09:06:22 -> 09:06:28]  called a template I didn't go over this
[09:06:24 -> 09:06:32]  yet but uh well I I did I I guess I did
[09:06:28 -> 09:06:33]  in the uh in the manal section but uh
[09:06:32 -> 09:06:37]  template
[09:06:33 -> 09:06:38]  is it essentially if we go down it'll
[09:06:37 -> 09:06:41]  help if if we kind of go down and and
[09:06:38 -> 09:06:44]  look at where this supplies um so when
[09:06:41 -> 09:06:46]  we call this kernel we do this and then
[09:06:44 -> 09:06:48]  we provide our arguments right we have
[09:06:46 -> 09:06:50]  the kernel configuration here and then
[09:06:48 -> 09:06:52]  we have this this this is where the
[09:06:50 -> 09:06:56]  template comes in so we specify this
[09:06:52 -> 09:06:59]  scaler t type and that essentially means
[09:06:56 -> 09:07:03]  um we're just going to make sure
[09:06:59 -> 09:07:07]  that this uh the
[09:07:03 -> 09:07:10]  X the y or the the input the output uh
[09:07:07 -> 09:07:11]  are of this type and so this type is
[09:07:10 -> 09:07:13]  essentially going to be handled by P
[09:07:11 -> 09:07:17]  torch and instead of specifying like a
[09:07:13 -> 09:07:19]  float or or a double or like I don't
[09:07:17 -> 09:07:22]  know some other some other like maybe
[09:07:19 -> 09:07:24]  quantize like fp16 type um it's going to
[09:07:22 -> 09:07:26]  handle that for us and it's going to
[09:07:24 -> 09:07:28]  just automatically recognize which one
[09:07:26 -> 09:07:29]  it is and it's going to compile that
[09:07:28 -> 09:07:31]  down and deal with it appropriately so
[09:07:29 -> 09:07:33]  this is something built into py torch
[09:07:31 -> 09:07:35]  and it's custom type that we have uh so
[09:07:33 -> 09:07:38]  this is kind of just like the default to
[09:07:35 -> 09:07:41]  use here um easiest and then we have
[09:07:38 -> 09:07:43]  this restrict so restrict in short
[09:07:41 -> 09:07:46]  essentially means we're not going to be
[09:07:43 -> 09:07:49]  overlapping memory uh we're not going to
[09:07:46 -> 09:07:52]  be overlapping memory accesses so we
[09:07:49 -> 09:07:54]  have uh we have X here we have an output
[09:07:52 -> 09:07:57]  here all we're doing is we're doing an
[09:07:54 -> 09:08:00]  operation on this thing and then we're
[09:07:57 -> 09:08:02]  storing that in here um we're not we're
[09:08:00 -> 09:08:04]  not mixing things over we're not doing
[09:08:02 -> 09:08:06]  two things on like the same location and
[09:08:04 -> 09:08:08]  then having some output that's stored
[09:08:06 -> 09:08:10]  we're not doing anything messy like that
[09:08:08 -> 09:08:12]  so we can just say restrict and that'll
[09:08:10 -> 09:08:15]  allow the compiler to aggressively
[09:08:12 -> 09:08:19]  optimize U essentially what the binary
[09:08:15 -> 09:08:22]  code code is um and so in here we we
[09:08:19 -> 09:08:26]  simply just do a uh we do this over the
[09:08:22 -> 09:08:29]  X Dimension the typical Kuda kernel
[09:08:26 -> 09:08:33]  indexing um and then we do you know the
[09:08:29 -> 09:08:35]  square the plus and then the plus one so
[09:08:33 -> 09:08:36]  uh that's the I mean the the kernel is
[09:08:35 -> 09:08:38]  surprisingly simple there's just some
[09:08:36 -> 09:08:43]  new colors and new keywords to pay
[09:08:38 -> 09:08:44]  attention uh to pay attention for but uh
[09:08:43 -> 09:08:47]  then we scroll down a bit and then we
[09:08:44 -> 09:08:50]  got some C++ syntax going on what the
[09:08:47 -> 09:08:52]  heck does this do well we have this Auto
[09:08:50 -> 09:08:54]  which essentially figures out like which
[09:08:52 -> 09:08:56]  which type to this to so it's going to
[09:08:54 -> 09:08:57]  recognize that this is going to be some
[09:08:56 -> 09:08:59]  torch type and it's going to
[09:08:57 -> 09:09:01]  automatically select that and this is
[09:08:59 -> 09:09:04]  just torch. empty like so it's going to
[09:09:01 -> 09:09:07]  have the same shape as X the input um
[09:09:04 -> 09:09:08]  which is a torch tensor type so Auto is
[09:09:07 -> 09:09:12]  going to recognize it's going
[09:09:08 -> 09:09:14]  essentially going to uh it's going to
[09:09:12 -> 09:09:16]  it's going to do this we'll just leave
[09:09:14 -> 09:09:18]  it as Auto for now because it looks
[09:09:16 -> 09:09:21]  nicer um and then we have our threads
[09:09:18 -> 09:09:24]  which the typical 10,24 threads per
[09:09:21 -> 09:09:26]  block as per the maximum and then we
[09:09:24 -> 09:09:28]  have our you know numb elements plus
[09:09:26 -> 09:09:29]  threads minus one divided by number of
[09:09:28 -> 09:09:31]  threads so this is the typical we
[09:09:29 -> 09:09:33]  already went over this this should be
[09:09:31 -> 09:09:37]  very trivial stuff
[09:09:33 -> 09:09:40]  um and then we have um essentially just
[09:09:37 -> 09:09:43]  some some extra stuff about
[09:09:40 -> 09:09:45]  uh how the kernel is going to be called
[09:09:43 -> 09:09:49]  so which which what things are we
[09:09:45 -> 09:09:52]  feeding into it um we return an output
[09:09:49 -> 09:09:57]  based on that um and then down here we
[09:09:52 -> 09:09:58]  just have our are python bindings so uh
[09:09:57 -> 09:10:01]  don't worry about this too much just
[09:09:58 -> 09:10:04]  kind of trust the process um I can kind
[09:10:01 -> 09:10:08]  of read off of like what I was able to
[09:10:04 -> 09:10:13]  uh find about this exactly
[09:10:08 -> 09:10:16]  so um Pi bind 11 module
[09:10:13 -> 09:10:19]  this this is a macro that defines the
[09:10:16 -> 09:10:21]  entry point for the python module um
[09:10:19 -> 09:10:24]  torch extension
[09:10:21 -> 09:10:26]  name uh is it's a macro defined by
[09:10:24 -> 09:10:29]  pytorch that expands to the name of the
[09:10:26 -> 09:10:31]  extension so usually defined from the
[09:10:29 -> 09:10:35]  setup.py file over
[09:10:31 -> 09:10:38]  here then we have an m and an
[09:10:35 -> 09:10:41]  m.de so that adds a new function to the
[09:10:38 -> 09:10:43]  module so the first argument polinomial
[09:10:41 -> 09:10:46]  activation uh is the name of the
[09:10:43 -> 09:10:48]  function in Python so that's the
[09:10:46 -> 09:10:51]  function we end up calling be P
[09:10:48 -> 09:10:54]  polinomial activation open close
[09:10:51 -> 09:10:58]  parentheses um
[09:10:54 -> 09:11:02]  the polinomial activation Cuda pointer
[09:10:58 -> 09:11:05]  uh or it is it is a pointer to the C++
[09:11:02 -> 09:11:07]  function to be called and then the last
[09:11:05 -> 09:11:08]  argument is a dock string for the
[09:11:07 -> 09:11:11]  function so this it's just like a a
[09:11:08 -> 09:11:14]  simple doc string it's not like entirely
[09:11:11 -> 09:11:16]  required but we we put it there
[09:11:14 -> 09:11:19]  anyways
[09:11:16 -> 09:11:20]  so anyways this is the this is the full
[09:11:19 -> 09:11:22]  Cuda script this is essentially as
[09:11:20 -> 09:11:25]  simple as it gets you can't really do
[09:11:22 -> 09:11:27]  anything more simple than this uh so
[09:11:25 -> 09:11:31]  this is kind of like a template you can
[09:11:27 -> 09:11:33]  work off of and then in here we uh we
[09:11:31 -> 09:11:35]  import our compiled polinomial Cuda
[09:11:33 -> 09:11:38]  function which we'll compile in a second
[09:11:35 -> 09:11:42]  here we have a class so we use torch
[09:11:38 -> 09:11:44]  autograd and by default when we when we
[09:11:42 -> 09:11:45]  when we do an autograd when we do an
[09:11:44 -> 09:11:47]  autograd function we have to include a
[09:11:45 -> 09:11:49]  forward and backward method um these are
[09:11:47 -> 09:11:51]  both going to be static so we just add
[09:11:49 -> 09:11:54]  this decorator that says this is
[09:11:51 -> 09:11:57]  compiled elsewhere um
[09:11:54 -> 09:12:00]  and then forward we're going to do um
[09:11:57 -> 09:12:03]  just this polinomial Activation so comes
[09:12:00 -> 09:12:07]  from the bin error that we compiled and
[09:12:03 -> 09:12:11]  then uh polinomial activation of X right
[09:12:07 -> 09:12:12]  as we said before and then backward uh
[09:12:11 -> 09:12:14]  backward we just don't support it yet so
[09:12:12 -> 09:12:16]  we could just say not not implemented
[09:12:14 -> 09:12:18]  error backward pass not implemented um
[09:12:16 -> 09:12:20]  and then in here where we actually do
[09:12:18 -> 09:12:22]  our n and do module the py should be
[09:12:20 -> 09:12:24]  familiar to of course this this should
[09:12:22 -> 09:12:27]  look this should be super easy to
[09:12:24 -> 09:12:30]  understand um from a from a separate
[09:12:27 -> 09:12:34]  standpoint um but we just do the init we
[09:12:30 -> 09:12:37]  do the forward in here we we say you
[09:12:34 -> 09:12:38]  know implementation is going to be the P
[09:12:37 -> 09:12:41]  we just set it to a string P Tor so it's
[09:12:38 -> 09:12:44]  kind of easy to read um and then if the
[09:12:41 -> 09:12:48]  implementation is p torch we do this raw
[09:12:44 -> 09:12:50]  and if it's not then we do this uh if if
[09:12:48 -> 09:12:52]  it's Cuda then we do this this other one
[09:12:50 -> 09:12:53]  which we specified up here so it's going
[09:12:52 -> 09:12:57]  to do this
[09:12:53 -> 09:12:59]  um and it's going to apply that to X and
[09:12:57 -> 09:13:02]  then else we just we just say oh if you
[09:12:59 -> 09:13:05]  did like uh if you like missed the T it'
[09:13:02 -> 09:13:07]  be like oh you Pi orch is not
[09:13:05 -> 09:13:10]  implemented right unknown
[09:13:07 -> 09:13:12]  implementation and then so we go down
[09:13:10 -> 09:13:14]  and let's just go to the main function
[09:13:12 -> 09:13:15]  first then to the Benchmark so in here
[09:13:14 -> 09:13:18]  random
[09:13:15 -> 09:13:19]  seed normally distributed random tensor
[09:13:18 -> 09:13:22]  on
[09:13:19 -> 09:13:25]  device implementation on pytorch
[09:13:22 -> 09:13:28]  implementation on Cuda we move these to
[09:13:25 -> 09:13:30]  the device so to Cuda this is just like
[09:13:28 -> 09:13:32]  essentially uh saying that this function
[09:13:30 -> 09:13:33]  is going to be executed on that you
[09:13:32 -> 09:13:37]  should have probably seen this before
[09:13:33 -> 09:13:40]  it's not too bad um and then we do um we
[09:13:37 -> 09:13:42]  can actually just print out uh whatever
[09:13:40 -> 09:13:47]  that input
[09:13:42 -> 09:13:49]  was um and then here we do um like here
[09:13:47 -> 09:13:51]  we we essentially just Benchmark so we
[09:13:49 -> 09:13:53]  go um the actual activation function
[09:13:51 -> 09:13:55]  itself the input we're doing it on
[09:13:53 -> 09:13:58]  um and then just a string that we're
[09:13:55 -> 09:14:01]  going to print in the uh in the name
[09:13:58 -> 09:14:03]  here so here we just set a time we go
[09:14:01 -> 09:14:06]  for a number of
[09:14:03 -> 09:14:08]  runs uh and then we we we Cuda
[09:14:06 -> 09:14:10]  synchronize so we ensure everything is
[09:14:08 -> 09:14:12]  all caught up and then we end the time
[09:14:10 -> 09:14:15]  and then we essentially return the name
[09:14:12 -> 09:14:20]  so whatever whichever these it
[09:14:15 -> 09:14:22]  was um the the Delta time so like the
[09:14:20 -> 09:14:24]  essentially the not the Delta time but
[09:14:22 -> 09:14:25]  the difference um and then we divide
[09:14:24 -> 09:14:28]  that by the number of runs because we're
[09:14:25 -> 09:14:31]  doing an average times a th so that we
[09:14:28 -> 09:14:32]  can get it in milliseconds format um and
[09:14:31 -> 09:14:36]  that just that just kind of works as we
[09:14:32 -> 09:14:41]  expected to now to actually compile this
[09:14:36 -> 09:14:43]  um we go to uh we go to the read me and
[09:14:41 -> 09:14:47]  literally all you have to do is just
[09:14:43 -> 09:14:47]  python uh setup.py
[09:14:50 -> 09:14:57]  install give that a second it's going to
[09:14:53 -> 09:14:57]  use ninja to build
[09:14:58 -> 09:15:04]  um so we'll we'll give that a moment um
[09:15:02 -> 09:15:08]  but this is going to create see a build
[09:15:04 -> 09:15:08]  a build build folder in here
[09:15:24 -> 09:15:28]  and we'll print out um forward and
[09:15:27 -> 09:15:31]  backward in a second we're going we're
[09:15:28 -> 09:15:33]  going to print both of these
[09:15:31 -> 09:15:36]  so okay awesome it just it just wrote
[09:15:33 -> 09:15:38]  that out so now we can go ahead and
[09:15:36 -> 09:15:42]  python polinomial
[09:15:38 -> 09:15:45]  activation and it'll it'll do these this
[09:15:42 -> 09:15:47]  Cuda activation. forward of x uh and
[09:15:45 -> 09:15:50]  then it'll print whatever that output
[09:15:47 -> 09:15:51]  was and so we get like a tensor and it
[09:15:50 -> 09:15:53]  it actually formats this properly which
[09:15:51 -> 09:15:56]  you're like oh my gosh we just we wrote
[09:15:53 -> 09:15:58]  this in in in Cuda and c and now it
[09:15:56 -> 09:16:00]  prints so nicely like this yeah that's
[09:15:58 -> 09:16:03]  what pytorch does for you um and then we
[09:16:00 -> 09:16:07]  see that the pytorch built-in gets about
[09:16:03 -> 09:16:09]  10.47 milliseconds over uh over a th000
[09:16:07 -> 09:16:11]  runs on average and then the Cuda
[09:16:09 -> 09:16:13]  extension gets about
[09:16:11 -> 09:16:16]  0.0 243
[09:16:13 -> 09:16:20]  milliseconds so if we actually compare
[09:16:16 -> 09:16:24]  these so if I go
[09:16:20 -> 09:16:26]  um this divided by uh this we will see
[09:16:24 -> 09:16:28]  the speed up that we get from kud so we
[09:16:26 -> 09:16:31]  get about a
[09:16:28 -> 09:16:34]  431x speed up which is pretty good if
[09:16:31 -> 09:16:37]  you ask me especially on bigger tensors
[09:16:34 -> 09:16:40]  right this would be awesome uh so so
[09:16:37 -> 09:16:43]  that's that and if we actually do a DOT
[09:16:40 -> 09:16:45]  backward here you'll see that it's both
[09:16:43 -> 09:16:46]  you know it changed to White so that
[09:16:45 -> 09:16:49]  that's a good good sign that it doesn't
[09:16:46 -> 09:16:53]  work and if we actually run this it'll
[09:16:49 -> 09:16:56]  say uh has no attribute backward right
[09:16:53 -> 09:17:00]  um so essentially raise the attribute
[09:16:56 -> 09:17:02]  error um doesn't work and then we
[09:17:00 -> 09:17:02]  default back to
[09:17:04 -> 09:17:10]  forward and it'll pop back to this to
[09:17:07 -> 09:17:12]  this nn. module uh awesome okay so
[09:17:10 -> 09:17:14]  that's uh that's pytorch extensions for
[09:17:12 -> 09:17:17]  you you can feel free to add whatever
[09:17:14 -> 09:17:19]  you want to this um so you know if you
[09:17:17 -> 09:17:21]  have your own custom like research that
[09:17:19 -> 09:17:23]  you want to add and make it super easy
[09:17:21 -> 09:17:25]  for others and and yourself and your
[09:17:23 -> 09:17:27]  organization to use uh totally feel free
[09:17:25 -> 09:17:28]  to go down this route copy this copy
[09:17:27 -> 09:17:30]  this template code do whatever you want
[09:17:28 -> 09:17:33]  with it um this is just the easy the
[09:17:30 -> 09:17:38]  easiest example I found um to both you
[09:17:33 -> 09:17:42]  know write and explain so uh yeah that's
[09:17:38 -> 09:17:44]  that uh that is pretty much the
[09:17:42 -> 09:17:46]  first the first uh I don't even know I'm
[09:17:44 -> 09:17:49]  not going to unify a percent but that
[09:17:46 -> 09:17:51]  was the majority of the course now we
[09:17:49 -> 09:17:54]  actually have a final project to go into
[09:17:51 -> 09:17:57]  so this final project is super exciting
[09:17:54 -> 09:17:59]  and it's going to help you understand uh
[09:17:57 -> 09:18:03]  neural networks from scratch how to
[09:17:59 -> 09:18:05]  optimize them for performance um as well
[09:18:03 -> 09:18:06]  as like data loaders we're going to add
[09:18:05 -> 09:18:09]  a bunch of things a bunch of
[09:18:06 -> 09:18:11]  optimizations into making a real world
[09:18:09 -> 09:18:11]  training run
[09:18:12 -> 09:18:19]  work I am so so excited for this part in
[09:18:17 -> 09:18:21]  this last section of the course we're
[09:18:19 -> 09:18:23]  going to be doing the final project this
[09:18:21 -> 09:18:27]  final project is awesome we're going to
[09:18:23 -> 09:18:29]  do an mnist MLP training run from
[09:18:27 -> 09:18:31]  scratch it's going to go in the
[09:18:29 -> 09:18:33]  following order we're going to start in
[09:18:31 -> 09:18:35]  Python pytorch super easy right then
[09:18:33 -> 09:18:37]  we're going to go to numpy make it a
[09:18:35 -> 09:18:40]  little harder but understand what's
[09:18:37 -> 09:18:42]  going on under the hood and then we're
[09:18:40 -> 09:18:44]  going to go take our numpy code and Port
[09:18:42 -> 09:18:45]  that over to C right it's going to run
[09:18:44 -> 09:18:47]  on CPU it's going to be super easy to
[09:18:45 -> 09:18:49]  read and comprehend and understand right
[09:18:47 -> 09:18:51]  then we're going to push that over to
[09:18:49 -> 09:18:55]  Cuda and then we're going to rank it
[09:18:51 -> 09:18:57]  fast in Cuda awesome so navigate over to
[09:18:55 -> 09:18:59]  this folder we're going to do a
[09:18:57 -> 09:19:00]  different one so it's not going to be uh
[09:18:59 -> 09:19:02]  SL Cuda course it's going to be slash
[09:19:00 -> 09:19:04]  mnus Cuda is like a just a separate
[09:19:02 -> 09:19:05]  thing to help organize things better
[09:19:04 -> 09:19:08]  maybe you want to show this to a friend
[09:19:05 -> 09:19:09]  or whatever or or present as someone and
[09:19:08 -> 09:19:10]  this way you can kind of just have
[09:19:09 -> 09:19:12]  things separated out and neatly
[09:19:10 -> 09:19:14]  organized right so you can consider this
[09:19:12 -> 09:19:16]  as like kind of a separate part we're
[09:19:14 -> 09:19:18]  just kind of building on what we did
[09:19:16 -> 09:19:20]  previously uh so if we copy this we're
[09:19:18 -> 09:19:22]  going to go ahead and actually clone
[09:19:20 -> 09:19:24]  this into uh a new directory and I'll
[09:19:22 -> 09:19:26]  kind of walk you through how things are
[09:19:24 -> 09:19:29]  going to go so I'll go ahead and full
[09:19:26 -> 09:19:29]  screen this
[09:19:31 -> 09:19:39]  up sure already saved now I'm going to
[09:19:35 -> 09:19:42]  CD into that going to go uh UV VNV just
[09:19:39 -> 09:19:43]  like that I'm going to activate this I
[09:19:42 -> 09:19:44]  just create like a python virtual
[09:19:43 -> 09:19:48]  environment that's all I really did
[09:19:44 -> 09:19:54]  there uh and then we're going to go pip
[09:19:48 -> 09:19:56]  uh UV pip install d r
[09:19:54 -> 09:20:00]  requirements and it's going to go ahead
[09:19:56 -> 09:20:04]  and install everything that we need um
[09:20:00 -> 09:20:06]  okay sweet now we're going to pop into
[09:20:04 -> 09:20:10]  the uh we're actually just going to open
[09:20:06 -> 09:20:14]  this up in vs code go ahead and do that
[09:20:10 -> 09:20:18]  um now if we pop into the uh let me full
[09:20:14 -> 09:20:20]  screen this perfect okay so if we go
[09:20:18 -> 09:20:22]  into this python folder just to like
[09:20:20 -> 09:20:23]  help you navigate the structure of this
[09:20:22 -> 09:20:26]  so we're we're going to what we're going
[09:20:23 -> 09:20:27]  to do is we're going to progress up from
[09:20:26 -> 09:20:31]  uh python so we're going to go through
[09:20:27 -> 09:20:34]  pytorch uh as seen in
[09:20:31 -> 09:20:36]  here uh so that pretty much just like an
[09:20:34 -> 09:20:37]  entire P torch training run for a
[09:20:36 -> 09:20:40]  multi-layer
[09:20:37 -> 09:20:42]  perceptron uh and then a little Jupiter
[09:20:40 -> 09:20:45]  notebook that's like the same as that uh
[09:20:42 -> 09:20:47]  but just in a notebook format instead
[09:20:45 -> 09:20:50]  and then we have a numpy script so just
[09:20:47 -> 09:20:53]  uh going through and writing everything
[09:20:50 -> 09:20:55]  from scratch and numpy
[09:20:53 -> 09:20:57]  um this may not look the same by the
[09:20:55 -> 09:21:00]  time you're watching this but it'll be
[09:20:57 -> 09:21:02]  very similar and very easy to understand
[09:21:00 -> 09:21:04]  um at least for the python stuff so I
[09:21:02 -> 09:21:05]  mean you you've probably already written
[09:21:04 -> 09:21:07]  some pytorch already or at least you
[09:21:05 -> 09:21:09]  understand like the basic you know nn.
[09:21:07 -> 09:21:11]  linear layer right you probably
[09:21:09 -> 09:21:14]  understand that already so all we're
[09:21:11 -> 09:21:16]  actually doing in this one and this is
[09:21:14 -> 09:21:18]  what I'm going to demo initially is
[09:21:16 -> 09:21:22]  we're just uh we're literally just
[09:21:18 -> 09:21:24]  training an mnist MLP from scratch using
[09:21:22 -> 09:21:28]  basic Pi torch so I import everything
[09:21:24 -> 09:21:31]  initially so like time nump torch right
[09:21:28 -> 09:21:34]  uh the data loader and the torch vision
[09:21:31 -> 09:21:38]  for the data set itself which is mest um
[09:21:34 -> 09:21:41]  we specify hybrid parameters here so um
[09:21:38 -> 09:21:43]  you know learning rate uh batch size
[09:21:41 -> 09:21:44]  number of epoch I think there's like an
[09:21:43 -> 09:21:47]  extra one here I don't know why that's
[09:21:44 -> 09:21:50]  there um you know train size for like
[09:21:47 -> 09:21:51]  number of elements in the train set and
[09:21:50 -> 09:21:54]  then we're going to go and adjust this
[09:21:51 -> 09:21:54]  data directory
[09:21:58 -> 09:22:02]  uh just print this out
[09:22:05 -> 09:22:12]  actually take
[09:22:08 -> 09:22:12]  that inject it into
[09:22:13 -> 09:22:19]  here and we'll go slash SL python all
[09:22:18 -> 09:22:24]  right
[09:22:19 -> 09:22:27]  now uh sure SL data why
[09:22:24 -> 09:22:29]  not now this is going to make sure that
[09:22:27 -> 09:22:31]  we're using tf32 so that's going to use
[09:22:29 -> 09:22:33]  tensor cores and make things really fast
[09:22:31 -> 09:22:35]  um this is going to initialize the data
[09:22:33 -> 09:22:37]  set properly with like you know mean and
[09:22:35 -> 09:22:39]  standard deviation um this is just kind
[09:22:37 -> 09:22:41]  of the best the best practice for mnus
[09:22:39 -> 09:22:43]  so a lot of this here I'm kind of just
[09:22:41 -> 09:22:46]  following uh some boiler plate uh code
[09:22:43 -> 09:22:49]  template examples that kind of thing uh
[09:22:46 -> 09:22:51]  and now we go down further and
[09:22:49 -> 09:22:54]  it's you know we initialize our data
[09:22:51 -> 09:22:57]  sets we initialize our loaders we load
[09:22:54 -> 09:22:59]  in all of the data uh and we have this
[09:22:57 -> 09:23:02]  exist on the CPU so notice how we're not
[09:22:59 -> 09:23:04]  doing um device equals Cuda or do Cuda
[09:23:02 -> 09:23:09]  yet we're not doing that this is this
[09:23:04 -> 09:23:14]  just exists on CPU right uh system Ram
[09:23:09 -> 09:23:18]  so we load all of our train get in
[09:23:14 -> 09:23:19]  right Lo of our test data in we we're
[09:23:18 -> 09:23:22]  just printing some stuff at each step
[09:23:19 -> 09:23:25]  here it per Epoch so per epoch we're
[09:23:22 -> 09:23:27]  going to do train size which is 10,000
[09:23:25 -> 09:23:30]  divided by batch size which in this case
[09:23:27 -> 09:23:32]  is four so it's going to be about 2,500
[09:23:30 -> 09:23:36]  iterations per Epoch and we're going to
[09:23:32 -> 09:23:39]  do uh three different epochs so
[09:23:36 -> 09:23:42]  7,500 uh steps
[09:23:39 -> 09:23:44]  total now we go down further and we have
[09:23:42 -> 09:23:47]  the actual architecture for this itself
[09:23:44 -> 09:23:49]  right um so we take in we take in an X
[09:23:47 -> 09:23:53]  which is our which is our input we
[09:23:49 -> 09:23:56]  reshape this to batch size by 784
[09:23:53 -> 09:23:59]  um we do our first layer second layer
[09:23:56 -> 09:24:02]  third right so it's just like mat one
[09:23:59 -> 09:24:05]  activation m 2 and this is organized as
[09:24:02 -> 09:24:06]  such um in features hidden features
[09:24:05 -> 09:24:08]  hidden hidden features and then the
[09:24:06 -> 09:24:12]  output Dimension or num
[09:24:08 -> 09:24:15]  class uh we have our in feature set to
[09:24:12 -> 09:24:20]  784 so uh this way it's going to be like
[09:24:15 -> 09:24:24]  a like a batch size by um batch size is
[09:24:20 -> 09:24:28]  the X and then weight is going to be 784
[09:24:24 -> 09:24:29]  by 256 right so that's what that's
[09:24:28 -> 09:24:31]  that's the shape of this it's going to
[09:24:29 -> 09:24:33]  multiply by this linear layer and then
[09:24:31 -> 09:24:34]  we have the rally which is going to take
[09:24:33 -> 09:24:35]  that and it's going to do the Rue
[09:24:34 -> 09:24:36]  activation on it you can search that up
[09:24:35 -> 09:24:40]  if you don't know what that is already
[09:24:36 -> 09:24:42]  it's very very basic to understand um
[09:24:40 -> 09:24:45]  it's literally just a
[09:24:42 -> 09:24:48]  graph and in. linear same idea we're
[09:24:45 -> 09:24:50]  going to take that previous um batch
[09:24:48 -> 09:24:52]  size by hidden features we're going to
[09:24:50 -> 09:24:53]  multiply that by a hidden featur
[09:24:52 -> 09:24:55]  feachers by num classes and we're going
[09:24:53 -> 09:24:57]  to end up with B by num classes right or
[09:24:55 -> 09:25:01]  B by output Dimension whatever you want
[09:24:57 -> 09:25:02]  to say right and that's the idea is we
[09:25:01 -> 09:25:04]  just want to keep forward and back
[09:25:02 -> 09:25:06]  propagating through that and making the
[09:25:04 -> 09:25:07]  network smarter we're going to go Ahad
[09:25:06 -> 09:25:10]  and transfer the model to
[09:25:07 -> 09:25:12]  Cuda we're going to we can do we can do
[09:25:10 -> 09:25:13]  torw compile for a faster version but
[09:25:12 -> 09:25:15]  we're not going to do that just because
[09:25:13 -> 09:25:18]  it like takes a little bit of extra time
[09:25:15 -> 09:25:21]  to do that um we're going to use cross
[09:25:18 -> 09:25:24]  entropy lws for this for the entire
[09:25:21 -> 09:25:28]  project here cross entropy loss is is
[09:25:24 -> 09:25:31]  critical to uh understand
[09:25:28 -> 09:25:33]  um I'm going to go over some of this as
[09:25:31 -> 09:25:34]  well like don't worry if this doesn't
[09:25:33 -> 09:25:36]  make sense yet we're going to like
[09:25:34 -> 09:25:37]  literally write this in C so don't worry
[09:25:36 -> 09:25:39]  if this doesn't entirely make sense I'm
[09:25:37 -> 09:25:42]  just kind of giving you the rundown as
[09:25:39 -> 09:25:43]  to like what everything is based off of
[09:25:42 -> 09:25:45]  we're going to use stochastic gradient
[09:25:43 -> 09:25:47]  descent so that's just your literally
[09:25:45 -> 09:25:50]  your Optimizer so it's going to just
[09:25:47 -> 09:25:52]  nudge the uh you know when you calculate
[09:25:50 -> 09:25:55]  like a gradient or the S so the error
[09:25:52 -> 09:25:58]  for a given weight uh it's going to just
[09:25:55 -> 09:26:01]  literally do learning rate times that
[09:25:58 -> 09:26:03]  and then subtract that result from the
[09:26:01 -> 09:26:07]  from the actual weight
[09:26:03 -> 09:26:09]  itself so uh stochastic rate in a sense
[09:26:07 -> 09:26:12]  sounds comple uh complicated but it's
[09:26:09 -> 09:26:16]  not it it's not that bad uh and then we
[09:26:12 -> 09:26:18]  just have our training Loop here um and
[09:26:16 -> 09:26:20]  I'll I'll I'll go through sort of more
[09:26:18 -> 09:26:23]  intimately what this means in the in the
[09:26:20 -> 09:26:25]  numpy script in a second here here but
[09:26:23 -> 09:26:28]  all that matters is that we kind of
[09:26:25 -> 09:26:31]  understand uh this boiler plate this is
[09:26:28 -> 09:26:33]  designed to be like quite minimalistic
[09:26:31 -> 09:26:37]  um just kind of match what we're doing
[09:26:33 -> 09:26:38]  everywhere else and uh yeah I'm not
[09:26:37 -> 09:26:40]  going to highlight this too much but
[09:26:38 -> 09:26:45]  we'll go ahead and run this uh just just
[09:26:40 -> 09:26:48]  for just for fun you know CD into there
[09:26:45 -> 09:26:50]  we'll do python forch reference and
[09:26:48 -> 09:26:51]  literally what this is going to do it's
[09:26:50 -> 09:26:54]  just going to install our nness data so
[09:26:51 -> 09:26:57]  notice how we got it in the python data
[09:26:54 -> 09:26:57]  directory
[09:26:59 -> 09:27:05]  um awesome so now it's initializing
[09:27:02 -> 09:27:07]  everything it's loading all the data um
[09:27:05 -> 09:27:11]  so we can see this this is learning over
[09:27:07 -> 09:27:13]  three Epoch total of 25 200 each and we
[09:27:11 -> 09:27:16]  end up with an average batch accuracy of
[09:27:13 -> 09:27:19]  90% which is really good so when it's
[09:27:16 -> 09:27:20]  classifying those digits it's getting
[09:27:19 -> 09:27:23]  about n out of 10 of those guesses
[09:27:20 -> 09:27:25]  correct which is really good uh and we
[09:27:23 -> 09:27:30]  can see that by the loss here right the
[09:27:25 -> 09:27:33]  loss is if I move this up the loss we
[09:27:30 -> 09:27:36]  start from you know loading everything
[09:27:33 -> 09:27:39]  in start from a loss of about 2.38 which
[09:27:36 -> 09:27:43]  if we pop into um if we pop into Chrome
[09:27:39 -> 09:27:46]  here go to go to wol from alpha just to
[09:27:43 -> 09:27:49]  like provide a reference
[09:27:46 -> 09:27:53]  um
[09:27:49 -> 09:27:56]  exp of -2. was it
[09:27:53 -> 09:28:02]  2.38 that's about 0.92 if it's like if
[09:27:56 -> 09:28:04]  we round down and say -2.3 it's about uh
[09:28:02 -> 09:28:06]  0.1 which if you convert that to a
[09:28:04 -> 09:28:08]  percentage is about 10% accuracy which
[09:28:06 -> 09:28:11]  is exactly what we want they their
[09:28:08 -> 09:28:13]  images arranged or or the labels are
[09:28:11 -> 09:28:16]  between zero and N9 so zero like
[09:28:13 -> 09:28:19]  literally there's 10 values there so
[09:28:16 -> 09:28:20]  there should be at initialization a 10%
[09:28:19 -> 09:28:23]  chance of getting one correct because
[09:28:20 -> 09:28:24]  it's randomly initialized right right um
[09:28:23 -> 09:28:27]  and so that's exactly what we want
[09:28:24 -> 09:28:32]  everything is initialized properly there
[09:28:27 -> 09:28:34]  um now before I actually go into uh the
[09:28:32 -> 09:28:36]  numpy implementation like numpy is like
[09:28:34 -> 09:28:39]  taking P torch and seeing like what the
[09:28:36 -> 09:28:41]  heck that's doing under the hood um just
[09:28:39 -> 09:28:43]  like kind of a from scratch approach I
[09:28:41 -> 09:28:48]  would recommend that you pause the video
[09:28:43 -> 09:28:51]  right now and watch um Andre carpo's
[09:28:48 -> 09:28:52]  micro guide video so he made this micro
[09:28:51 -> 09:28:55]  guide video about two years ago it's
[09:28:52 -> 09:28:57]  done really really well and it literally
[09:28:55 -> 09:28:58]  just explains the concept of back
[09:28:57 -> 09:29:01]  propagation on the level of scalar
[09:28:58 -> 09:29:03]  values so in this one we're using the uh
[09:29:01 -> 09:29:06]  we're using back propagation on the
[09:29:03 -> 09:29:09]  level of tensors but understanding this
[09:29:06 -> 09:29:12]  uh is crucial for moving up the tensors
[09:29:09 -> 09:29:13]  so after after you're done this and it's
[09:29:12 -> 09:29:15]  like still a little bit confusing about
[09:29:13 -> 09:29:18]  how we're going to bring this up to like
[09:29:15 -> 09:29:20]  bigger matrices I would recommend that
[09:29:18 -> 09:29:22]  you uh also take a peek at my back
[09:29:20 -> 09:29:24]  propagation video this is just on on my
[09:29:22 -> 09:29:27]  YouTube channel here Elliot codes um
[09:29:24 -> 09:29:31]  about right now 5.7 th000 subscribers
[09:29:27 -> 09:29:33]  you should be able to find me um and I
[09:29:31 -> 09:29:35]  just kind of made like a funny title um
[09:29:33 -> 09:29:37]  because back propagation is annoying to
[09:29:35 -> 09:29:40]  understand sometimes and so I I did a I
[09:29:37 -> 09:29:42]  tried to do my best job to break down uh
[09:29:40 -> 09:29:44]  what the heck that means on the on the
[09:29:42 -> 09:29:48]  level of tensors on the Whiteboard um
[09:29:44 -> 09:29:50]  this does have this does have uh 1440p
[09:29:48 -> 09:29:54]  quality so you can actually you can you
[09:29:50 -> 09:29:55]  can see stuff um but yeah and then there
[09:29:54 -> 09:29:56]  there's just like a ton of like content
[09:29:55 -> 09:29:58]  that you can watch right I don't
[09:29:56 -> 09:30:00]  encourage you to just like consume all
[09:29:58 -> 09:30:02]  the content out there but these are the
[09:30:00 -> 09:30:04]  three that I thought were the easiest uh
[09:30:02 -> 09:30:06]  to like build an intuition on what back
[09:30:04 -> 09:30:09]  propagation is um we're going to be
[09:30:06 -> 09:30:10]  going over back propagation as well um
[09:30:09 -> 09:30:13]  but this
[09:30:10 -> 09:30:15]  is uh this is like where you want to
[09:30:13 -> 09:30:18]  start from if this is like a completely
[09:30:15 -> 09:30:22]  foreign concept to you um so Entre aroy
[09:30:18 -> 09:30:26]  does really good I I go above Andre arpo
[09:30:22 -> 09:30:27]  um lecture and go up to tensors and then
[09:30:26 -> 09:30:29]  three blue and brown which you you've
[09:30:27 -> 09:30:32]  probably heard of
[09:30:29 -> 09:30:35]  already uh does does a a quick little
[09:30:32 -> 09:30:37]  lecture on uh on back propagation
[09:30:35 -> 09:30:38]  intuitively so let's go Ahad and jump
[09:30:37 -> 09:30:41]  into
[09:30:38 -> 09:30:43]  numpy okay awesome so now uh let's go
[09:30:41 -> 09:30:45]  Ahad and take a look at what exactly
[09:30:43 -> 09:30:48]  this diagram is supposed to be um I
[09:30:45 -> 09:30:51]  could I could walk through and sort of
[09:30:48 -> 09:30:52]  just like explain what's going down uh
[09:30:51 -> 09:30:54]  like what essentially what is happening
[09:30:52 -> 09:30:56]  step by step in here but first I want to
[09:30:54 -> 09:30:57]  make this obvious how our neuronet
[09:30:56 -> 09:30:59]  architecture is structured and how the
[09:30:57 -> 09:31:02]  neurons actually work right what is a
[09:30:59 -> 09:31:06]  neuron that type of thing
[09:31:02 -> 09:31:07]  um so I drew this little diagram to help
[09:31:06 -> 09:31:09]  us understand what's happening and I'm
[09:31:07 -> 09:31:11]  going to lay this out here so we
[09:31:09 -> 09:31:14]  essentially take an image this is the uh
[09:31:11 -> 09:31:16]  hand handwritten digit it's 28 by 28 and
[09:31:14 -> 09:31:17]  we're going to have this in a batch
[09:31:16 -> 09:31:20]  right so it's going to be 28 by 28 and
[09:31:17 -> 09:31:22]  then depth be our batch batch size you
[09:31:20 -> 09:31:26]  could say um just a bunch of panes
[09:31:22 -> 09:31:28]  essentially layered on top of each other
[09:31:26 -> 09:31:30]  we flatten these so 28 * 28 that's going
[09:31:28 -> 09:31:32]  to is going to have this this whole
[09:31:30 -> 09:31:33]  image and we're going to take a row and
[09:31:32 -> 09:31:35]  we're just going to add it to the end
[09:31:33 -> 09:31:37]  until it stretches out to the full
[09:31:35 -> 09:31:40]  length it's going to be 28 of these rows
[09:31:37 -> 09:31:42]  that are added to the edge right and
[09:31:40 -> 09:31:46]  then we we plug this into the first
[09:31:42 -> 09:31:52]  weight so X1 * W1 that's going to be b b
[09:31:46 -> 09:31:53]  by 784 so B by 784 and then times 7 84
[09:31:52 -> 09:31:58]  by
[09:31:53 -> 09:32:01]  256 so a column uh the the the the size
[09:31:58 -> 09:32:04]  of a column in the weight Matrix is 784
[09:32:01 -> 09:32:06]  and then the length of a row in X is
[09:32:04 -> 09:32:08]  also the 784 so it's going to layer on
[09:32:06 -> 09:32:10]  top of those
[09:32:08 -> 09:32:13]  right um and we're going to end up with
[09:32:10 -> 09:32:15]  P by 256 then we go to the next one and
[09:32:13 -> 09:32:16]  we get this we essentially just feed
[09:32:15 -> 09:32:18]  this through the Rue so it's going to
[09:32:16 -> 09:32:21]  just it's going to do the the rally
[09:32:18 -> 09:32:24]  function on each value pretty simple
[09:32:21 -> 09:32:26]  then we do X2 * W2 and that's going to
[09:32:24 -> 09:32:28]  do B by 256 which was output from The
[09:32:26 -> 09:32:32]  Rue it's going to Matrix multiply that
[09:32:28 -> 09:32:34]  with 256 by 10 10 is our output size
[09:32:32 -> 09:32:37]  right so we have this B by headden size
[09:32:34 -> 09:32:39]  which is 256 that's each uh that's each
[09:32:37 -> 09:32:42]  neuron output that's that's uh that's
[09:32:39 -> 09:32:46]  essentially how many um neurons we have
[09:32:42 -> 09:32:48]  is 256 in our in our in our hidden layer
[09:32:46 -> 09:32:51]  there and then we just have to ensure
[09:32:48 -> 09:32:53]  that broadcasting is done correctly so
[09:32:51 -> 09:32:56]  uh I mean you could always you could pop
[09:32:53 -> 09:32:58]  over to like pytorch uh
[09:32:56 -> 09:33:01]  broadcasting broadcast
[09:32:58 -> 09:33:03]  rules broadcasting semantics here so
[09:33:01 -> 09:33:05]  this is pretty much just how you're
[09:33:03 -> 09:33:06]  allowed to like multiply things and and
[09:33:05 -> 09:33:10]  do operations with tensors when they're
[09:33:06 -> 09:33:12]  certain sizes right so um you know if
[09:33:10 -> 09:33:14]  you haven't gone through this already
[09:33:12 -> 09:33:17]  it's it probably is a good idea um it's
[09:33:14 -> 09:33:19]  not too hard to learn pytor handles this
[09:33:17 -> 09:33:21]  stuff pretty well so it's just a little
[09:33:19 -> 09:33:22]  short short read to do just kind of
[09:33:21 -> 09:33:24]  understand like what's happening there
[09:33:22 -> 09:33:25]  but um essentially you just want to make
[09:33:24 -> 09:33:27]  sure those inner values are the same and
[09:33:25 -> 09:33:29]  then the outer ones are going to be your
[09:33:27 -> 09:33:33]  output shape right so we have this B
[09:33:29 -> 09:33:35]  this B by 256 and then the the 256 here
[09:33:33 -> 09:33:38]  is going to just it's going to do
[09:33:35 -> 09:33:39]  product it's going to take each column
[09:33:38 -> 09:33:43]  and do product there and you're going to
[09:33:39 -> 09:33:47]  end up with a total of uh 10 values for
[09:33:43 -> 09:33:50]  each batch element right and this is
[09:33:47 -> 09:33:53]  essentially uh this this this 10 here is
[09:33:50 -> 09:33:56]  a distribution of probabilities so prob
[09:33:53 -> 09:34:00]  probability distribution in batches and
[09:33:56 -> 09:34:02]  in in a batch um and those are going to
[09:34:00 -> 09:34:05]  be our predictions the probability of
[09:34:02 -> 09:34:08]  which digit it is so if the screen says
[09:34:05 -> 09:34:10]  like a zero
[09:34:08 -> 09:34:13]  um then what's going to happen after our
[09:34:10 -> 09:34:16]  network is really smart is the if we go
[09:34:13 -> 09:34:18]  to say any batch element so any any
[09:34:16 -> 09:34:21]  essentially row and we go to the first
[09:34:18 -> 09:34:22]  the zero with index in that um that's
[09:34:21 -> 09:34:24]  most likely going to be the highest
[09:34:22 -> 09:34:27]  number in the entire distribution right
[09:34:24 -> 09:34:29]  that's kind of what's going there um and
[09:34:27 -> 09:34:30]  then we just do the a loss function on
[09:34:29 -> 09:34:32]  this called cross entropy loss which
[09:34:30 -> 09:34:35]  I'll explain in a second CR cross
[09:34:32 -> 09:34:37]  entropy loss isn't too bad um it's
[09:34:35 -> 09:34:38]  different than MSE loss but it's better
[09:34:37 -> 09:34:41]  for batches and kind of what we're doing
[09:34:38 -> 09:34:43]  here um
[09:34:41 -> 09:34:44]  so we get our loss which initially is
[09:34:43 -> 09:34:48]  going to be around
[09:34:44 -> 09:34:50]  2.3 um based on the wol from alpha based
[09:34:48 -> 09:34:53]  on the W wol from alpha calculation you
[09:34:50 -> 09:34:57]  can do right where you go exp uh like
[09:34:53 -> 09:35:00]  exponentiate negative uh Negative X so
[09:34:57 -> 09:35:03]  um exponentiate negative 2.3 you get
[09:35:00 -> 09:35:06]  about 0.1 for that um which is 10%
[09:35:03 -> 09:35:08]  accuracy and then you do the derivative
[09:35:06 -> 09:35:09]  of the loss which is just going to
[09:35:08 -> 09:35:12]  essentially element wise subtract the
[09:35:09 -> 09:35:13]  softmax probabilities by the true one
[09:35:12 -> 09:35:16]  hot probabilities and I'll explain those
[09:35:13 -> 09:35:17]  as well what one hot means and then we
[09:35:16 -> 09:35:19]  just back propagate right so this is
[09:35:17 -> 09:35:22]  kind of the forward pass here where we
[09:35:19 -> 09:35:24]  go um matl one
[09:35:22 -> 09:35:28]  activation m 2 calculate the loss and
[09:35:24 -> 09:35:30]  then we backwards this way um so we do
[09:35:28 -> 09:35:32]  derivative derivative of the loss and
[09:35:30 -> 09:35:35]  then we do our chain roll back
[09:35:32 -> 09:35:37]  propagation so we calculate dw2 and
[09:35:35 -> 09:35:39]  that's just going to be uh keep in mind
[09:35:37 -> 09:35:42]  the gradient of the weight is supposed
[09:35:39 -> 09:35:46]  to match the shape of the weight itself
[09:35:42 -> 09:35:49]  so notice how W2 here was 26 256 by 10
[09:35:46 -> 09:35:52]  here we do 256x B matal that with B by
[09:35:49 -> 09:35:56]  10 and so we end up with the Dimensions
[09:35:52 -> 09:35:58]  256x 10 and that matches up um then we
[09:35:56 -> 09:36:01]  do dx2 which is needed for you know
[09:35:58 -> 09:36:04]  continually back prop back propagating
[09:36:01 -> 09:36:07]  through the Rue out layer um and so here
[09:36:04 -> 09:36:08]  it's literally just taking the dx2 and
[09:36:07 -> 09:36:10]  then uh doing an element wise
[09:36:08 -> 09:36:13]  multiplication with the with the
[09:36:10 -> 09:36:16]  derivative of the r function of X so
[09:36:13 -> 09:36:17]  whatever the whatever the um the input
[09:36:16 -> 09:36:19]  was into
[09:36:17 -> 09:36:22]  that and then we end up with the same
[09:36:19 -> 09:36:25]  shape that we got here we simply plug
[09:36:22 -> 09:36:26]  this we plug this Rue out into both of
[09:36:25 -> 09:36:28]  these we don't actually need to do it
[09:36:26 -> 09:36:29]  for dx1 but it's just kind of here just
[09:36:28 -> 09:36:33]  in case in case you have like a deeper
[09:36:29 -> 09:36:34]  Network and you want to modify things um
[09:36:33 -> 09:36:38]  but
[09:36:34 -> 09:36:42]  dw1 it's going to be so x x1. t right so
[09:36:38 -> 09:36:43]  we're transposing X1 X1 is over here so
[09:36:42 -> 09:36:47]  just flipping those flipping those
[09:36:43 -> 09:36:52]  Dimensions instead of being B by 784
[09:36:47 -> 09:36:54]  it's now going to be um 784 by B so you
[09:36:52 -> 09:36:57]  have each column is an image right
[09:36:54 -> 09:37:01]  instead of each row being an image
[09:36:57 -> 09:37:03]  um we we we map that with B by 256 which
[09:37:01 -> 09:37:07]  was the Rue output so that's like the
[09:37:03 -> 09:37:11]  the last output uh gradient um and then
[09:37:07 -> 09:37:15]  we get our our our weight value right
[09:37:11 -> 09:37:18]  so uh and then from here we pretty much
[09:37:15 -> 09:37:20]  just uh modify the the weights value we
[09:37:18 -> 09:37:22]  get the same shape there so that kind of
[09:37:20 -> 09:37:23]  checks out
[09:37:22 -> 09:37:26]  and then dx1 I mean we don't actually
[09:37:23 -> 09:37:30]  need dx1 here but I include it uh just
[09:37:26 -> 09:37:34]  in case um kind of the point there is uh
[09:37:30 -> 09:37:36]  you know dx1 relies on uh on
[09:37:34 -> 09:37:39]  W1
[09:37:36 -> 09:37:41]  so uh we we we don't actually need to
[09:37:39 -> 09:37:43]  like update anything for this it's just
[09:37:41 -> 09:37:46]  if there was like a a dw0 if there was
[09:37:43 -> 09:37:51]  like a layer before that um cuz notice
[09:37:46 -> 09:37:53]  how uh sorry notice how this Ru layer
[09:37:51 -> 09:37:57]  relies on this right so if you had like
[09:37:53 -> 09:38:00]  a like if we had a Ru um if we had a ra
[09:37:57 -> 09:38:02]  layer like before this if we had a rail
[09:38:00 -> 09:38:05]  layer somewhere in here then you would
[09:38:02 -> 09:38:08]  actually need to like dx0 for example
[09:38:05 -> 09:38:11]  and dw0 you would actually need to uh
[09:38:08 -> 09:38:12]  use this but since we're not we can kind
[09:38:11 -> 09:38:14]  of just understand that it's something
[09:38:12 -> 09:38:15]  you would have if it was a deeper
[09:38:14 -> 09:38:17]  network but you don't actually need it
[09:38:15 -> 09:38:19]  to make the network improve in
[09:38:17 -> 09:38:21]  performance right it's just like an
[09:38:19 -> 09:38:22]  extra calculation that's redundant so
[09:38:21 -> 09:38:25]  you you can just kind of exclude it
[09:38:22 -> 09:38:26]  unless you want to modify it um but I'm
[09:38:25 -> 09:38:28]  not going to ramble on about that for
[09:38:26 -> 09:38:31]  too long let's actually get to what the
[09:38:28 -> 09:38:35]  heck these neurons are doing all
[09:38:31 -> 09:38:38]  right so I got this little like editor
[09:38:35 -> 09:38:41]  open here it's like my first time using
[09:38:38 -> 09:38:44]  it but essentially what's what's H how a
[09:38:41 -> 09:38:46]  neuron works is we're going to just like
[09:38:44 -> 09:38:48]  zoom in how a neuron works is we're
[09:38:46 -> 09:38:49]  going to take a bunch of X values all
[09:38:48 -> 09:38:53]  right so
[09:38:49 -> 09:38:53]  X1 X2
[09:38:54 -> 09:38:59]  X3 and uh these are going to go see
[09:38:58 -> 09:39:02]  these are going
[09:38:59 -> 09:39:06]  forward uh into a neuron
[09:39:02 -> 09:39:09]  right what's going to happen here is
[09:39:06 -> 09:39:12]  these are going to go um maybe I should
[09:39:09 -> 09:39:12]  draw these a little
[09:39:15 -> 09:39:18]  better so this is our neuron here we're
[09:39:17 -> 09:39:20]  just going to put a plus there and
[09:39:18 -> 09:39:23]  you'll understand why in a second this
[09:39:20 -> 09:39:26]  neuron is g to have a bunch of different
[09:39:23 -> 09:39:28]  weights okay this neuron is g to have
[09:39:26 -> 09:39:32]  this neuron is gonna have one one weight
[09:39:28 -> 09:39:34]  for each of these X values so w
[09:39:32 -> 09:39:37]  W1
[09:39:34 -> 09:39:39]  W2 and W3 and all this is going to do is
[09:39:37 -> 09:39:41]  just going to do product these so it's
[09:39:39 -> 09:39:43]  going to go
[09:39:41 -> 09:39:46]  W1
[09:39:43 -> 09:39:49]  X1 plus
[09:39:46 -> 09:39:49]  W2 uh
[09:39:49 -> 09:39:56]  X2 plus W
[09:39:52 -> 09:40:00]  3 X3 and that's going to give
[09:39:56 -> 09:40:04]  us output right
[09:40:00 -> 09:40:06]  so this this times this this times this
[09:40:04 -> 09:40:08]  and this times this right that's a
[09:40:06 -> 09:40:11]  single neuron and what we're going to
[09:40:08 -> 09:40:12]  end up with is uh when we sum these
[09:40:11 -> 09:40:18]  together we're going to end up with a
[09:40:12 -> 09:40:18]  single value end up with a single value
[09:40:22 -> 09:40:25]  one
[09:40:26 -> 09:40:32]  Val one value
[09:40:28 -> 09:40:37]  here now when we jump up to a bunch of
[09:40:32 -> 09:40:40]  neurons so if we had say um like
[09:40:37 -> 09:40:42]  X
[09:40:40 -> 09:40:48]  X1
[09:40:42 -> 09:40:50]  X2 and we have say 1 two three
[09:40:48 -> 09:40:52]  neurons this one is going to have each
[09:40:50 -> 09:40:56]  of these are going to have two neurons
[09:40:52 -> 09:40:58]  right so this going to have um one
[09:40:56 -> 09:41:03]  two one
[09:40:58 -> 09:41:05]  2 and one two this looks very familiar
[09:41:03 -> 09:41:06]  to something you've seen before which is
[09:41:05 -> 09:41:08]  the MLP that I previously showed you
[09:41:06 -> 09:41:10]  right there the images youve probably
[09:41:08 -> 09:41:11]  seen like you know clickbait thumbnails
[09:41:10 -> 09:41:15]  with these before and this is exactly
[09:41:11 -> 09:41:17]  what it is right so you're going to have
[09:41:15 -> 09:41:20]  uh two weights in here two weights in
[09:41:17 -> 09:41:22]  here and two weights in
[09:41:20 -> 09:41:24]  here and so it's going to do prodct with
[09:41:22 -> 09:41:28]  each of these you're going to have G to
[09:41:24 -> 09:41:31]  have this which is going to be
[09:41:28 -> 09:41:34]  um you know like a
[09:41:31 -> 09:41:36]  W1
[09:41:34 -> 09:41:38]  X1 plus a
[09:41:36 -> 09:41:40]  W2
[09:41:38 -> 09:41:42]  X2 and then the same for these as well
[09:41:40 -> 09:41:44]  right it's going to do all of these and
[09:41:42 -> 09:41:45]  you're going to end up with three total
[09:41:44 -> 09:41:49]  values so you're going to end up with
[09:41:45 -> 09:41:51]  one two three values in the
[09:41:49 -> 09:41:53]  output now
[09:41:51 -> 09:41:55]  if we actually go to linear algebra and
[09:41:53 -> 09:41:56]  try to understand this concept things
[09:41:55 -> 09:42:00]  are going to actually make a lot more
[09:41:56 -> 09:42:04]  sense okay so we go to linear algebra
[09:42:00 -> 09:42:06]  say we have um something of size one
[09:42:04 -> 09:42:10]  by uh
[09:42:06 -> 09:42:13]  say 784 all
[09:42:10 -> 09:42:17]  right and then we matal this with
[09:42:13 -> 09:42:17]  something of size
[09:42:18 -> 09:42:24]  784 by 256 okay ignore my handwriting
[09:42:21 -> 09:42:27]  it's terrible um but this is this is
[09:42:24 -> 09:42:30]  going to be each x value
[09:42:27 -> 09:42:31]  right
[09:42:30 -> 09:42:36]  X1 and
[09:42:31 -> 09:42:36]  then X um
[09:42:39 -> 09:42:47]  784 and we're going to have 256 neurons
[09:42:42 -> 09:42:52]  okay so this is number one and this is
[09:42:47 -> 09:42:52]  uh 256 okay
[09:42:53 -> 09:43:00]  now all we're going to do is literally
[09:42:56 -> 09:43:03]  take the at this is going to be like
[09:43:00 -> 09:43:05]  essentially a it's going to have this is
[09:43:03 -> 09:43:07]  the number of uh rows it has right this
[09:43:05 -> 09:43:08]  is the this is the height of the Matrix
[09:43:07 -> 09:43:10]  so it's going to essentially just look
[09:43:08 -> 09:43:12]  like a vector it's going to be it's
[09:43:10 -> 09:43:12]  going to be this and it's just going to
[09:43:12 -> 09:43:15]  be a
[09:43:12 -> 09:43:17]  line and this one is going to be quite
[09:43:15 -> 09:43:21]  tall so it's actually going to look like
[09:43:17 -> 09:43:21]  this it's going to be
[09:43:25 -> 09:43:33]  784 by 256 like that now
[09:43:30 -> 09:43:33]  this this column right
[09:43:34 -> 09:43:39]  here is going to be a single neuron
[09:43:37 -> 09:43:42]  right this is going to be
[09:43:39 -> 09:43:44]  um say
[09:43:42 -> 09:43:46]  one
[09:43:44 -> 09:43:48]  neuron this is going to be a set of
[09:43:46 -> 09:43:50]  Weights that's going to map it's each
[09:43:48 -> 09:43:53]  each of those weight values is going to
[09:43:50 -> 09:43:58]  multiply with a single x value right so
[09:43:53 -> 09:44:00]  W1 is going to multiply with this one um
[09:43:58 -> 09:44:01]  W2 is going to multiply with X2 down
[09:44:00 -> 09:44:04]  here and then all the way down to
[09:44:01 -> 09:44:06]  whatever n is or or sorry down to down
[09:44:04 -> 09:44:09]  to the down to 784 which is the length
[09:44:06 -> 09:44:11]  of it or or the the length of the column
[09:44:09 -> 09:44:13]  I guess you the height of the column
[09:44:11 -> 09:44:16]  whatever you want to interpret that as
[09:44:13 -> 09:44:19]  um and this a is a single neuron right
[09:44:16 -> 09:44:21]  as long as that's clear we're good now
[09:44:19 -> 09:44:22]  we have a bunch of these neurons as a
[09:44:21 -> 09:44:25]  matter of fact we have
[09:44:22 -> 09:44:28]  256 different
[09:44:25 -> 09:44:28]  neurons
[09:44:31 -> 09:44:34]  right I don't know how to write with my
[09:44:33 -> 09:44:38]  hand yet I'm still getting used to
[09:44:34 -> 09:44:41]  it but we have 256 different neurons
[09:44:38 -> 09:44:43]  right and they're each dot producting
[09:44:41 -> 09:44:47]  with the input
[09:44:43 -> 09:44:49]  itself and we end up with a final output
[09:44:47 -> 09:44:52]  so notice these you know cancel out and
[09:44:49 -> 09:44:52]  we end up with
[09:44:55 -> 09:45:04]  1 by 256 so for one example we're going
[09:45:00 -> 09:45:06]  to get 256 neuron outputs now if you
[09:45:04 -> 09:45:11]  envision this as
[09:45:06 -> 09:45:13]  say um batch by 2 784 so instead of one
[09:45:11 -> 09:45:16]  image flattened it's a bunch of
[09:45:13 -> 09:45:17]  flattened images right each flatten
[09:45:16 -> 09:45:20]  image is a
[09:45:17 -> 09:45:24]  row then essentially all you end up with
[09:45:20 -> 09:45:26]  is this B just replaces the one so you
[09:45:24 -> 09:45:26]  you you get rid of
[09:45:28 -> 09:45:34]  that and you end up with this B here and
[09:45:31 -> 09:45:35]  this is just uh a batch of neuron
[09:45:34 -> 09:45:38]  outputs right so it's like think it's
[09:45:35 -> 09:45:40]  like imagine thinking about this entire
[09:45:38 -> 09:45:42]  this architecture here where you have
[09:45:40 -> 09:45:45]  each x value plugging into like a
[09:45:42 -> 09:45:47]  different neuron just imagine this but
[09:45:45 -> 09:45:48]  you but you like layer them on top of
[09:45:47 -> 09:45:51]  each other so you have like you have
[09:45:48 -> 09:45:53]  like say you have this like as it's as
[09:45:51 -> 09:45:56]  it's looking at you at the screen and
[09:45:53 -> 09:45:58]  then you you plop it down like this then
[09:45:56 -> 09:46:00]  you put another one on top batch element
[09:45:58 -> 09:46:02]  one batch element two batch element
[09:46:00 -> 09:46:05]  three batch element four and each of
[09:46:02 -> 09:46:07]  these is going to give you something so
[09:46:05 -> 09:46:11]  this all these outputs is going to give
[09:46:07 -> 09:46:12]  you like say uh this is going to give
[09:46:11 -> 09:46:14]  you
[09:46:12 -> 09:46:18]  [Music]
[09:46:14 -> 09:46:19]  um what's it called 256 different
[09:46:18 -> 09:46:22]  outputs and you're going to have these
[09:46:19 -> 09:46:24]  in a batch batch size right so there 256
[09:46:22 -> 09:46:28]  outputs are all layered out here and
[09:46:24 -> 09:46:30]  then each one stacked on top is just
[09:46:28 -> 09:46:33]  another set of 256 right that's how I
[09:46:30 -> 09:46:35]  like sort of like to to reason through
[09:46:33 -> 09:46:36]  this in my head you might want to reason
[09:46:35 -> 09:46:38]  through it differently but I find that
[09:46:36 -> 09:46:40]  to be a pretty cool trick to
[09:46:38 -> 09:46:43]  understanding how these are
[09:46:40 -> 09:46:46]  working now there's another
[09:46:43 -> 09:46:49]  term that I did sort of leave out and
[09:46:46 -> 09:46:52]  this is the bias right so the
[09:46:49 -> 09:46:54]  bias each one of these each little one
[09:46:52 -> 09:46:56]  of these neurons is going to have its
[09:46:54 -> 09:47:00]  own bias right
[09:46:56 -> 09:47:03]  so you know
[09:47:00 -> 09:47:07]  X X1 X
[09:47:03 -> 09:47:07]  xn uh and now each of these little
[09:47:07 -> 09:47:12]  neurons is going to have its own bias so
[09:47:10 -> 09:47:14]  we're going to do you know a do product
[09:47:12 -> 09:47:20]  operation as we normally would we're
[09:47:14 -> 09:47:23]  going to do like X2 or X1 * um X1 * W1
[09:47:20 -> 09:47:24]  Plus X2 * W2 all the all the way through
[09:47:23 -> 09:47:26]  all the different weights inside of a
[09:47:24 -> 09:47:29]  neuron and then once we get that we're
[09:47:26 -> 09:47:30]  going to add a bias to that and the bias
[09:47:29 -> 09:47:32]  is just going to be a single number
[09:47:30 -> 09:47:34]  because of course it's a scalar value
[09:47:32 -> 09:47:35]  right so it's going to be essentially
[09:47:34 -> 09:47:40]  it's going to look like
[09:47:35 -> 09:47:41]  this W * X we're going to do we're going
[09:47:40 -> 09:47:44]  to do all of these that we need to dot
[09:47:41 -> 09:47:48]  product essentially a like a like a
[09:47:44 -> 09:47:49]  vector a vector vector multiplication
[09:47:48 -> 09:47:52]  and we're going to add a single bias
[09:47:49 -> 09:47:55]  value to that right
[09:47:52 -> 09:47:57]  so the vector um Vector X is going to
[09:47:55 -> 09:48:00]  look like
[09:47:57 -> 09:48:02]  this and the vector W is going to look
[09:48:00 -> 09:48:05]  like this it's going to be a
[09:48:02 -> 09:48:07]  column so we take this column and we we
[09:48:05 -> 09:48:09]  do a essentially a matrix multiplication
[09:48:07 -> 09:48:11]  but they're vectors so it's like
[09:48:09 -> 09:48:14]  literally what it looks like
[09:48:11 -> 09:48:17]  is and you just take this element and
[09:48:14 -> 09:48:19]  you you multiply with this one right so
[09:48:17 -> 09:48:21]  you kind of you kind of like put like a
[09:48:19 -> 09:48:23]  stick through it you go
[09:48:21 -> 09:48:25]  and it's like cut up into a bunch of
[09:48:23 -> 09:48:28]  little multiplications and you sum them
[09:48:25 -> 09:48:31]  together and you do your
[09:48:28 -> 09:48:33]  bias and we can back pop get through
[09:48:31 -> 09:48:34]  that as well so that that hopefully that
[09:48:33 -> 09:48:37]  just clears up some of the intuition as
[09:48:34 -> 09:48:40]  to how we're actually structuring um
[09:48:37 -> 09:48:43]  this part here so that rule that I just
[09:48:40 -> 09:48:45]  showed you also applies for the X2 and
[09:48:43 -> 09:48:47]  W2 right the point is we're just trying
[09:48:45 -> 09:48:48]  to understand what the whole design of
[09:48:47 -> 09:48:51]  the neuron is and how we can abstract
[09:48:48 -> 09:48:53]  that up to linear algebra and then write
[09:48:51 -> 09:48:55]  really really fast C and C++ and Cuda
[09:48:53 -> 09:48:57]  code to to make this like run fast
[09:48:55 -> 09:48:59]  instead of just considering each little
[09:48:57 -> 09:49:01]  neuron operation independently we can
[09:48:59 -> 09:49:04]  build we can put a layer of abstraction
[09:49:01 -> 09:49:06]  up and say this is we we've proven that
[09:49:04 -> 09:49:09]  this is uh this is how this works in
[09:49:06 -> 09:49:10]  linear linear algebra and now we can
[09:49:09 -> 09:49:12]  step forward and try to really optimize
[09:49:10 -> 09:49:13]  that based on our knowledge about matrix
[09:49:12 -> 09:49:16]  multiplication right this is one of the
[09:49:13 -> 09:49:17]  reasons why emphasize the Mel so much is
[09:49:16 -> 09:49:19]  because it's an extremely important
[09:49:17 -> 09:49:21]  algorithm in all this deep learning
[09:49:19 -> 09:49:24]  stuff right um
[09:49:21 -> 09:49:26]  but yeah now we can uh now let's look at
[09:49:24 -> 09:49:29]  how we can like you know cross entropy
[09:49:26 -> 09:49:32]  loss and back propagating through all of
[09:49:29 -> 09:49:34]  this all right awesome so now we next
[09:49:32 -> 09:49:36]  have to do the loss function and the
[09:49:34 -> 09:49:40]  loss function isn't actually too bad so
[09:49:36 -> 09:49:43]  if we pop over to here notice how we
[09:49:40 -> 09:49:44]  have uh this cross entropy loss right
[09:49:43 -> 09:49:48]  this is our this is our loss function
[09:49:44 -> 09:49:50]  once we get the we do model. forbo from
[09:49:48 -> 09:49:53]  our X batch and we get the Y predict did
[09:49:50 -> 09:49:54]  and then a cache right uh don't worry
[09:49:53 -> 09:49:58]  about the C just worry about these this
[09:49:54 -> 09:50:01]  y PR and the the batch y right so we go
[09:49:58 -> 09:50:02]  over to our cross entropy loss and
[09:50:01 -> 09:50:05]  inside here we're going to dissect this
[09:50:02 -> 09:50:07]  thing by the way don't worry too much
[09:50:05 -> 09:50:09]  inside of this thing we have batch size
[09:50:07 -> 09:50:10]  um so just we essentially just take an
[09:50:09 -> 09:50:12]  element from the shape do our
[09:50:10 -> 09:50:15]  probabilities from the softmax function
[09:50:12 -> 09:50:19]  which we did go over before we have this
[09:50:15 -> 09:50:21]  piece of work here uh and then uh and
[09:50:19 -> 09:50:24]  then we just finish it up by calculating
[09:50:21 -> 09:50:25]  the loss right so assuming that we
[09:50:24 -> 09:50:27]  understand how our softmax function
[09:50:25 -> 09:50:29]  works we can actually go ahead and dig
[09:50:27 -> 09:50:32]  into this
[09:50:29 -> 09:50:32]  so I'm going to open
[09:50:33 -> 09:50:40]  up open up a Jupiter notebook here so
[09:50:38 -> 09:50:43]  going to go ahead and import
[09:50:40 -> 09:50:47]  numpy then we're going
[09:50:43 -> 09:50:50]  to define the cross entropy loss uh
[09:50:47 -> 09:50:51]  function itself and softmax so let's go
[09:50:50 -> 09:50:54]  ahead and take
[09:50:51 -> 09:50:57]  these and uh pop these into
[09:50:54 -> 09:51:00]  here then we're going to say set our
[09:50:57 -> 09:51:03]  batch size as um as two and then we're
[09:51:00 -> 09:51:08]  going to set uh
[09:51:03 -> 09:51:08]  num classes to five all
[09:51:08 -> 09:51:13]  right then we're going to go and make a
[09:51:12 -> 09:51:16]  shape we're going to go ahe and make our
[09:51:13 -> 09:51:19]  both of our um inputs to cross inoc here
[09:51:16 -> 09:51:22]  the Y PR and the Y true all right so
[09:51:19 -> 09:51:26]  yred we're going to set this equal to uh
[09:51:22 -> 09:51:27]  this nump uh Rand n batch sized by num
[09:51:26 -> 09:51:30]  classes right so that's going to be our
[09:51:27 -> 09:51:32]  predictions we have a bunch of uh we
[09:51:30 -> 09:51:35]  have a bunch of batch elements each with
[09:51:32 -> 09:51:37]  a probability distribution about which
[09:51:35 -> 09:51:40]  which uh which output it think should be
[09:51:37 -> 09:51:43]  those are the predictions right and then
[09:51:40 -> 09:51:47]  we have a y true which is going to be
[09:51:43 -> 09:51:47]  random integers
[09:51:48 -> 09:51:52]  um and I'm going to print these out so
[09:51:50 -> 09:51:56]  you can
[09:51:52 -> 09:52:00]  see so y looks like this we have batch
[09:51:56 -> 09:52:01]  size of two so 1 two and we have a bunch
[09:52:00 -> 09:52:03]  of these elements right this is our
[09:52:01 -> 09:52:05]  probability uh actually these are not
[09:52:03 -> 09:52:07]  our our probab probability distribution
[09:52:05 -> 09:52:09]  yet these are actually called lits so I
[09:52:07 -> 09:52:12]  skipped a step there but lits are the
[09:52:09 -> 09:52:15]  step before the softmax right softmax is
[09:52:12 -> 09:52:16]  going to make sure that everything uh is
[09:52:15 -> 09:52:19]  above zero right it's going to make sure
[09:52:16 -> 09:52:21]  everything is like zero between zero and
[09:52:19 -> 09:52:22]  one right that that's idea there and
[09:52:21 -> 09:52:24]  it's going to express with confidence
[09:52:22 -> 09:52:25]  which numbers should should be high
[09:52:24 -> 09:52:29]  right because it's
[09:52:25 -> 09:52:32]  exponentiating um these are lits because
[09:52:29 -> 09:52:35]  it's like um like the lodge it I guess
[09:52:32 -> 09:52:37]  you could say it's like the log like the
[09:52:35 -> 09:52:38]  log probabilities so because you have to
[09:52:37 -> 09:52:40]  exponentiate up to get to softmax it's
[09:52:38 -> 09:52:42]  like log because you go
[09:52:40 -> 09:52:44]  down it's going to do Ln not actually
[09:52:42 -> 09:52:47]  log with base two it's or log base 10
[09:52:44 -> 09:52:48]  it's going to be Ln so base two base e
[09:52:47 -> 09:52:51]  or
[09:52:48 -> 09:52:55]  2.71 and we can go ahead and print out
[09:52:51 -> 09:52:57]  our y true so this is
[09:52:55 -> 09:53:02]  um this is an
[09:52:57 -> 09:53:02]  array it's very yeah you're you're
[09:53:02 -> 09:53:04]  you're going to see this you're going to
[09:53:02 -> 09:53:07]  see why this is important in a second
[09:53:04 -> 09:53:11]  here um but if we actually pop to our
[09:53:07 -> 09:53:14]  next step we can go probabilities equals
[09:53:11 -> 09:53:16]  softmax of Y PR so we're going to see
[09:53:14 -> 09:53:19]  these values get exponentiated we can
[09:53:16 -> 09:53:21]  print out our probabilities so we can
[09:53:19 -> 09:53:23]  see that as a part of this which values
[09:53:21 -> 09:53:25]  were the biggest right so this one was
[09:53:23 -> 09:53:28]  the biggest like you know closer to
[09:53:25 -> 09:53:30]  positive Infinity um and the this is
[09:53:28 -> 09:53:31]  like negative negative negative negative
[09:53:30 -> 09:53:33]  right so this is like the biggest this
[09:53:31 -> 09:53:35]  is the second biggest right so we can
[09:53:33 -> 09:53:36]  see this is the biggest number this is
[09:53:35 -> 09:53:37]  the second biggest and this is like
[09:53:36 -> 09:53:41]  these are these numbers are like smaller
[09:53:37 -> 09:53:44]  right um and same for this one we notice
[09:53:41 -> 09:53:46]  how oh this 1.95 is really big and then
[09:53:44 -> 09:53:48]  we have a 1.2 so it's like this is the
[09:53:46 -> 09:53:51]  biggest second biggest right that's kind
[09:53:48 -> 09:53:54]  of how that plays out
[09:53:51 -> 09:54:00]  then we're going to go and do correct uh
[09:53:54 -> 09:54:01]  correct log probs is going to be um or
[09:54:00 -> 09:54:05]  correct correct
[09:54:01 -> 09:54:06]  probs we're going to do um this part
[09:54:05 -> 09:54:07]  we're going to do this part here so
[09:54:06 -> 09:54:10]  probabilities and then we're going to
[09:54:07 -> 09:54:12]  stick these two inside of it all right
[09:54:10 -> 09:54:12]  so when we do
[09:54:15 -> 09:54:22]  [Music]
[09:54:17 -> 09:54:27]  this we notice that what this does is it
[09:54:22 -> 09:54:30]  gives us um it gives us the indices of
[09:54:27 -> 09:54:32]  the correct probs inside of the
[09:54:30 -> 09:54:34]  prediction in inside of the prediction
[09:54:32 -> 09:54:39]  uh Matrix
[09:54:34 -> 09:54:44]  right so inside of here if we actually
[09:54:39 -> 09:54:47]  print out um if we print out you know
[09:54:44 -> 09:54:47]  np.
[09:54:47 -> 09:54:51]  arrange right and then we have our y
[09:54:49 -> 09:54:53]  true
[09:54:51 -> 09:54:56]  we can essentially think of this four
[09:54:53 -> 09:54:59]  and one as our correct labels so in in
[09:54:56 -> 09:55:01]  batch number in batch uh in in this
[09:54:59 -> 09:55:04]  first batch element the correct index
[09:55:01 -> 09:55:08]  the the the correct answer is uh is
[09:55:04 -> 09:55:10]  number four right so it Go 0 1 2 3 4
[09:55:08 -> 09:55:11]  this is supposed to be close to one if
[09:55:10 -> 09:55:13]  this is close to one we're doing a good
[09:55:11 -> 09:55:14]  job that means the loss is going to be
[09:55:13 -> 09:55:16]  low right this is the this is the
[09:55:14 -> 09:55:18]  correct answer so if everything is like
[09:55:16 -> 09:55:20]  favoring this index here if this was the
[09:55:18 -> 09:55:22]  correct say class to pick
[09:55:20 -> 09:55:25]  um that that's what we want to optimize
[09:55:22 -> 09:55:27]  for then this one is picking out the
[09:55:25 -> 09:55:28]  same thing but for the second element
[09:55:27 -> 09:55:32]  right so that's why we do when we go up
[09:55:28 -> 09:55:34]  to Y true here we're doing um that's why
[09:55:32 -> 09:55:36]  we do this batch size here so it's going
[09:55:34 -> 09:55:39]  to span the elements the total number of
[09:55:36 -> 09:55:42]  elements in batch size um and it's going
[09:55:39 -> 09:55:44]  to just select the the indices right so
[09:55:42 -> 09:55:46]  we kind of just do a random number here
[09:55:44 -> 09:55:48]  because it's between zero and numb
[09:55:46 -> 09:55:52]  classes so you know it goes from zero to
[09:55:48 -> 09:55:55]  essentially four so one 2 3 4 5 or 0 1 2
[09:55:52 -> 09:55:56]  3 4 um and so that that's what's
[09:55:55 -> 09:56:00]  happening there we just initialize
[09:55:56 -> 09:56:02]  things randomly um but we see that we
[09:56:00 -> 09:56:06]  did number four so it's going to pluck
[09:56:02 -> 09:56:08]  out number four of the first one so 1
[09:56:06 -> 09:56:09]  0.165 then same thing for the second
[09:56:08 -> 09:56:12]  right we have we have index one so it's
[09:56:09 -> 09:56:15]  going to go to
[09:56:12 -> 09:56:15]  0.227
[09:56:16 -> 09:56:24]  now we have this we we have this correct
[09:56:19 -> 09:56:27]  probs which I just explained here
[09:56:24 -> 09:56:29]  and if we go down or sorry if we if we
[09:56:27 -> 09:56:31]  pop up a little bit further we see that
[09:56:29 -> 09:56:34]  we just did this part right so now if I
[09:56:31 -> 09:56:40]  wrap np. log around that
[09:56:34 -> 09:56:44]  term np. log we'll just say
[09:56:40 -> 09:56:47]  um we'll just say correct uh log probs
[09:56:44 -> 09:56:51]  we'll just do np. log of that
[09:56:47 -> 09:56:54]  um and then if we do correct log
[09:56:51 -> 09:56:56]  probs uh we notice that we're just doing
[09:56:54 -> 09:56:59]  the log of each of these of each of
[09:56:56 -> 09:57:01]  these number right so so if we go import
[09:56:59 -> 09:57:07]  math and then we go
[09:57:01 -> 09:57:07]  um math. log of
[09:57:08 -> 09:57:14]  0.165 we get this number right so uh -
[09:57:12 -> 09:57:16]  1.8 we're just doing this for each of
[09:57:14 -> 09:57:20]  these elements right and then we
[09:57:16 -> 09:57:24]  continue further and we go
[09:57:20 -> 09:57:27]  loss is the sum
[09:57:24 -> 09:57:29]  of loss is the uh sum of all of those so
[09:57:27 -> 09:57:30]  the correct log problem is going to sum
[09:57:29 -> 09:57:33]  these all up together so it's going to
[09:57:30 -> 09:57:35]  be like uh negative 1.8 and then
[09:57:33 -> 09:57:38]  negative we're on 1.5 so they're going
[09:57:35 -> 09:57:43]  to add and they're going to get about -
[09:57:38 -> 09:57:44]  3.3 umga 3.47 or
[09:57:43 -> 09:57:47]  3.27
[09:57:44 -> 09:57:48]  um and then we divide all of this by
[09:57:47 -> 09:57:50]  batch size to kind of you know normalize
[09:57:48 -> 09:57:51]  AC cross batch we don't want it to to
[09:57:50 -> 09:57:53]  get too massive like if you increase
[09:57:51 -> 09:57:54]  your batch size to like a thousand then
[09:57:53 -> 09:57:55]  your loss is going to be insanely high
[09:57:54 -> 09:57:57]  and you're going to get numerical
[09:57:55 -> 09:57:59]  instability from that you just don't
[09:57:57 -> 09:58:01]  want it so you want to normalize over
[09:57:59 -> 09:58:02]  the over the amount of samples that you
[09:58:01 -> 09:58:04]  actually have in the batch and that's
[09:58:02 -> 09:58:06]  going to help stabilize things for us
[09:58:04 -> 09:58:08]  later during the training process we
[09:58:06 -> 09:58:10]  don't want things to like step up and
[09:58:08 -> 09:58:12]  get worse every single training step um
[09:58:10 -> 09:58:15]  so if we go ahead and print out the loss
[09:58:12 -> 09:58:21]  we notice um we notice how we get this
[09:58:15 -> 09:58:26]  1.64 right so go back going back up here
[09:58:21 -> 09:58:28]  um this was you know this was quite off
[09:58:26 -> 09:58:30]  this was a little bit less off but it's
[09:58:28 -> 09:58:33]  still fairly high like these are still
[09:58:30 -> 09:58:37]  like this is like 16% 177% chance and
[09:58:33 -> 09:58:39]  this is 23% chance and so that's not
[09:58:37 -> 09:58:43]  very good so our loss is going to be
[09:58:39 -> 09:58:45]  higher but if our um if our correct
[09:58:43 -> 09:58:45]  probs
[09:58:45 -> 09:58:54]  array sorry our correct probs array was
[09:58:50 -> 09:58:59]  um see numpy do
[09:58:54 -> 09:58:59]  array and we go say
[09:58:59 -> 09:59:02]  um
[09:59:02 -> 09:59:06]  0.9 and
[09:59:06 -> 09:59:12]  0.8 and then we uh you know continue to
[09:59:09 -> 09:59:16]  go through this so correct uh correct
[09:59:12 -> 09:59:20]  log probs uh equals and then and then we
[09:59:16 -> 09:59:22]  do our loss right and we print out out
[09:59:20 -> 09:59:24]  our loss again loss is going to be
[09:59:22 -> 09:59:28]  significantly lower because these values
[09:59:24 -> 09:59:31]  the the the difference between uh the
[09:59:28 -> 09:59:34]  difference between our prediction
[09:59:31 -> 09:59:36]  accuracy and the actual label was very
[09:59:34 -> 09:59:38]  close right so we we thought there was a
[09:59:36 -> 09:59:39]  90% chance that this index was going to
[09:59:38 -> 09:59:41]  be it right and we were we we were we
[09:59:39 -> 09:59:42]  were correct right so that's a very high
[09:59:41 -> 09:59:44]  confidence that we were correct and
[09:59:42 -> 09:59:46]  that's a good thing this is also quite
[09:59:44 -> 09:59:48]  high confidence that we were correct so
[09:59:46 -> 09:59:50]  we want to reward ourselves for that or
[09:59:48 -> 09:59:52]  minimize the loss right so if we have
[09:59:50 -> 09:59:55]  values like 10% chance or 5% chance our
[09:59:52 -> 09:59:57]  loss is going to be stupid high but uh
[09:59:55 -> 09:59:58]  if we get closer and closer to what what
[09:59:57 -> 10:00:01]  we sort of minimizing the difference
[09:59:58 -> 10:00:03]  between what the model thinks it is and
[10:00:01 -> 10:00:04]  what the actual thing is that is a good
[10:00:03 -> 10:00:06]  thing that's that's what we're trying to
[10:00:04 -> 10:00:09]  optimize for
[10:00:06 -> 10:00:12]  here now there's another little step
[10:00:09 -> 10:00:15]  that comes along with this
[10:00:12 -> 10:00:18]  and that's the derivative of the loss
[10:00:15 -> 10:00:20]  right so if you go over to Gro here and
[10:00:18 -> 10:00:23]  say um
[10:00:20 -> 10:00:27]  what what
[10:00:23 -> 10:00:29]  is what is the derivative of cross
[10:00:27 -> 10:00:29]  entropy
[10:00:32 -> 10:00:38]  loss so it's defined as this which we
[10:00:35 -> 10:00:41]  just went over so Yi is the true label
[10:00:38 -> 10:00:41]  either zero or
[10:00:43 -> 10:00:49]  one and this is the predict so so Yi
[10:00:47 -> 10:00:51]  without a hat is the true label and Y
[10:00:49 -> 10:00:53]  hat I is the protective probability and
[10:00:51 -> 10:00:54]  we run through this we do some we do
[10:00:53 -> 10:00:57]  some
[10:00:54 -> 10:00:59]  differentiation um we go a few steps
[10:00:57 -> 10:01:01]  later I'm not going to really cover the
[10:00:59 -> 10:01:02]  math behind this differentiation part
[10:01:01 -> 10:01:04]  this part is you know you can you can do
[10:01:02 -> 10:01:06]  that on your own if you really feel it's
[10:01:04 -> 10:01:10]  necessary um but really what we care
[10:01:06 -> 10:01:12]  about is just the answer so the actual
[10:01:10 -> 10:01:15]  derivative so putting it all together
[10:01:12 -> 10:01:18]  look at the gradient um what we what we
[10:01:15 -> 10:01:24]  actually care about is
[10:01:18 -> 10:01:27]  um why hat minus y and notice y hat was
[10:01:24 -> 10:01:30]  uh the predicted probability so this so
[10:01:27 -> 10:01:33]  that soft maxed uh the soft maxed logits
[10:01:30 -> 10:01:35]  right uh the probability distribution
[10:01:33 -> 10:01:39]  and then just normal Y is the true label
[10:01:35 -> 10:01:41]  so if we go back to our code here um and
[10:01:39 -> 10:01:45]  we look at how that's
[10:01:41 -> 10:01:47]  calculated we pop down and we see it is
[10:01:45 -> 10:01:49]  the softmax probability distribution
[10:01:47 -> 10:01:52]  minus the true labels right so that's
[10:01:49 -> 10:01:54]  exact what we want it to be um and a lot
[10:01:52 -> 10:01:56]  of this what we're doing here this is
[10:01:54 -> 10:02:01]  mainly just converting things to the
[10:01:56 -> 10:02:03]  actual one hot Vector um so uh yeah
[10:02:01 -> 10:02:05]  that's that that's pretty much that's
[10:02:03 -> 10:02:07]  pretty much how you do soft Max and then
[10:02:05 -> 10:02:10]  derivative or sorry cross entropy loss
[10:02:07 -> 10:02:14]  and derivative of it right now going
[10:02:10 -> 10:02:16]  back um what's next so now we have now
[10:02:14 -> 10:02:22]  we actually have to go through and
[10:02:16 -> 10:02:26]  calculate the the gradient of of our W2
[10:02:22 -> 10:02:29]  our X2 even maybe possibly Ford ones and
[10:02:26 -> 10:02:32]  the bias right so let's go and look at
[10:02:29 -> 10:02:35]  that I'm going to do an example where we
[10:02:32 -> 10:02:37]  pretty much do uh we calculate dw1 and
[10:02:35 -> 10:02:39]  then we kind of just bring uh the
[10:02:37 -> 10:02:41]  intuition from that into everything else
[10:02:39 -> 10:02:44]  and then go ahead and apply it right so
[10:02:41 -> 10:02:47]  in in dw1 I'm going to I'm actually
[10:02:44 -> 10:02:51]  going to go to this in a second here
[10:02:47 -> 10:02:55]  um but I'll bring my
[10:02:51 -> 10:02:58]  whiteb a little closer so I don't know
[10:02:55 -> 10:03:00]  if you can see this but I pretty much
[10:02:58 -> 10:03:04]  did um based on you know the micr
[10:03:00 -> 10:03:08]  tutorial I did uh like an output this is
[10:03:04 -> 10:03:10]  a neuron output right so X1 * W1 + X2 *
[10:03:08 -> 10:03:12]  W2 where it's like each of the X values
[10:03:10 -> 10:03:18]  go in and
[10:03:12 -> 10:03:19]  then um the the neuron will a s like two
[10:03:18 -> 10:03:22]  x values go in and there's a single
[10:03:19 -> 10:03:25]  neuron that has uh two weights in it um
[10:03:22 -> 10:03:30]  W1 and W2 and then it outputs those by
[10:03:25 -> 10:03:35]  doing um by doing a doc product between
[10:03:30 -> 10:03:38]  so it does X1 * W1 and then plus that to
[10:03:35 -> 10:03:40]  X2 * W2 and I just wrote this in the
[10:03:38 -> 10:03:44]  context of microG grad so there's a data
[10:03:40 -> 10:03:46]  and a grad attribute for this there is a
[10:03:44 -> 10:03:48]  a data and a grad attribute for this so
[10:03:46 -> 10:03:51]  notice how I did the the this is a plus
[10:03:48 -> 10:03:54]  sign I know you can't see it because
[10:03:51 -> 10:03:56]  it's yeah it's very small but ient did a
[10:03:54 -> 10:03:58]  plus sign so those two added together
[10:03:56 -> 10:04:00]  and the gradients are going to the this
[10:03:58 -> 10:04:03]  grad right here of two that's going to
[10:04:00 -> 10:04:07]  flow to both of these nodes this X1 * W1
[10:04:03 -> 10:04:09]  and X2 * W2 that's going to flow to
[10:04:07 -> 10:04:12]  these um the same so when you when you
[10:04:09 -> 10:04:15]  like uh for example when you're try to
[10:04:12 -> 10:04:17]  differentiate a constant um it it ends
[10:04:15 -> 10:04:19]  up just it ends up just becoming one
[10:04:17 -> 10:04:22]  right so there's actually no change when
[10:04:19 -> 10:04:24]  you have um like a graph which is which
[10:04:22 -> 10:04:27]  you add like a bias to it and just shift
[10:04:24 -> 10:04:29]  that curve upwards it still has the same
[10:04:27 -> 10:04:31]  slope right so that's essentially what's
[10:04:29 -> 10:04:33]  happening here and then we continue
[10:04:31 -> 10:04:37]  forward and this now becomes just a
[10:04:33 -> 10:04:39]  multiplication so W2 uh or or sorry X1 *
[10:04:37 -> 10:04:41]  W1 and then we have a data and a grad
[10:04:39 -> 10:04:43]  attribute for those so that's just like
[10:04:41 -> 10:04:45]  kind of how a neuron is going to be
[10:04:43 -> 10:04:50]  structured in the context of
[10:04:45 -> 10:04:53]  micrograph now uh if we pop over to
[10:04:50 -> 10:04:55]  um if we pop over to this
[10:04:53 -> 10:05:00]  example which
[10:04:55 -> 10:05:04]  is over here so this dw1
[10:05:00 -> 10:05:08]  output what I pretty much did is we go
[10:05:04 -> 10:05:11]  uh 784 Time by B so let's just draw that
[10:05:08 -> 10:05:14]  out really quick
[10:05:11 -> 10:05:14]  um
[10:05:17 -> 10:05:23]  so 784 by
[10:05:20 -> 10:05:29]  by
[10:05:23 -> 10:05:29]  B and then we have um B by 256
[10:05:29 -> 10:05:33]  right this is an at symbol it's really
[10:05:31 -> 10:05:36]  bad
[10:05:33 -> 10:05:38]  drawing
[10:05:36 -> 10:05:41]  B by
[10:05:38 -> 10:05:43]  256 all right so how do we actually
[10:05:41 -> 10:05:45]  Matrix multiply here well we take a
[10:05:43 -> 10:05:47]  column of
[10:05:45 -> 10:05:50]  this and we draw product with a row of
[10:05:47 -> 10:05:51]  this right and notice how this is
[10:05:50 -> 10:05:53]  batches right so this is going to be a
[10:05:51 -> 10:05:54]  batch element this is a batch element
[10:05:53 -> 10:05:58]  this is a batch element so it's going
[10:05:54 -> 10:06:01]  this is we're essentially doing
[10:05:58 -> 10:06:03]  um we're we're taking the first pixel
[10:06:01 -> 10:06:05]  across the first batch element so if we
[10:06:03 -> 10:06:06]  were to just go here that would be an
[10:06:05 -> 10:06:07]  entire image right that would be the
[10:06:06 -> 10:06:09]  first batch element it would be an
[10:06:07 -> 10:06:12]  entire image but we're doing the first
[10:06:09 -> 10:06:13]  pixel across the entire batch right so
[10:06:12 -> 10:06:15]  across all the different batch elements
[10:06:13 -> 10:06:20]  we're taking we're do producting the
[10:06:15 -> 10:06:23]  pixel values um so like for example X
[10:06:20 -> 10:06:23]  um x.
[10:06:24 -> 10:06:30]  data at
[10:06:26 -> 10:06:32]  index0 right and then in this context
[10:06:30 -> 10:06:35]  this is B by 256 which is the same as
[10:06:32 -> 10:06:39]  the uh output layer so if we did like
[10:06:35 -> 10:06:41]  for example the the classical input of
[10:06:39 -> 10:06:44]  um you go back to this it's like B by
[10:06:41 -> 10:06:47]  784 * 784 by 256 you end up with B by
[10:06:44 -> 10:06:49]  256 and that's our output gradient right
[10:06:47 -> 10:06:51]  so we're we're essentially just just
[10:06:49 -> 10:06:53]  taking this and we're shifting it around
[10:06:51 -> 10:06:56]  so we're doing we're taking this B by
[10:06:53 -> 10:06:58]  256 that stays the same but we're
[10:06:56 -> 10:07:02]  transposing the input so it's like x.t
[10:06:58 -> 10:07:05]  or sorry x. data
[10:07:02 -> 10:07:06]  transpose um Matrix multiply that with
[10:07:05 -> 10:07:08]  the output output gradient and we're
[10:07:06 -> 10:07:12]  getting the gradient for the weights
[10:07:08 -> 10:07:14]  right the dw1 value um so if we go back
[10:07:12 -> 10:07:19]  to here
[10:07:14 -> 10:07:21]  256 remember um 784 is how many weights
[10:07:19 -> 10:07:22]  are in a single neuron but 256 is the
[10:07:21 -> 10:07:26]  amount of neurons we actually have in
[10:07:22 -> 10:07:27]  that hidden layer so 256 is going to be
[10:07:26 -> 10:07:29]  like this is this these are all the
[10:07:27 -> 10:07:31]  neurons laid out right but what we're
[10:07:29 -> 10:07:34]  going to do is instead of taking like a
[10:07:31 -> 10:07:36]  a single a single set of all the neurons
[10:07:34 -> 10:07:38]  from a single batch element we're going
[10:07:36 -> 10:07:41]  to take all of the all of the the index
[10:07:38 -> 10:07:45]  zero gradients uh across
[10:07:41 -> 10:07:47]  the uh across the
[10:07:45 -> 10:07:50]  across
[10:07:47 -> 10:07:53]  essentially uh batch element
[10:07:50 -> 10:07:55]  no sorry not batch element zero across
[10:07:53 -> 10:07:57]  we're going to take the first neurons
[10:07:55 -> 10:08:01]  across the entire batch that's what
[10:07:57 -> 10:08:06]  we're doing so this ends up being um
[10:08:01 -> 10:08:06]  we'll just say y.
[10:08:06 -> 10:08:09]  grad
[10:08:14 -> 10:08:20]  um y. grad at index zero we're doing it
[10:08:18 -> 10:08:22]  across the entire B batch so it's just a
[10:08:20 -> 10:08:24]  single neuron right it's just a single
[10:08:22 -> 10:08:26]  neuron but we're generalizing that
[10:08:24 -> 10:08:28]  across the entire batch so we have this
[10:08:26 -> 10:08:29]  x component generalizing across the
[10:08:28 -> 10:08:31]  whole batch and this y component
[10:08:29 -> 10:08:33]  generalizing across the whole batch we
[10:08:31 -> 10:08:37]  use the same intuition that we got from
[10:08:33 -> 10:08:39]  the microgr lecture and we just apply it
[10:08:37 -> 10:08:41]  here except we get that generalization
[10:08:39 -> 10:08:42]  ability from using batch processing
[10:08:41 -> 10:08:44]  that's where the big thing comes in here
[10:08:42 -> 10:08:47]  because we're doing we're essentially
[10:08:44 -> 10:08:48]  doing uh columns here do product with
[10:08:47 -> 10:08:51]  rows and notice the shapes are oriented
[10:08:48 -> 10:08:51]  in a certain way
[10:08:51 -> 10:08:55]  yeah
[10:08:52 -> 10:09:00]  it's it it is a lot to take in but in
[10:08:55 -> 10:09:03]  the end we end up with um so this is
[10:09:00 -> 10:09:04]  this is like 784 by B and then B by 256
[10:09:03 -> 10:09:07]  so the B's cancel out and we're left
[10:09:04 -> 10:09:07]  with
[10:09:08 -> 10:09:12]  um I know these shapes are out of
[10:09:10 -> 10:09:17]  proportion
[10:09:12 -> 10:09:19]  784 by 256 right so this 784 that's the
[10:09:17 -> 10:09:22]  first pixel value so each of these or
[10:09:19 -> 10:09:23]  sorry each each of these elements that's
[10:09:22 -> 10:09:25]  a pixel
[10:09:23 -> 10:09:28]  right that's a pixel in the whole
[10:09:25 -> 10:09:31]  flatten image and these are all the
[10:09:28 -> 10:09:34]  neurons so we end up calculating these
[10:09:31 -> 10:09:36]  um in a way that we can that we can
[10:09:34 -> 10:09:38]  actually update uh all of the weights
[10:09:36 -> 10:09:40]  properly right so when all these are
[10:09:38 -> 10:09:43]  laid out you have um the gradient for
[10:09:40 -> 10:09:47]  each neuron across the first uh across
[10:09:43 -> 10:09:49]  the first pixel value and that is the
[10:09:47 -> 10:09:51]  that is the same way that our initial
[10:09:49 -> 10:09:53]  weight Matrix is organized right so in
[10:09:51 -> 10:09:56]  our weight matrix it's taking these
[10:09:53 -> 10:09:58]  Columns of of like a single neuron of
[10:09:56 -> 10:10:02]  weights and do producting that with with
[10:09:58 -> 10:10:04]  a single um with a with a single image
[10:10:02 -> 10:10:06]  right and it and it's doing that and
[10:10:04 -> 10:10:10]  that's very intuitive and it makes sense
[10:10:06 -> 10:10:10]  uh but in this example
[10:10:10 -> 10:10:15]  um you know we we end up getting those
[10:10:14 -> 10:10:17]  we end up getting the same ones that we
[10:10:15 -> 10:10:19]  would we would want to update those with
[10:10:17 -> 10:10:21]  so you can kind of Translate and see
[10:10:19 -> 10:10:23]  like what is this column here what does
[10:10:21 -> 10:10:26]  that consist of and what does this row
[10:10:23 -> 10:10:31]  here consist of and we can plug that in
[10:10:26 -> 10:10:33]  and we can see that our weight updates
[10:10:31 -> 10:10:35]  are actually going to make sense here so
[10:10:33 -> 10:10:39]  in this one we we essentially end up
[10:10:35 -> 10:10:43]  with this thing of 256 values and that
[10:10:39 -> 10:10:46]  is just going to be the essentially the
[10:10:43 -> 10:10:48]  gradient of all of the neurons with
[10:10:46 -> 10:10:51]  respect to a single Pixel right cuz this
[10:10:48 -> 10:10:52]  is a single Pixel across an entire batch
[10:10:51 -> 10:10:55]  we're again we're just using the batch
[10:10:52 -> 10:10:56]  generalization idea here um we're just
[10:10:55 -> 10:10:58]  like generalizing across whole batch
[10:10:56 -> 10:10:59]  instead of using one specific sample
[10:10:58 -> 10:11:02]  that way it's like better for
[10:10:59 -> 10:11:04]  stabilizing training um and and we're
[10:11:02 -> 10:11:08]  just we're taking that and we're just
[10:11:04 -> 10:11:12]  laying everything out for
[10:11:08 -> 10:11:15]  um for all of the neurons across a
[10:11:12 -> 10:11:17]  single um across a single Pixel right
[10:11:15 -> 10:11:19]  that's how we're calculating these so
[10:11:17 -> 10:11:21]  this this uh it's going to be like this
[10:11:19 -> 10:11:23]  row this is our pixel AC like all the
[10:11:21 -> 10:11:27]  pixels for like for all all the first
[10:11:23 -> 10:11:30]  pixel for all the batches and then we
[10:11:27 -> 10:11:33]  have our our single
[10:11:30 -> 10:11:35]  um we have our our first neuron and it's
[10:11:33 -> 10:11:36]  going to go first neuron boom it's going
[10:11:35 -> 10:11:39]  to put the first value there and then
[10:11:36 -> 10:11:41]  second neuron boom all the way to 256
[10:11:39 -> 10:11:44]  neurons and it's going to spit this out
[10:11:41 -> 10:11:48]  in a column
[10:11:44 -> 10:11:50]  right that's all of the neurons um for
[10:11:48 -> 10:11:52]  the for the entire first
[10:11:50 -> 10:11:53]  pixel and then if we look at how it goes
[10:11:52 -> 10:11:57]  down
[10:11:53 -> 10:11:59]  columnwise then we get um then we get
[10:11:57 -> 10:12:01]  all the neurons for the second pixel and
[10:11:59 -> 10:12:03]  the third pixel and the fourth one right
[10:12:01 -> 10:12:06]  and so we can sort of
[10:12:03 -> 10:12:07]  see that this ties back again into uh
[10:12:06 -> 10:12:10]  the original forward pass example that
[10:12:07 -> 10:12:12]  we were doing um if this if this doesn't
[10:12:10 -> 10:12:16]  make sense I know my explanations might
[10:12:12 -> 10:12:18]  be bad um you know you might want to go
[10:12:16 -> 10:12:20]  back and and I mean you you could draw
[10:12:18 -> 10:12:22]  this out your yourself um that does work
[10:12:20 -> 10:12:25]  and just sort of visualizing it in your
[10:12:22 -> 10:12:27]  head that's a good idea to do uh that is
[10:12:25 -> 10:12:29]  a good idea to try to understand how
[10:12:27 -> 10:12:31]  this is working um alternatively you can
[10:12:29 -> 10:12:33]  you can watch my tensor uh my tensor
[10:12:31 -> 10:12:35]  level back propop video I think I did a
[10:12:33 -> 10:12:38]  good job on that um it's about 30
[10:12:35 -> 10:12:41]  minutes long so uh
[10:12:38 -> 10:12:44]  anyways well we if if this doesn't
[10:12:41 -> 10:12:49]  entirely make sense we're going to use
[10:12:44 -> 10:12:49]  this intuition of we have this 784
[10:12:49 -> 10:12:54]  by
[10:12:50 -> 10:12:59]  256 and this is the same as just our our
[10:12:54 -> 10:12:59]  W um our w. dat
[10:13:00 -> 10:13:04]  right so our w.g
[10:13:08 -> 10:13:14]  grad and our
[10:13:11 -> 10:13:17]  w. dat have the same
[10:13:14 -> 10:13:19]  shape so what we can essentially do is
[10:13:17 -> 10:13:22]  we can take
[10:13:19 -> 10:13:25]  um say in in the grad in the gradient
[10:13:22 -> 10:13:27]  for example we can take this value
[10:13:25 -> 10:13:30]  multiply it by a learning rate of say LR
[10:13:27 -> 10:13:33]  equal
[10:13:30 -> 10:13:36]  0.1 and we can then subtract this from
[10:13:33 -> 10:13:38]  the original one right so if the
[10:13:36 -> 10:13:40]  gradient is really high that means
[10:13:38 -> 10:13:43]  there's a lot of error and if the
[10:13:40 -> 10:13:46]  gradient is really low um that means
[10:13:43 -> 10:13:48]  that there's there's going to be um that
[10:13:46 -> 10:13:49]  means there's going to be less error
[10:13:48 -> 10:13:51]  right this is why it's called a
[10:13:49 -> 10:13:53]  stochastic gradient descent cuz you're
[10:13:51 -> 10:14:01]  descending the gradient um so it's going
[10:13:53 -> 10:14:05]  to be essentially um w. data equals LR
[10:14:01 -> 10:14:08]  times um just do X for times learning
[10:14:05 -> 10:14:11]  ratees
[10:14:08 -> 10:14:16]  times the grad element
[10:14:11 -> 10:14:19]  right and then we do um minus
[10:14:16 -> 10:14:21]  equals so that if the gradient values
[10:14:19 -> 10:14:23]  really high and learning rate is 0.1
[10:14:21 -> 10:14:25]  then it's going to be positive time
[10:14:23 -> 10:14:26]  negative and then this is going to get
[10:14:25 -> 10:14:30]  adjusted in the negative direction if
[10:14:26 -> 10:14:32]  the gradient is really high um and if
[10:14:30 -> 10:14:34]  the gradient is really if it's really
[10:14:32 -> 10:14:37]  negative gradient is really
[10:14:34 -> 10:14:41]  negative
[10:14:37 -> 10:14:43]  um then this is going to this is going
[10:14:41 -> 10:14:45]  to multiply with this it's going to give
[10:14:43 -> 10:14:47]  a negative value and then since we're
[10:14:45 -> 10:14:49]  subtracting a negative it's going to go
[10:14:47 -> 10:14:50]  up and the way it's going to go up so
[10:14:49 -> 10:14:52]  that's that's literally all we're doing
[10:14:50 -> 10:14:55]  in gradient
[10:14:52 -> 10:14:57]  descent and we're doing that for each
[10:14:55 -> 10:15:01]  element now we can sort of take that
[10:14:57 -> 10:15:05]  intuition and Branch it off to you know
[10:15:01 -> 10:15:08]  dw2 right and we can apply that to
[10:15:05 -> 10:15:10]  dx2 um you know this isn't really a
[10:15:08 -> 10:15:12]  course on back propop so don't worry too
[10:15:10 -> 10:15:14]  much if that doesn't completely make
[10:15:12 -> 10:15:16]  sense it is important but if it doesn't
[10:15:14 -> 10:15:19]  completely make sense you're still going
[10:15:16 -> 10:15:19]  to be okay um cuz we're going to
[10:15:19 -> 10:15:21]  implement this and you're going to
[10:15:19 -> 10:15:23]  actually be able to see the network
[10:15:21 -> 10:15:24]  learning and you can actually print out
[10:15:23 -> 10:15:26]  things to understand what's happening
[10:15:24 -> 10:15:29]  under the
[10:15:26 -> 10:15:32]  hood and of course we do need these uh
[10:15:29 -> 10:15:34]  these these X values for calculating the
[10:15:32 -> 10:15:37]  intermediate these intermediate layers
[10:15:34 -> 10:15:39]  right with activation functions um and
[10:15:37 -> 10:15:42]  the activation functions like literally
[10:15:39 -> 10:15:45]  if you want me to like draw out how that
[10:15:42 -> 10:15:45]  would work
[10:15:47 -> 10:15:50]  um Rue
[10:15:51 -> 10:15:55]  it goes like this
[10:15:52 -> 10:15:59]  right it's like it's like a line and
[10:15:55 -> 10:16:01]  then it goes up just like that so what
[10:15:59 -> 10:16:03]  you can do for this is you can say if my
[10:16:01 -> 10:16:05]  value is zero or less I'm going to set
[10:16:03 -> 10:16:07]  the gradient to zero because there's no
[10:16:05 -> 10:16:09]  slope the slope is
[10:16:07 -> 10:16:12]  zero and if it's if it's if it's greater
[10:16:09 -> 10:16:15]  than that if it's greater than zero then
[10:16:12 -> 10:16:16]  uh we're going to set that to one right
[10:16:15 -> 10:16:17]  because if it's it's really is going to
[10:16:16 -> 10:16:20]  make it remain the same if it's above
[10:16:17 -> 10:16:23]  zero
[10:16:20 -> 10:16:23]  um okay
[10:16:24 -> 10:16:29]  awesome now let's pop into um sort of
[10:16:28 -> 10:16:32]  this this python script that we have
[10:16:29 -> 10:16:34]  running here and uh just dissect
[10:16:32 -> 10:16:37]  everything that's happening so we can
[10:16:34 -> 10:16:39]  just tie it back to uh this image that
[10:16:37 -> 10:16:40]  we that that I wrote in excal draw just
[10:16:39 -> 10:16:42]  so that everything kind of makes sense
[10:16:40 -> 10:16:43]  on the code on on a conceptual and a
[10:16:42 -> 10:16:45]  code
[10:16:43 -> 10:16:48]  level all right so just kind of going
[10:16:45 -> 10:16:51]  through uh this numpy script now uh we
[10:16:48 -> 10:16:52]  import numpy normally um I mean I
[10:16:51 -> 10:16:54]  actually already went through this part
[10:16:52 -> 10:16:55]  I'm not going to review this again um
[10:16:54 -> 10:16:56]  but if we go down you can see a bunch of
[10:16:55 -> 10:16:58]  functions here right and these are all
[10:16:56 -> 10:17:00]  these are all super useful functions
[10:16:58 -> 10:17:02]  that we're going to use to essentially
[10:17:00 -> 10:17:04]  uh train a single layer single hidden
[10:17:02 -> 10:17:06]  layer multi-layer perceptron from
[10:17:04 -> 10:17:08]  scratch um using the intuition we
[10:17:06 -> 10:17:12]  previously built on that from you know
[10:17:08 -> 10:17:14]  microgr Etc right so we we scroll down
[10:17:12 -> 10:17:16]  and in this main function we declare
[10:17:14 -> 10:17:19]  some some variables here so like hidden
[10:17:16 -> 10:17:21]  size which I'll just set to like 256 in
[10:17:19 -> 10:17:23]  this case um output size is 10 right so
[10:17:21 -> 10:17:26]  10 different digits 0 through 9 and our
[10:17:23 -> 10:17:28]  input size is 28x 28 pixels flattened
[10:17:26 -> 10:17:30]  out um and then we're going to put our
[10:17:28 -> 10:17:34]  bat size I'll just put eight here so we
[10:17:30 -> 10:17:40]  can get speed um learning rate of
[10:17:34 -> 10:17:42]  0.1 or or um alternatively 1 * 10 -3 and
[10:17:40 -> 10:17:44]  then epox we're going to do five so epox
[10:17:42 -> 10:17:45]  is how many times you go through the
[10:17:44 -> 10:17:47]  entire training set right so you go
[10:17:45 -> 10:17:49]  through it once that's like certain
[10:17:47 -> 10:17:50]  number of iterations certain number of
[10:17:49 -> 10:17:52]  like forward and backward passes it's
[10:17:50 -> 10:17:54]  like one iteration and then you do
[10:17:52 -> 10:17:56]  multiple epochs which is you know times
[10:17:54 -> 10:17:59]  that you go over the train
[10:17:56 -> 10:18:01]  data so inside of here we declare this
[10:17:59 -> 10:18:03]  model neural network right input hidden
[10:18:01 -> 10:18:04]  and output size and then we just have
[10:18:03 -> 10:18:06]  this train function which is going to
[10:18:04 -> 10:18:09]  actually do that for us so inside of
[10:18:06 -> 10:18:10]  here we have number of epochs that we're
[10:18:09 -> 10:18:12]  going to do and inside of each eoch
[10:18:10 -> 10:18:13]  we're going to do we're going to train
[10:18:12 -> 10:18:15]  in batches right so we have this batch
[10:18:13 -> 10:18:18]  size a batch of images that are all
[10:18:15 -> 10:18:20]  flattened so it's like B by 7 784 and
[10:18:18 -> 10:18:22]  we're just going to step through um kind
[10:18:20 -> 10:18:24]  of going through one by one here so in
[10:18:22 -> 10:18:27]  the model. forward I mean we're just
[10:18:24 -> 10:18:30]  we're in this this one we're just taking
[10:18:27 -> 10:18:31]  um the X train is just input images and
[10:18:30 -> 10:18:34]  then this is the output labels right
[10:18:31 -> 10:18:37]  it's the it's the batch y so in the
[10:18:34 -> 10:18:39]  model forward um we input a batch and we
[10:18:37 -> 10:18:41]  get an output cache and we get a we get
[10:18:39 -> 10:18:44]  y right the Y predictions the
[10:18:41 -> 10:18:46]  probability distribution in a batch in
[10:18:44 -> 10:18:49]  batches
[10:18:46 -> 10:18:51]  right uh after we do model forward we're
[10:18:49 -> 10:18:54]  going to do take the loss function of Y
[10:18:51 -> 10:18:58]  PR and batch Y which we got from
[10:18:54 -> 10:19:01]  here and then we're going so cross cross
[10:18:58 -> 10:19:04]  entropy loss calculates the loss and
[10:19:01 -> 10:19:06]  then separately we're going to take that
[10:19:04 -> 10:19:09]  output again which keep in mind this is
[10:19:06 -> 10:19:11]  logits so not actual um probability
[10:19:09 -> 10:19:14]  distribution it's uh cross entropy loss
[10:19:11 -> 10:19:17]  is going to softmax those
[10:19:14 -> 10:19:18]  um and then it's going to return a loss
[10:19:17 -> 10:19:20]  right so it does soft Max inside of here
[10:19:18 -> 10:19:22]  so when we're actually getting the uh
[10:19:20 -> 10:19:23]  derivative of the Cross entropy loss we
[10:19:22 -> 10:19:25]  have to go and do that separately right
[10:19:23 -> 10:19:27]  we have to get our probability
[10:19:25 -> 10:19:30]  distribution and then we have to
[10:19:27 -> 10:19:32]  essentially just um like I walked
[10:19:30 -> 10:19:33]  through before how we actually do the
[10:19:32 -> 10:19:35]  how we do the cross entropy loss
[10:19:33 -> 10:19:37]  derivative I walked through that with
[10:19:35 -> 10:19:40]  grock previously and uh this is
[10:19:37 -> 10:19:41]  literally all we do so uh you know feel
[10:19:40 -> 10:19:42]  free to like print this out but this
[10:19:41 -> 10:19:45]  should be fairly intuitive if you work
[10:19:42 -> 10:19:47]  with python and pytorch before um and
[10:19:45 -> 10:19:49]  then we just you know do the do the
[10:19:47 -> 10:19:51]  minus for our gr out output and then we
[10:19:49 -> 10:19:52]  back propagate from there right so we
[10:19:51 -> 10:19:55]  take our grad output and we use the
[10:19:52 -> 10:19:58]  cache the
[10:19:55 -> 10:20:01]  cache um so keep in mind when we do
[10:19:58 -> 10:20:03]  model. forward we have ypr which is the
[10:20:01 -> 10:20:06]  logits and then we have the cache which
[10:20:03 -> 10:20:07]  is literally just um the inputs right so
[10:20:06 -> 10:20:09]  all the different pieces of the layer
[10:20:07 -> 10:20:11]  that we're going to need like for
[10:20:09 -> 10:20:16]  example our dx2 the derivative of the
[10:20:11 -> 10:20:18]  second um the second x value or um for
[10:20:16 -> 10:20:19]  example The Rue output right so so just
[10:20:18 -> 10:20:21]  stuff like this that we're going to need
[10:20:19 -> 10:20:23]  to back propagate through all the layers
[10:20:21 -> 10:20:25]  and not just like a single weight or
[10:20:23 -> 10:20:26]  single weight here we're going to need
[10:20:25 -> 10:20:28]  that whole cache of like through the
[10:20:26 -> 10:20:31]  forward pass right um so that's that's
[10:20:28 -> 10:20:33]  all that is it's just just a coule of
[10:20:31 -> 10:20:35]  those and then of course that output the
[10:20:33 -> 10:20:36]  loits right so just to clear up like
[10:20:35 -> 10:20:40]  what the heck cache means there I know
[10:20:36 -> 10:20:42]  that can be like sometimes misleading um
[10:20:40 -> 10:20:44]  so we do model. backward and then we
[10:20:42 -> 10:20:48]  just do model upd weights and we pass in
[10:20:44 -> 10:20:51]  you know weights bias um weights and
[10:20:48 -> 10:20:53]  bias again
[10:20:51 -> 10:20:56]  so let's walk through what's happening
[10:20:53 -> 10:20:57]  in forward this part I assume kind of
[10:20:56 -> 10:21:00]  just makes sense we'll walk through
[10:20:57 -> 10:21:01]  model. backward and then update weights
[10:21:00 -> 10:21:05]  so in
[10:21:01 -> 10:21:08]  forward uh model. forward over here so
[10:21:05 -> 10:21:11]  inside of here we have the batch size as
[10:21:08 -> 10:21:12]  x. shape at position zero so it's going
[10:21:11 -> 10:21:16]  to it's going to list the shape it's
[10:21:12 -> 10:21:18]  going to be B by 784 or sorry B by is
[10:21:16 -> 10:21:20]  going to be um the batch that we get so
[10:21:18 -> 10:21:23]  when we when we take this part we're
[10:21:20 -> 10:21:25]  getting an actual batch so I and then to
[10:21:23 -> 10:21:28]  I plus batch size so it gets a little
[10:21:25 -> 10:21:29]  segment of like eight eight images um
[10:21:28 -> 10:21:32]  and inside of here we're going to take
[10:21:29 -> 10:21:34]  the first the the the leading dimension
[10:21:32 -> 10:21:36]  of that which is batch size and we set
[10:21:34 -> 10:21:42]  batch size here and then we do reshape
[10:21:36 -> 10:21:45]  we go batch size by um and then the
[10:21:42 -> 10:21:47]  essentially just the the last the last
[10:21:45 -> 10:21:49]  one so it's going to be reshaped to
[10:21:47 -> 10:21:52]  batch size
[10:21:49 -> 10:21:56]  by um this is just a short way of doing
[10:21:52 -> 10:21:58]  28 * 28 so 784 that's what this is going
[10:21:56 -> 10:22:00]  to reshape to and then we go ahead and
[10:21:58 -> 10:22:02]  do our our linear forwards and these are
[10:22:00 -> 10:22:06]  these are should be very uh sensical
[10:22:02 -> 10:22:09]  right so in our linear forwards
[10:22:06 -> 10:22:11]  um we take in a weight sorry we take in
[10:22:09 -> 10:22:17]  an x a weight and a bias right so we do
[10:22:11 -> 10:22:21]  X at w + B right instead of w WX plus b
[10:22:17 -> 10:22:25]  it's x w Plus plus b um so that's it's I
[10:22:21 -> 10:22:30]  mean it's it's B by 784 we go back to
[10:22:25 -> 10:22:34]  this so we go in here B by 784 * 784 by
[10:22:30 -> 10:22:36]  256 and we end up with B by 256 right um
[10:22:34 -> 10:22:38]  should be fairly
[10:22:36 -> 10:22:40]  intuitive and then we add the we add the
[10:22:38 -> 10:22:41]  bias as well which is another term I
[10:22:40 -> 10:22:43]  actually did not include in this but you
[10:22:41 -> 10:22:45]  can kind of just we can kind of just
[10:22:43 -> 10:22:50]  think of the bias as like an extra extra
[10:22:45 -> 10:22:50]  thing that it adds on um
[10:22:50 -> 10:22:56]  now scroll down to relu right so relu I
[10:22:55 -> 10:22:59]  mean this is self-explanatory it's just
[10:22:56 -> 10:23:01]  going to do a point R it's going to be
[10:22:59 -> 10:23:03]  num. maximum it's going to apply that to
[10:23:01 -> 10:23:07]  every single value in there doing a you
[10:23:03 -> 10:23:08]  know if it's uh if if the value is like
[10:23:07 -> 10:23:10]  negative 1 then zero is going to be the
[10:23:08 -> 10:23:12]  maximum it's like which one is higher Z
[10:23:10 -> 10:23:14]  or negative 1 then it's going to pick
[10:23:12 -> 10:23:16]  zero and if it's like one then it's
[10:23:14 -> 10:23:17]  going to be like oh one is higher than
[10:23:16 -> 10:23:20]  zero right so it's just kind of the
[10:23:17 -> 10:23:21]  relue and then really derivative is you
[10:23:20 -> 10:23:24]  know as we explained before how you have
[10:23:21 -> 10:23:26]  like the chart and then goes like this D
[10:23:24 -> 10:23:28]  gradient of derivative of zero then it's
[10:23:26 -> 10:23:31]  going to go gradient one after after the
[10:23:28 -> 10:23:33]  zero right um so that's just that's what
[10:23:31 -> 10:23:37]  this is doing here um because we want to
[10:23:33 -> 10:23:40]  ra you derivative right
[10:23:37 -> 10:23:44]  um
[10:23:40 -> 10:23:47]  now going back another linear forward we
[10:23:44 -> 10:23:48]  take the Rue output so that's the new
[10:23:47 -> 10:23:50]  input to the to the next line your
[10:23:48 -> 10:23:53]  forward layer the the weights the
[10:23:50 -> 10:23:54]  weights two and then the bias two right
[10:23:53 -> 10:23:56]  so that that should also make sense and
[10:23:54 -> 10:23:58]  then we just return that so forward pass
[10:23:56 -> 10:23:59]  isn't actually too complicated we can
[10:23:58 -> 10:24:01]  sort of just walk through and understand
[10:23:59 -> 10:24:02]  how the shapes are changing this is more
[10:24:01 -> 10:24:05]  a template example of how to understand
[10:24:02 -> 10:24:07]  this from scratch Now we move down to
[10:24:05 -> 10:24:13]  backward which is a little harder um go
[10:24:07 -> 10:24:15]  to backward here we have the W1 B1 W2 B2
[10:24:13 -> 10:24:17]  right so we put in the grad output so
[10:24:15 -> 10:24:19]  the starting the wherever we start from
[10:24:17 -> 10:24:21]  in back proper ation and go for and go
[10:24:19 -> 10:24:23]  backward through the layers and then
[10:24:21 -> 10:24:25]  cache as well which is the which is the
[10:24:23 -> 10:24:28]  forward pass like intermediate cach
[10:24:25 -> 10:24:31]  stored values right um so we go into
[10:24:28 -> 10:24:34]  backward here and we see
[10:24:31 -> 10:24:37]  um we get in this grad output and the
[10:24:34 -> 10:24:40]  cache as we'd expect um and then we just
[10:24:37 -> 10:24:43]  lay out that Tuple so fc1 input so we
[10:24:40 -> 10:24:43]  just
[10:24:44 -> 10:24:52]  um fc1 input fc1 output uh value output
[10:24:49 -> 10:24:56]  right um so just kind of just unpacking
[10:24:52 -> 10:24:58]  this again um now we go to here and it's
[10:24:56 -> 10:25:02]  linear backward so if we step back to
[10:24:58 -> 10:25:04]  this linear backward one is going to um
[10:25:02 -> 10:25:06]  it's going to calculate both of these
[10:25:04 -> 10:25:10]  right so linear backward is a bit bigger
[10:25:06 -> 10:25:13]  actually um we go here taking a grad
[10:25:10 -> 10:25:14]  output select the output thing the input
[10:25:13 -> 10:25:17]  and then the weights right so we can
[10:25:14 -> 10:25:20]  calculate both the uh the the grad
[10:25:17 -> 10:25:25]  attribute for both the X and the W value
[10:25:20 -> 10:25:28]  right so in here we do um grad weights
[10:25:25 -> 10:25:30]  um is X is X transpose times the grad
[10:25:28 -> 10:25:33]  output and so if we go to here we can
[10:25:30 -> 10:25:34]  see uh X transpose and then times the
[10:25:33 -> 10:25:38]  grad output which in this case and the
[10:25:34 -> 10:25:40]  first layer is is derivative of the loss
[10:25:38 -> 10:25:43]  um and then in this dx2 for example we
[10:25:40 -> 10:25:44]  see the gr output times the transpose
[10:25:43 -> 10:25:47]  weight 2
[10:25:44 -> 10:25:50]  right so gr output times transpose
[10:25:47 -> 10:25:53]  weight two and then the bias
[10:25:50 -> 10:25:56]  um I'll break that down more so in in
[10:25:53 -> 10:25:59]  the C- section but um this is this is
[10:25:56 -> 10:26:02]  the grad bias right so um I can actually
[10:25:59 -> 10:26:05]  just like print that out let's let's pop
[10:26:02 -> 10:26:08]  into here really quick and just exit
[10:26:05 -> 10:26:15]  that um so how did this go again we do
[10:26:08 -> 10:26:20]  np. suum so I'll just do um NP just do I
[10:26:15 -> 10:26:23]  python import numpy
[10:26:20 -> 10:26:27]  as n p then we go x
[10:26:23 -> 10:26:31]  equals just do torch. and um we'll do 3
[10:26:27 -> 10:26:36]  by we'll say 2 by 4 right we print
[10:26:31 -> 10:26:38]  out Imports import torch then go back up
[10:26:36 -> 10:26:45]  print out X and we get this right so if
[10:26:38 -> 10:26:50]  we do um we print out torch. suum of X
[10:26:45 -> 10:26:53]  we do axis equal 0 and we go at keep
[10:26:50 -> 10:26:55]  dims I think it's keep is it keep dims
[10:26:53 -> 10:26:59]  How does it
[10:26:55 -> 10:27:01]  go keep dims equals
[10:26:59 -> 10:27:04]  true we can see that literally all this
[10:27:01 -> 10:27:07]  does is it is it mushes these together
[10:27:04 -> 10:27:11]  so it's going to go two + 1 and then
[10:27:07 -> 10:27:13]  this is like 21 plus that so it's 3.23
[10:27:11 -> 10:27:15]  and then so essentially like mushing
[10:27:13 -> 10:27:17]  knees moing knees mushing knes right
[10:27:15 -> 10:27:20]  right um and it's going to do this
[10:27:17 -> 10:27:22]  across the the across like this the
[10:27:20 -> 10:27:25]  horizontal right so it's going to Mush
[10:27:22 -> 10:27:27]  cross hor vertical sorry because that is
[10:27:25 -> 10:27:29]  the that is the zero axis right the the
[10:27:27 -> 10:27:31]  leading Dimension here so that's that's
[10:27:29 -> 10:27:33]  this vertical part so it's going to Mush
[10:27:31 -> 10:27:37]  vertically
[10:27:33 -> 10:27:39]  um and so that that's really all that is
[10:27:37 -> 10:27:41]  um so we're just combining things across
[10:27:39 -> 10:27:44]  the entire batch right but you'll see
[10:27:41 -> 10:27:47]  this more intuitively in the in the C
[10:27:44 -> 10:27:49]  version now we do the you know this as I
[10:27:47 -> 10:27:50]  as I mentioned before and we just
[10:27:49 -> 10:27:53]  essentially return those right for the
[10:27:50 -> 10:27:56]  linear backward
[10:27:53 -> 10:27:58]  layer we return all of our all of our
[10:27:56 -> 10:28:01]  gradients and then we perform a
[10:27:58 -> 10:28:03]  optimization step so we do the model.
[10:28:01 -> 10:28:05]  backward and we do model. update weights
[10:28:03 -> 10:28:08]  and we pass one 2 3 4 as well as the
[10:28:05 -> 10:28:10]  learning rate in and inside of update
[10:28:08 -> 10:28:12]  weights we'll see right here we
[10:28:10 -> 10:28:15]  literally just do self. weights self
[10:28:12 -> 10:28:19]  dobias weights and bias and we do minus
[10:28:15 -> 10:28:21]  equals the learning rate times the
[10:28:19 -> 10:28:23]  gradient right so as I was talking about
[10:28:21 -> 10:28:27]  before you're trying to reduce you're
[10:28:23 -> 10:28:29]  trying to essentially gr do gradient um
[10:28:27 -> 10:28:30]  gradient descent that's what this is
[10:28:29 -> 10:28:32]  just that the stochastic gradient
[10:28:30 -> 10:28:35]  descent because it's it's doing it
[10:28:32 -> 10:28:37]  constantly every single time and uh yeah
[10:28:35 -> 10:28:40]  that's that's how we update the network
[10:28:37 -> 10:28:42]  so you know if if if the gradient is if
[10:28:40 -> 10:28:45]  the gradient is really high that means
[10:28:42 -> 10:28:47]  there's a lot of error right so if we do
[10:28:45 -> 10:28:49]  um learning rate times something really
[10:28:47 -> 10:28:52]  high so a positive times a positive and
[10:28:49 -> 10:28:54]  then subtract that from here um that's
[10:28:52 -> 10:28:56]  going to mean it's contributing a lot
[10:28:54 -> 10:28:58]  and then it's going to mean it's it's
[10:28:56 -> 10:29:00]  initially contributing you know a lot of
[10:28:58 -> 10:29:02]  error and we want to reduce that we want
[10:29:00 -> 10:29:04]  to change it
[10:29:02 -> 10:29:06]  significantly um and then if it's like
[10:29:04 -> 10:29:07]  say lower we don't we don't we don't
[10:29:06 -> 10:29:09]  want to adjust that as much right so
[10:29:07 -> 10:29:10]  it's kind of just going to balance
[10:29:09 -> 10:29:13]  between in the middle whichever one
[10:29:10 -> 10:29:16]  gives us the most error right
[10:29:13 -> 10:29:18]  um and we do this we do the same idea
[10:29:16 -> 10:29:21]  for all of these and we just just do
[10:29:18 -> 10:29:23]  this essentially this scalar value
[10:29:21 -> 10:29:25]  multiplied by each thing in the entire
[10:29:23 -> 10:29:28]  weights and the bias Matrix we do that
[10:29:25 -> 10:29:30]  everywhere and that's pretty much it U
[10:29:28 -> 10:29:33]  we do that for every single every single
[10:29:30 -> 10:29:35]  iteration we do a we do a we get we get
[10:29:33 -> 10:29:37]  whatever inputs and output uh
[10:29:35 -> 10:29:39]  predictions we need we do a forward pass
[10:29:37 -> 10:29:42]  we do loss function derivative of the
[10:29:39 -> 10:29:44]  loss model. backward so backward pass
[10:29:42 -> 10:29:45]  update weights and then if we need to we
[10:29:44 -> 10:29:47]  just like print out the progress over
[10:29:45 -> 10:29:49]  time right so if we go ahead and run
[10:29:47 -> 10:29:49]  this
[10:29:50 -> 10:29:56]  um python C friendly we can go and see
[10:29:54 -> 10:29:59]  this is actually training quite well so
[10:29:56 -> 10:30:01]  we this is training quite fast as well
[10:29:59 -> 10:30:03]  you know numpy is bed to C which C is
[10:30:01 -> 10:30:05]  really fast we can see that you know
[10:30:03 -> 10:30:09]  over the first one over the first 7500
[10:30:05 -> 10:30:11]  iterations we get 93% accuracy so just
[10:30:09 -> 10:30:15]  to iterate a little more about this
[10:30:11 -> 10:30:18]  whole linear backward um np. suum cross
[10:30:15 -> 10:30:20]  axis zero where you take each column and
[10:30:18 -> 10:30:24]  you squash it together the reason why we
[10:30:20 -> 10:30:26]  do that is because each of those is like
[10:30:24 -> 10:30:30]  across the entire batch right so this
[10:30:26 -> 10:30:33]  would be like a single uh uh a single
[10:30:30 -> 10:30:35]  layer right single bunch of a bunch of
[10:30:33 -> 10:30:37]  biases for like all the neurons or
[10:30:35 -> 10:30:39]  whatever you want to say and what we're
[10:30:37 -> 10:30:42]  doing here is we're taking a single
[10:30:39 -> 10:30:44]  neuron and we're squashing everything
[10:30:42 -> 10:30:48]  together cuz like imagine if you have a
[10:30:44 -> 10:30:49]  really big um like a really sparse
[10:30:48 -> 10:30:51]  really big reward signal for a single
[10:30:49 -> 10:30:53]  example and then like you do 20 other
[10:30:51 -> 10:30:54]  ones and they have the complete opposite
[10:30:53 -> 10:30:56]  right the idea is to like kind of
[10:30:54 -> 10:30:58]  average all them together you're not
[10:30:56 -> 10:31:00]  you're not like divide you're not adding
[10:30:58 -> 10:31:02]  them all together and then dividing but
[10:31:00 -> 10:31:03]  you're you're just like accumulating all
[10:31:02 -> 10:31:05]  of them together so you end up with
[10:31:03 -> 10:31:07]  something that's like close and pushes
[10:31:05 -> 10:31:09]  in the direction of like where
[10:31:07 -> 10:31:11]  generalization should be so I know that
[10:31:09 -> 10:31:14]  sounds like really conceptually Advanced
[10:31:11 -> 10:31:16]  but it's not it's you're just trying to
[10:31:14 -> 10:31:18]  push in whatever way the average favors
[10:31:16 -> 10:31:20]  so that's why you do it AC cross the
[10:31:18 -> 10:31:22]  actual batch itself CU if you had just
[10:31:20 -> 10:31:24]  this Vector laid out I mean you could do
[10:31:22 -> 10:31:26]  that but training might not go as
[10:31:24 -> 10:31:28]  smoothly whereas if you were to just
[10:31:26 -> 10:31:30]  accumulate everything so you get like on
[10:31:28 -> 10:31:31]  average what is the best way to move
[10:31:30 -> 10:31:33]  what is the best way to move that bias
[10:31:31 -> 10:31:35]  value then it then it helps a little bit
[10:31:33 -> 10:31:38]  more so that's why we do that um but now
[10:31:35 -> 10:31:41]  let's get into C this is pretty much a
[10:31:38 -> 10:31:44]  port of just the last script that we ran
[10:31:41 -> 10:31:45]  so this v1c you'll find this in the
[10:31:44 -> 10:31:47]  naive CPU because this is a naive
[10:31:45 -> 10:31:49]  algorithms these aren't like really
[10:31:47 -> 10:31:51]  really fast are just like the easiest
[10:31:49 -> 10:31:54]  way to write them um very like intuitive
[10:31:51 -> 10:31:56]  to understand um but gives us a basis
[10:31:54 -> 10:32:00]  for how we can modify this and turn turn
[10:31:56 -> 10:32:01]  it to Cuda right um so inside of here we
[10:32:00 -> 10:32:05]  do the same
[10:32:01 -> 10:32:05]  idea we have a little neural network
[10:32:06 -> 10:32:10]  thing at the top I mean it should
[10:32:08 -> 10:32:13]  probably go from like top to bottom but
[10:32:10 -> 10:32:15]  uh yeah so inside of here learning rate
[10:32:13 -> 10:32:17]  same learning rate we have 10 hex which
[10:32:15 -> 10:32:18]  is a bit different I Chang batch size to
[10:32:17 -> 10:32:20]  four because have having it as 8 or 16
[10:32:18 -> 10:32:22]  or 32 just took a ridiculous amount of
[10:32:20 -> 10:32:25]  time to compute for through each layer
[10:32:22 -> 10:32:26]  so I set this a little lower to four
[10:32:25 -> 10:32:29]  input size Remains the Same it has to hi
[10:32:26 -> 10:32:31]  and size 256 output size is 10 train
[10:32:29 -> 10:32:35]  size 10,000 test size we're not going to
[10:32:31 -> 10:32:36]  really need this but 1,000 for that um
[10:32:35 -> 10:32:38]  and then we have this neural network
[10:32:36 -> 10:32:39]  struct right so we can't actually do a
[10:32:38 -> 10:32:41]  CL we can't do a class in C but we can
[10:32:39 -> 10:32:44]  do struct we don't we don't have like
[10:32:41 -> 10:32:46]  the the class and and objectoriented as
[10:32:44 -> 10:32:49]  aspect that we do in C++ right this is a
[10:32:46 -> 10:32:51]  functional functional language so we're
[10:32:49 -> 10:32:52]  only allowed to use strs and inside of
[10:32:51 -> 10:32:54]  here we just store a bunch of arrays so
[10:32:52 -> 10:32:56]  all the weights and biases and then the
[10:32:54 -> 10:32:59]  gradients for those right uh just to
[10:32:56 -> 10:33:02]  kind of Mark everything down easily and
[10:32:59 -> 10:33:06]  and use this very simple uh struct
[10:33:02 -> 10:33:08]  right now we have some functions for um
[10:33:06 -> 10:33:09]  for actually loading the data now I
[10:33:08 -> 10:33:12]  don't want you to worry too much about
[10:33:09 -> 10:33:13]  the loading data aspect um this part it
[10:33:12 -> 10:33:16]  kind of just depends on like which use
[10:33:13 -> 10:33:19]  case you have but in this case um I I
[10:33:16 -> 10:33:21]  run the uh down downloader script so
[10:33:19 -> 10:33:26]  this downloader script uh just saves
[10:33:21 -> 10:33:29]  everything to a binary file um and then
[10:33:26 -> 10:33:31]  inside of C we just write those back
[10:33:29 -> 10:33:33]  again so or sorry we we we read them we
[10:33:31 -> 10:33:36]  read from the binary so notice how we do
[10:33:33 -> 10:33:38]  like file open and then the file name
[10:33:36 -> 10:33:41]  and then a read binary um and then it
[10:33:38 -> 10:33:45]  just turns that into into a usful format
[10:33:41 -> 10:33:48]  right so uh yeah it's not not entirely
[10:33:45 -> 10:33:51]  uh too too crazy um we essentially just
[10:33:48 -> 10:33:54]  like directly read this into um into
[10:33:51 -> 10:33:56]  bytes and then we modify that as needed
[10:33:54 -> 10:33:58]  later on we do the same thing for labels
[10:33:56 -> 10:34:00]  so very simple uh data loading functions
[10:33:58 -> 10:34:02]  um not too crazy compared to the mest
[10:34:00 -> 10:34:05]  one but this is in C right so it's
[10:34:02 -> 10:34:08]  obviously going to be a bit different
[10:34:05 -> 10:34:09]  now we have this interesting thing here
[10:34:08 -> 10:34:12]  called initialize wage which I probably
[10:34:09 -> 10:34:15]  should have shown you back here in our
[10:34:12 -> 10:34:18]  um where did it go in our C friendly
[10:34:15 -> 10:34:20]  script so notice in here how we have
[10:34:18 -> 10:34:21]  multiple functions right we have r r
[10:34:20 -> 10:34:23]  derivative initialize weights
[10:34:21 -> 10:34:25]  initialized bias and then these other
[10:34:23 -> 10:34:28]  ones which I already went over um
[10:34:25 -> 10:34:31]  initialize initializing weights and
[10:34:28 -> 10:34:33]  biases are kind of simple all right they
[10:34:31 -> 10:34:36]  just follow specific guide so if we do
[10:34:33 -> 10:34:38]  start off with initialized bias it's
[10:34:36 -> 10:34:40]  literally just going to be um just a
[10:34:38 -> 10:34:41]  bunch of zeros right it's all it's going
[10:34:40 -> 10:34:42]  to be just need the bias is a bunch of
[10:34:41 -> 10:34:45]  zeros that's okay we can start and we
[10:34:42 -> 10:34:48]  can move up and down from there
[10:34:45 -> 10:34:50]  um but the weights cuz
[10:34:48 -> 10:34:51]  so so bias is bias is floating flowing
[10:34:50 -> 10:34:53]  from the previous layer so having it as
[10:34:51 -> 10:34:55]  a zero doesn't actually matter it
[10:34:53 -> 10:34:57]  doesn't affect anything the bias the the
[10:34:55 -> 10:34:59]  bias gradient are just flowing directly
[10:34:57 -> 10:35:02]  from the previous layer those unmodified
[10:34:59 -> 10:35:04]  the same gradients um but the weights
[10:35:02 -> 10:35:07]  themselves are a little different so how
[10:35:04 -> 10:35:09]  we initialize weights is I'm actually
[10:35:07 -> 10:35:14]  going to go over to here we going to
[10:35:09 -> 10:35:16]  search up P torch timing
[10:35:14 -> 10:35:19]  initialization and this is how you
[10:35:16 -> 10:35:23]  actually initialize um I'll just do I'll
[10:35:19 -> 10:35:26]  just do uh torch do uh nn.
[10:35:23 -> 10:35:28]  linear um we'll go to
[10:35:26 -> 10:35:34]  linear
[10:35:28 -> 10:35:34]  so in pytorch we do this
[10:35:34 -> 10:35:39]  um where is
[10:35:36 -> 10:35:43]  it yes
[10:35:39 -> 10:35:45]  so the biases are initialized um in this
[10:35:43 -> 10:35:47]  distribution which we we can we we can
[10:35:45 -> 10:35:51]  do we don't entirely have to worry about
[10:35:47 -> 10:35:54]  that it's not a big deal um but then the
[10:35:51 -> 10:35:56]  then the weights themselves these are
[10:35:54 -> 10:35:59]  nor these are initialized uh on this
[10:35:56 -> 10:36:02]  basis right so we have um of shape
[10:35:59 -> 10:36:06]  output features by input
[10:36:02 -> 10:36:09]  features and we make this from uh from
[10:36:06 -> 10:36:15]  negative square Ro TK of K to positive
[10:36:09 -> 10:36:19]  square root of K where K is
[10:36:15 -> 10:36:22]  um K is one over the input features so
[10:36:19 -> 10:36:25]  if we pop back to this we're doing a
[10:36:22 -> 10:36:27]  random normal distribution right and
[10:36:25 -> 10:36:29]  with each of these numbers we have to we
[10:36:27 -> 10:36:31]  have to Clump them to this range right
[10:36:29 -> 10:36:33]  so these values are these values are in
[10:36:31 -> 10:36:38]  some normally distributed range and all
[10:36:33 -> 10:36:38]  we have to do there is
[10:36:39 -> 10:36:47]  um we do we we initialize to this right
[10:36:42 -> 10:36:47]  so K in this case
[10:36:49 -> 10:36:54]  okay so if we actually go to the H
[10:36:51 -> 10:36:56]  ination paper he init
[10:36:54 -> 10:36:59]  paper
[10:36:56 -> 10:37:02]  um this is I think this is
[10:36:59 -> 10:37:04]  it climing hey so climing and hay
[10:37:02 -> 10:37:07]  climing and it hey and it are the are
[10:37:04 -> 10:37:10]  the same thing um but if we go into here
[10:37:07 -> 10:37:14]  we go two ided by no maybe it's not
[10:37:10 -> 10:37:19]  there formula is somewhere in here
[10:37:14 -> 10:37:19]  um where did it go
[10:37:27 -> 10:37:33]  I search up
[10:37:30 -> 10:37:33]  um
[10:37:44 -> 10:37:51]  Rue initialization
[10:37:47 -> 10:37:51]  maybe that's a better term to look
[10:38:02 -> 10:38:07]  for
[10:38:05 -> 10:38:11]  yes this right
[10:38:07 -> 10:38:13]  here so this leads to a zero mean G CH
[10:38:11 -> 10:38:16]  distribution whose standard deviation is
[10:38:13 -> 10:38:21]  square < TK of two over um over this
[10:38:16 -> 10:38:22]  term and this term is the um I can't
[10:38:21 -> 10:38:24]  remember exactly what this is but I
[10:38:22 -> 10:38:27]  think this is length
[10:38:24 -> 10:38:29]  so we have an input size here which you
[10:38:27 -> 10:38:31]  could say as the length I don't know if
[10:38:29 -> 10:38:33]  that's specifically what L ties to but
[10:38:31 -> 10:38:35]  we'll just hold that assumption for now
[10:38:33 -> 10:38:37]  that that's that's the idea there is you
[10:38:35 -> 10:38:41]  would have
[10:38:37 -> 10:38:45]  um standard deviation is
[10:38:41 -> 10:38:46]  this um and if we continue to go forward
[10:38:45 -> 10:38:48]  maybe we might find something else here
[10:38:46 -> 10:38:51]  too
[10:38:48 -> 10:38:51]  um if we
[10:38:51 -> 10:38:57]  continue yeah so some
[10:38:54 -> 10:39:00]  layers other solution is to small Factor
[10:38:57 -> 10:39:00]  on the weights
[10:39:00 -> 10:39:03]  right
[10:39:03 -> 10:39:06]  anyways
[10:39:09 -> 10:39:14]  uh yeah this is pretty much the the
[10:39:12 -> 10:39:17]  inspiration from it so just looking at
[10:39:14 -> 10:39:20]  like kind of the purpose of this um
[10:39:17 -> 10:39:24]  this this specific initialization as
[10:39:20 -> 10:39:25]  compared to the P torch one um which I
[10:39:24 -> 10:39:27]  probably should have looked into
[10:39:25 -> 10:39:31]  beforehand the pytorch one is a little
[10:39:27 -> 10:39:33]  different um but the hay initialization
[10:39:31 -> 10:39:38]  is designed to work well with railu so
[10:39:33 -> 10:39:41]  it uses Square < TK of two / um input
[10:39:38 -> 10:39:43]  size as a standard deviation for the
[10:39:41 -> 10:39:45]  distribution that it's generated on
[10:39:43 -> 10:39:47]  right
[10:39:45 -> 10:39:50]  um it essentially counts out for the
[10:39:47 -> 10:39:52]  rause activation to zero out negative
[10:39:50 -> 10:39:54]  values so you might have these um
[10:39:52 -> 10:39:56]  so-called dead neurons that come up when
[10:39:54 -> 10:39:58]  you have like the Rue that just zeros
[10:39:56 -> 10:40:00]  something out and then when you try to
[10:39:58 -> 10:40:01]  uh like multiply that by something it
[10:40:00 -> 10:40:03]  ends up just like zeroing it out and you
[10:40:01 -> 10:40:05]  might like end up through the training
[10:40:03 -> 10:40:06]  process with like a row of zeros that
[10:40:05 -> 10:40:08]  just don't do anything and they're like
[10:40:06 -> 10:40:10]  useless and so you're not actually
[10:40:08 -> 10:40:12]  compressing information down into those
[10:40:10 -> 10:40:13]  because they're just zero so this helps
[10:40:12 -> 10:40:18]  deal with
[10:40:13 -> 10:40:19]  that now jumping back to C
[10:40:18 -> 10:40:22]  script
[10:40:19 -> 10:40:24]  um this is what we're doing here so we
[10:40:22 -> 10:40:26]  essentially have this we're we're just
[10:40:24 -> 10:40:28]  we just have to use like what we're what
[10:40:26 -> 10:40:31]  we're defaulted to with C we get this
[10:40:28 -> 10:40:34]  weights with the size that it's in we
[10:40:31 -> 10:40:36]  make this scale so square root function
[10:40:34 -> 10:40:38]  you know like we were doing before um
[10:40:36 -> 10:40:43]  square otk of two divided by size in
[10:40:38 -> 10:40:45]  this case um which size would be um you
[10:40:43 -> 10:40:46]  know size may not be appropriate I just
[10:40:45 -> 10:40:49]  kind of found this to work and and
[10:40:46 -> 10:40:51]  training exceptionally well with this so
[10:40:49 -> 10:40:53]  we're going to stick with that um but
[10:40:51 -> 10:40:55]  the size we're going to iterate through
[10:40:53 -> 10:40:57]  this and essentially for each weight
[10:40:55 -> 10:41:00]  value we're going to generate a value
[10:40:57 -> 10:41:03]  between Rand so Rand is going to be
[10:41:00 -> 10:41:06]  anywhere between zero and Rand Max so
[10:41:03 -> 10:41:09]  two one whatever this is so essentially
[10:41:06 -> 10:41:10]  this this in here is going to be zero
[10:41:09 -> 10:41:12]  between that Max number it's going to
[10:41:10 -> 10:41:14]  this is going to simplify to between a
[10:41:12 -> 10:41:17]  value between 0 and one decimal floating
[10:41:14 -> 10:41:19]  Point 32 number between 0 and one we're
[10:41:17 -> 10:41:22]  going to multiply this by the
[10:41:19 -> 10:41:25]  scale um which is which is going to be
[10:41:22 -> 10:41:29]  that and then we're going to subtract it
[10:41:25 -> 10:41:31]  by the um by the scale divided by uh
[10:41:29 -> 10:41:33]  divided by
[10:41:31 -> 10:41:36]  two and this is just going to give us a
[10:41:33 -> 10:41:38]  nice normal distribution for uh for our
[10:41:36 -> 10:41:39]  weights right this is going to do
[10:41:38 -> 10:41:41]  essentially the same job as we were
[10:41:39 -> 10:41:43]  doing
[10:41:41 -> 10:41:46]  before bias initialize all these to zero
[10:41:43 -> 10:41:47]  as we were doing before Ru is also very
[10:41:46 -> 10:41:49]  simple
[10:41:47 -> 10:41:52]  um the softmax I mean I think I showed
[10:41:49 -> 10:41:55]  you the softmax in the Triton section so
[10:41:52 -> 10:41:57]  um this is yeah this should be fairly
[10:41:55 -> 10:41:59]  intuitive we we get this you know we get
[10:41:57 -> 10:42:01]  this max value right and then when we're
[10:41:59 -> 10:42:03]  actually doing the exponentiation we we
[10:42:01 -> 10:42:06]  subtract the max value so we get still
[10:42:03 -> 10:42:07]  remain with numerical stability um not
[10:42:06 -> 10:42:10]  having that would just like give these
[10:42:07 -> 10:42:12]  crazy you know e to the whatever super
[10:42:10 -> 10:42:15]  crazy numbers when we have um ridiculous
[10:42:12 -> 10:42:16]  arrays um with like you know th negative
[10:42:15 -> 10:42:18]  a th000 negative 10 it's just it just
[10:42:16 -> 10:42:20]  gets out of hand right so we want we
[10:42:18 -> 10:42:22]  want Max to normalize that and just get
[10:42:20 -> 10:42:24]  rid of any of those
[10:42:22 -> 10:42:26]  instability
[10:42:24 -> 10:42:28]  um and then we yeah we just compute the
[10:42:26 -> 10:42:31]  soft Max right this is this isn't too
[10:42:28 -> 10:42:33]  bad same function we looked at before we
[10:42:31 -> 10:42:35]  have a matte mole so I specifically
[10:42:33 -> 10:42:37]  worded these so that it would make make
[10:42:35 -> 10:42:39]  the most sense so it's a map mole but
[10:42:37 -> 10:42:41]  this treats it as like you're taking in
[10:42:39 -> 10:42:45]  an array a and you Matrix multiply that
[10:42:41 -> 10:42:48]  with B right so say for example 2x4 is a
[10:42:45 -> 10:42:50]  and 4x three and so you'd end up with a
[10:42:48 -> 10:42:52]  2x3 and it would organize that in row
[10:42:50 -> 10:42:54]  major order right made these as simple
[10:42:52 -> 10:42:56]  as possible um you can dissect these but
[10:42:54 -> 10:42:57]  we already did a ton of stuff on matal
[10:42:56 -> 10:43:02]  so don't I'm not going to go over this
[10:42:57 -> 10:43:05]  for the like 20th time um then we have a
[10:43:02 -> 10:43:08]  a * B transposed so it's going to take
[10:43:05 -> 10:43:13]  in B is like say it's like a is 2x4 and
[10:43:08 -> 10:43:14]  then B is a um 3x4 and so it's going to
[10:43:13 -> 10:43:17]  uh do this operation as if it's
[10:43:14 -> 10:43:21]  transposing B to a 4x3 so end up with a
[10:43:17 -> 10:43:24]  2x3 right um and then same idea for this
[10:43:21 -> 10:43:29]  one if you end up with a 4X two and then
[10:43:24 -> 10:43:33]  a four four 4X two as as a and then 4 by
[10:43:29 -> 10:43:34]  uh 3 as B it's going to transpose a and
[10:43:33 -> 10:43:35]  it's going to make it a 2x4 and they're
[10:43:34 -> 10:43:37]  going to match up you're going to get
[10:43:35 -> 10:43:41]  two and three right so that that's just
[10:43:37 -> 10:43:45]  kind of the idea there and then we do um
[10:43:41 -> 10:43:48]  the the ru forward I probably wrote an
[10:43:45 -> 10:43:50]  additional um might have
[10:43:48 -> 10:43:53]  wrote yeah so this is like designed to
[10:43:50 -> 10:43:55]  work in batches um I might have probably
[10:43:53 -> 10:43:59]  written an additional one just like
[10:43:55 -> 10:43:59]  accidentally which I'll probably remove
[10:44:04 -> 10:44:12]  um oh R you there yeah so I should
[10:44:09 -> 10:44:13]  probably remove this actually but um
[10:44:12 -> 10:44:14]  well we'll I'll worry about that later
[10:44:13 -> 10:44:17]  and this will be updated by the time
[10:44:14 -> 10:44:18]  you're working on it um and then we just
[10:44:17 -> 10:44:22]  just have the bias forwarded which is
[10:44:18 -> 10:44:23]  going to you know add the bias um so
[10:44:22 -> 10:44:26]  literally what this is doing is it's
[10:44:23 -> 10:44:29]  iterating through uh it's it's going
[10:44:26 -> 10:44:31]  through batch size right and then we
[10:44:29 -> 10:44:33]  iterate through the actual size itself
[10:44:31 -> 10:44:37]  which is
[10:44:33 -> 10:44:39]  um which is the actual uh like the the
[10:44:37 -> 10:44:41]  the row length so it's going to go skip
[10:44:39 -> 10:44:43]  over as many number batch elements as it
[10:44:41 -> 10:44:44]  needs to so it's going to skip it's
[10:44:43 -> 10:44:46]  going to stride
[10:44:44 -> 10:44:48]  over then it's going to add this ey
[10:44:46 -> 10:44:52]  offset to it and it's just going to plus
[10:44:48 -> 10:44:54]  equals bias at that that value right so
[10:44:52 -> 10:44:59]  we we essentially just have this this
[10:44:54 -> 10:45:02]  row of biases and it's going to um just
[10:44:59 -> 10:45:04]  essentially add each of those it's for
[10:45:02 -> 10:45:07]  for like a given batch element for like
[10:45:04 -> 10:45:09]  batch element one um it's going to just
[10:45:07 -> 10:45:10]  add you know say it's like 10 values
[10:45:09 -> 10:45:12]  here it's going to add all 10 to the
[10:45:10 -> 10:45:13]  bias values and it's going to go down
[10:45:12 -> 10:45:14]  it's going to add those same values
[10:45:13 -> 10:45:16]  again it's just applying the same bias
[10:45:14 -> 10:45:18]  to each to each row right that's what
[10:45:16 -> 10:45:18]  this is doing
[10:45:19 -> 10:45:22]  scrolling down further I'm not going to
[10:45:20 -> 10:45:24]  go into these quite yet because there's
[10:45:22 -> 10:45:27]  like more happening but scroll down
[10:45:24 -> 10:45:31]  to um the
[10:45:27 -> 10:45:33]  actual uh train function and the where
[10:45:31 -> 10:45:36]  is
[10:45:33 -> 10:45:40]  this the in main function so in here we
[10:45:36 -> 10:45:42]  have a a pseudo random number generator
[10:45:40 -> 10:45:44]  um these are pseudo random they're not
[10:45:42 -> 10:45:46]  you can actually have completely random
[10:45:44 -> 10:45:47]  numbers that's like a very hard uh you
[10:45:46 -> 10:45:49]  know cryptographic graic problem and
[10:45:47 -> 10:45:51]  everything that's like something I'm not
[10:45:49 -> 10:45:54]  going to go into um you know in this
[10:45:51 -> 10:45:56]  case we're using srand to to to Generate
[10:45:54 -> 10:45:58]  random numbers but in Cuda you can use Q
[10:45:56 -> 10:46:00]  Rand so it's going to Generate random
[10:45:58 -> 10:46:01]  numbers in parallel really fast so you
[10:46:00 -> 10:46:03]  don't have to like wait for the CPU to
[10:46:01 -> 10:46:05]  do this one and this one then this one
[10:46:03 -> 10:46:07]  right it's kind of faster um we
[10:46:05 -> 10:46:10]  initialize this neural network class
[10:46:07 -> 10:46:12]  with NN or sorry struct struct got to
[10:46:10 -> 10:46:15]  use the politically cor correct terms we
[10:46:12 -> 10:46:18]  initialize the neuronet so we go here
[10:46:15 -> 10:46:21]  and we just have this NN and then the
[10:46:18 -> 10:46:24]  weight attribute right uh equals then we
[10:46:21 -> 10:46:26]  do Malik just a regular C Malik so
[10:46:24 -> 10:46:28]  weights one is going to be um hidden
[10:46:26 -> 10:46:30]  size by input size right so that's the
[10:46:28 -> 10:46:33]  256 by
[10:46:30 -> 10:46:36]  784 and then the weights two which is
[10:46:33 -> 10:46:39]  output size by hidden size so um this is
[10:46:36 -> 10:46:42]  going to be
[10:46:39 -> 10:46:47]  um what's it called 256x 10 so it's
[10:46:42 -> 10:46:48]  going to take the BX 256 and then 2 6 x
[10:46:47 -> 10:46:50]  10 and it's going to multiply those and
[10:46:48 -> 10:46:53]  it's going to get a B by10 output right
[10:46:50 -> 10:46:56]  for the after the weights um you know
[10:46:53 -> 10:46:58]  bias bias one is hidden size just adding
[10:46:56 -> 10:47:01]  again to that to that output of all of
[10:46:58 -> 10:47:02]  those neurons each value for each neuron
[10:47:01 -> 10:47:06]  IUS 2 same
[10:47:02 -> 10:47:09]  idea the grad weight so just this but
[10:47:06 -> 10:47:10]  the it's just a different uh it's just a
[10:47:09 -> 10:47:13]  different variable right so we're
[10:47:10 -> 10:47:14]  storing the gradients of those the error
[10:47:13 -> 10:47:17]  um and they're just going to be the same
[10:47:14 -> 10:47:19]  shape right
[10:47:17 -> 10:47:21]  um and then we initialize so we have
[10:47:19 -> 10:47:23]  initialized weight and initialize bias
[10:47:21 -> 10:47:25]  now this is going back to um the the
[10:47:23 -> 10:47:27]  climing init that we did the the hay
[10:47:25 -> 10:47:30]  initialization and the initialized bias
[10:47:27 -> 10:47:33]  that we did before
[10:47:30 -> 10:47:36]  um now now that we've initialized those
[10:47:33 -> 10:47:40]  with random values we go into here so
[10:47:36 -> 10:47:43]  our X train is going to be the train
[10:47:40 -> 10:47:46]  size so train size in this case is uh
[10:47:43 -> 10:47:46]  you know 10,000
[10:47:48 -> 10:47:53]  let me go down a little bit more train
[10:47:49 -> 10:47:55]  size times input size so an image is
[10:47:53 -> 10:47:57]  784 um you know flattened and then we
[10:47:55 -> 10:47:58]  have the train size which in this case
[10:47:57 -> 10:48:01]  is
[10:47:58 -> 10:48:04]  10,000 um the Y train is just going to
[10:48:01 -> 10:48:05]  be a bunch of integers that are that
[10:48:04 -> 10:48:08]  span this so we don't we don't actually
[10:48:05 -> 10:48:11]  need 784 there's only one integer value
[10:48:08 -> 10:48:13]  per sample um which is the label and
[10:48:11 -> 10:48:15]  then we have the same for for the for
[10:48:13 -> 10:48:18]  the test set
[10:48:15 -> 10:48:20]  right we load these in using the
[10:48:18 -> 10:48:24]  previous um loading loading scripts that
[10:48:20 -> 10:48:25]  I showed you before we can print the
[10:48:24 -> 10:48:27]  first image in the terminal so this
[10:48:25 -> 10:48:30]  going just going to print things out
[10:48:27 -> 10:48:32]  using the X thing right um so if I I'm
[10:48:30 -> 10:48:33]  going to compile this later and you'll
[10:48:32 -> 10:48:36]  kind of see what I mean but it just kind
[10:48:33 -> 10:48:38]  of shows us um like how good our our
[10:48:36 -> 10:48:40]  actual predictions are going to be um so
[10:48:38 -> 10:48:42]  we can actually like look at look at an
[10:48:40 -> 10:48:44]  image in the terminal and see okay what
[10:48:42 -> 10:48:45]  did it think this was what was the
[10:48:44 -> 10:48:46]  actual label right so we can kind of
[10:48:45 -> 10:48:48]  like look and sort of match things up in
[10:48:46 -> 10:48:50]  our own head I don't want to use like
[10:48:48 -> 10:48:52]  open CV or a custom extension to put a
[10:48:50 -> 10:48:54]  window cuz that's just a bunch of extra
[10:48:52 -> 10:48:56]  work it's easier to do it in the
[10:48:54 -> 10:48:58]  terminal um and then just the training
[10:48:56 -> 10:49:00]  labels for those as well right and then
[10:48:58 -> 10:49:02]  we go through and we do the train
[10:49:00 -> 10:49:04]  function which is comprehensive and then
[10:49:02 -> 10:49:07]  we free everything up we don't do Cuda
[10:49:04 -> 10:49:10]  free this just C we just do free and it
[10:49:07 -> 10:49:12]  get gets rid of the weights uh biases
[10:49:10 -> 10:49:14]  all the gradients for those the train
[10:49:12 -> 10:49:18]  set and the test set
[10:49:14 -> 10:49:19]  right now we go into TR inside of here
[10:49:18 -> 10:49:22]  there's a bunch of things happening all
[10:49:19 -> 10:49:23]  right so if I click this can see where
[10:49:22 -> 10:49:26]  the end
[10:49:23 -> 10:49:28]  is we have this we have this um this
[10:49:26 -> 10:49:32]  hidden right here so that's going to be
[10:49:28 -> 10:49:36]  you know the B by 256 as we looked at in
[10:49:32 -> 10:49:38]  um the B
[10:49:36 -> 10:49:42]  by where was
[10:49:38 -> 10:49:44]  it batch size by hidden size so that's
[10:49:42 -> 10:49:47]  like going to be the actual hidden layer
[10:49:44 -> 10:49:49]  output right so so here we get a B by
[10:49:47 -> 10:49:51]  256 that's the hidden layer it's the
[10:49:49 -> 10:49:53]  that's the first you know mmal output
[10:49:51 -> 10:49:55]  essentially and then we get the output
[10:49:53 -> 10:49:58]  which is batch size by output size which
[10:49:55 -> 10:50:01]  in this case is p
[10:49:58 -> 10:50:02]  by10 and then we do num batches so the
[10:50:01 -> 10:50:04]  number of batches we're actually going
[10:50:02 -> 10:50:07]  to do is train size divided by batch
[10:50:04 -> 10:50:09]  size and the reason we do this is
[10:50:07 -> 10:50:10]  because we don't want to just like
[10:50:09 -> 10:50:12]  offset each time we don't want to give
[10:50:10 -> 10:50:15]  it the same data each sample so if we
[10:50:12 -> 10:50:17]  take that total 60,000 and divide that
[10:50:15 -> 10:50:19]  by batch size meaning like four
[10:50:17 -> 10:50:21]  uh or sorry train size is 10,000 and
[10:50:19 -> 10:50:26]  then batch size is four we're going to
[10:50:21 -> 10:50:28]  get 2,500 total um total batches each
[10:50:26 -> 10:50:30]  with four images in them so we're going
[10:50:28 -> 10:50:32]  to do four and then four and then four
[10:50:30 -> 10:50:35]  and then four this way we don't like
[10:50:32 -> 10:50:37]  overlap right containing the same images
[10:50:35 -> 10:50:40]  in adjacent batches we just kind of give
[10:50:37 -> 10:50:42]  it new data every time and we do Epoch
[10:50:40 -> 10:50:44]  over that right so we do all of the
[10:50:42 -> 10:50:45]  batches and then we do another Epoch
[10:50:44 -> 10:50:47]  over that so what's going to happen it's
[10:50:45 -> 10:50:49]  it's going to it's going to do like up
[10:50:47 -> 10:50:52]  to 50% in the first Epoch or some some
[10:50:49 -> 10:50:54]  some number like that 50% and then it's
[10:50:52 -> 10:50:55]  going to start the next Epoch and it's
[10:50:54 -> 10:50:56]  going to learn from all the examples
[10:50:55 -> 10:50:58]  that it had previously and it's going to
[10:50:56 -> 10:51:00]  be like oh we saw those again we know
[10:50:58 -> 10:51:02]  exactly what to do there so it's going
[10:51:00 -> 10:51:04]  to it's going to get you know accuracy
[10:51:02 -> 10:51:05]  is going to like it's the loss is going
[10:51:04 -> 10:51:07]  to look like this it's going to figure
[10:51:05 -> 10:51:09]  start figuring everything out tuning
[10:51:07 -> 10:51:10]  whatever it can get its hands on and
[10:51:09 -> 10:51:12]  it's going to drop because it figures
[10:51:10 -> 10:51:14]  out what to optimize then it's going to
[10:51:12 -> 10:51:16]  Plateau through that Epoch it's going to
[10:51:14 -> 10:51:18]  sort of plateau out and then the next
[10:51:16 -> 10:51:20]  next Epoch starts and it's going to it's
[10:51:18 -> 10:51:22]  going to drop again because it's because
[10:51:20 -> 10:51:25]  it's seen those again and it can
[10:51:22 -> 10:51:27]  optimize for more it can compress more
[10:51:25 -> 10:51:28]  features into that because it's seen
[10:51:27 -> 10:51:30]  that already um so it's going to you
[10:51:28 -> 10:51:32]  know continue dropping again and then
[10:51:30 -> 10:51:35]  it's going to sort of plateau and then
[10:51:32 -> 10:51:36]  it's going to drop again and and then
[10:51:35 -> 10:51:38]  we're just going to end up at some place
[10:51:36 -> 10:51:41]  when whenever all the epoch are done
[10:51:38 -> 10:51:42]  right so we iterate through Epoch now
[10:51:41 -> 10:51:46]  this is
[10:51:42 -> 10:51:49]  um if I look at this actually um this
[10:51:46 -> 10:51:50]  finish is here so inside of a single
[10:51:49 -> 10:51:52]  Epoch this is where most of the work is
[10:51:50 -> 10:51:54]  done right we just do like free hidden
[10:51:52 -> 10:51:56]  and output so most of the work is
[10:51:54 -> 10:51:59]  actually done in the epox loop inside of
[10:51:56 -> 10:52:01]  here we do a total loss which we
[10:51:59 -> 10:52:03]  initialize to zero the number of correct
[10:52:01 -> 10:52:05]  answers so we also set that to zero this
[10:52:03 -> 10:52:07]  is just for tracking the accuracy so we
[10:52:05 -> 10:52:10]  can see the loss dropping versus what
[10:52:07 -> 10:52:12]  the percent accuracy is over the over
[10:52:10 -> 10:52:14]  the U over the the training samples
[10:52:12 -> 10:52:18]  right so each training step we can see
[10:52:14 -> 10:52:21]  which or every sorry every um every
[10:52:18 -> 10:52:24]  thousand every 1,000 or or every 100
[10:52:21 -> 10:52:27]  training steps we can see what the
[10:52:24 -> 10:52:29]  accuracy is over the
[10:52:27 -> 10:52:33]  batches then inside of here we iterate
[10:52:29 -> 10:52:38]  over num batches increasing by um by
[10:52:33 -> 10:52:38]  this each time right um batch batch
[10:52:40 -> 10:52:46]  Plus+ and
[10:52:43 -> 10:52:49]  then we use start idx this is going to
[10:52:46 -> 10:52:52]  be batch times batch times batch size we
[10:52:49 -> 10:52:55]  do our forward so we get essentially
[10:52:52 -> 10:52:56]  inside of here we pass in our neural net
[10:52:55 -> 10:53:00]  we passing our like our neural net
[10:52:56 -> 10:53:02]  struct pointer um an input so that's
[10:53:00 -> 10:53:05]  going to be train and it's going to be
[10:53:02 -> 10:53:08]  the um the start
[10:53:05 -> 10:53:12]  idx so whichever whichever this is um
[10:53:08 -> 10:53:15]  whichever actual batch it is um times
[10:53:12 -> 10:53:17]  the times the batch size so it's going
[10:53:15 -> 10:53:19]  to skip in increments a batch size right
[10:53:17 -> 10:53:19]  so it's going
[10:53:22 -> 10:53:27]  to it's going to instead of uh skipping
[10:53:26 -> 10:53:31]  like this isn't going to actually like
[10:53:27 -> 10:53:33]  plus equals um like batch itself this is
[10:53:31 -> 10:53:34]  going to add one each time so this is
[10:53:33 -> 10:53:37]  just going to act as like an increment
[10:53:34 -> 10:53:38]  so we're jumping right that's what
[10:53:37 -> 10:53:39]  that's what that's doing is it's going
[10:53:38 -> 10:53:41]  to jump four at a time instead of just
[10:53:39 -> 10:53:44]  one we just plus
[10:53:41 -> 10:53:47]  plus uh and
[10:53:44 -> 10:53:50]  then we pass in the hidden layer
[10:53:47 -> 10:53:52]  we pass in the output and then the batch
[10:53:50 -> 10:53:57]  size as well right so all of the inputs
[10:53:52 -> 10:53:58]  um hidden output size right um and this
[10:53:57 -> 10:54:01]  is just going to do our forward pass all
[10:53:58 -> 10:54:03]  the way from uh you know taking this
[10:54:01 -> 10:54:04]  flattened image which we've done already
[10:54:03 -> 10:54:07]  I'm just because it's just laid out in
[10:54:04 -> 10:54:09]  binary it just like exists as that you
[10:54:07 -> 10:54:10]  can reformat it and interpret it as
[10:54:09 -> 10:54:13]  whatever you want but in see in memory
[10:54:10 -> 10:54:15]  it's actually laid out as literally one
[10:54:13 -> 10:54:17]  through zero you know Z through 784 or
[10:54:15 -> 10:54:21]  whatever um so it's like not that hard
[10:54:17 -> 10:54:21]  to actually like mess around
[10:54:21 -> 10:54:29]  with we do the forward pass we calculate
[10:54:24 -> 10:54:31]  our cross entropy loss right using the
[10:54:29 -> 10:54:32]  uh same cross entropy loss idea that we
[10:54:31 -> 10:54:36]  did in the C friendly script so we're
[10:54:32 -> 10:54:38]  just porting that over to C um which if
[10:54:36 -> 10:54:39]  there's like if if if this doesn't like
[10:54:38 -> 10:54:40]  make sense then you can actually go in
[10:54:39 -> 10:54:42]  and you can see okay well what are we
[10:54:40 -> 10:54:44]  doing here versus here right you have
[10:54:42 -> 10:54:45]  tools like language models and and the
[10:54:44 -> 10:54:47]  internet which you can investigate these
[10:54:45 -> 10:54:50]  things through through and you can kind
[10:54:47 -> 10:54:56]  of see what's happening um yeah so we
[10:54:50 -> 10:54:58]  calculate our loss we add um we add the
[10:54:56 -> 10:55:01]  uh we add that loss to the total loss so
[10:54:58 -> 10:55:03]  inside of that actual Epoch we see okay
[10:55:01 -> 10:55:05]  well what was the uh what was the
[10:55:03 -> 10:55:08]  average loss right um you know in here
[10:55:05 -> 10:55:10]  we do total loss divided by you know
[10:55:08 -> 10:55:12]  numb batches so we kind of we kind of
[10:55:10 -> 10:55:12]  average that
[10:55:13 -> 10:55:19]  out um
[10:55:17 -> 10:55:20]  we do that every single Epoch and then
[10:55:19 -> 10:55:24]  inside of
[10:55:20 -> 10:55:27]  here we simply just this just acts as
[10:55:24 -> 10:55:30]  little increment for the correct counter
[10:55:27 -> 10:55:34]  so this is just going to see okay well
[10:55:30 -> 10:55:34]  um were we close or not um
[10:55:35 -> 10:55:41]  so this should be self-explanatory um
[10:55:39 -> 10:55:42]  this is also just like not necessarily
[10:55:41 -> 10:55:44]  part of the training run but just like
[10:55:42 -> 10:55:46]  an extra feature that you can use to
[10:55:44 -> 10:55:48]  print out uh what the accuracy was over
[10:55:46 -> 10:55:49]  time um not the loss but the actual
[10:55:48 -> 10:55:52]  percent
[10:55:49 -> 10:55:56]  accuracy the backward function so this
[10:55:52 -> 10:56:00]  takes in you know NN and it takes a uh
[10:55:56 -> 10:56:02]  pointer to uh to the input right so this
[10:56:00 -> 10:56:06]  is this is a memory address to this at
[10:56:02 -> 10:56:09]  this index which is going to be um
[10:56:06 -> 10:56:11]  starting index times input size right
[10:56:09 -> 10:56:14]  and then inside of here we pass in a few
[10:56:11 -> 10:56:16]  things meaning hidden um you know this
[10:56:14 -> 10:56:17]  neural net is going to contain all of
[10:56:16 -> 10:56:19]  our all of our weights and stuff so
[10:56:17 -> 10:56:23]  don't this is like all contained within
[10:56:19 -> 10:56:27]  that uh the input itself
[10:56:23 -> 10:56:28]  um hidden output labels and batch size
[10:56:27 -> 10:56:31]  so very similar structure to the forward
[10:56:28 -> 10:56:34]  pass except we also include uh we also
[10:56:31 -> 10:56:38]  include labels
[10:56:34 -> 10:56:40]  right update weights everything is now
[10:56:38 -> 10:56:43]  updated after that and then we can print
[10:56:40 -> 10:56:44]  out some useful stuff okay and that's
[10:56:43 -> 10:56:45]  pretty much all that happens in this
[10:56:44 -> 10:56:49]  training Loop a lot of it is just like
[10:56:45 -> 10:56:52]  print and keeping track of stuff um but
[10:56:49 -> 10:56:56]  yeah so going
[10:56:52 -> 10:56:57]  up if we actually look at our uh if we
[10:56:56 -> 10:57:00]  actually look at
[10:56:57 -> 10:57:02]  our let me jump up to the forward path
[10:57:00 -> 10:57:06]  where did this
[10:57:02 -> 10:57:09]  go okay awesome so inside the forward um
[10:57:06 -> 10:57:14]  we it's very simple right 1 2 3 four
[10:57:09 -> 10:57:16]  five six um not too bad now inside of
[10:57:14 -> 10:57:18]  here I added the extra sof Max just
[10:57:16 -> 10:57:19]  because I didn't want to be redundant
[10:57:18 -> 10:57:21]  and included in the whole like training
[10:57:19 -> 10:57:23]  Loop thing there was a lot happening in
[10:57:21 -> 10:57:24]  there it was it was quite you
[10:57:23 -> 10:57:27]  complicated to sort through
[10:57:24 -> 10:57:28]  everything but yeah it's it's really
[10:57:27 -> 10:57:30]  helpful when you break things up into
[10:57:28 -> 10:57:31]  smaller chunks this is super manageable
[10:57:30 -> 10:57:34]  I wanted to make the forward and
[10:57:31 -> 10:57:35]  backward pass as as modular as possible
[10:57:34 -> 10:57:37]  so that you guys could like really
[10:57:35 -> 10:57:40]  performance optimize it if you wanted to
[10:57:37 -> 10:57:42]  like on the side um but this is like
[10:57:40 -> 10:57:44]  literally identical to what we did in or
[10:57:42 -> 10:57:46]  almost identical to what we did in the C
[10:57:44 -> 10:57:48]  friendly function or the C friendly
[10:57:46 -> 10:57:50]  script right so in there we had this
[10:57:48 -> 10:57:54]  like linear forward method which would
[10:57:50 -> 10:57:56]  do the M mole and the bias um and then
[10:57:54 -> 10:57:59]  the linear backward which would do you
[10:57:56 -> 10:58:02]  know two mmoles and and uh and and a
[10:57:59 -> 10:58:03]  bias backward um but in this one we kind
[10:58:02 -> 10:58:05]  of just split it into easier more
[10:58:03 -> 10:58:08]  manageable chunks so a map mole
[10:58:05 -> 10:58:10]  specifically uh is like an operation
[10:58:08 -> 10:58:12]  that you can optimize on its own and so
[10:58:10 -> 10:58:14]  like optimizing a linear forward or or
[10:58:12 -> 10:58:15]  even more generally linear backward it's
[10:58:14 -> 10:58:16]  like kind of hard to do that right you
[10:58:15 -> 10:58:18]  have multiple things in there you have
[10:58:16 -> 10:58:21]  to like fuse kernels together it's more
[10:58:18 -> 10:58:23]  complicated so I decided to keep this
[10:58:21 -> 10:58:25]  like as as manageable as possible super
[10:58:23 -> 10:58:31]  hackable um you know
[10:58:25 -> 10:58:33]  modular but in the map mole the
[10:58:31 -> 10:58:35]  ab
[10:58:33 -> 10:58:38]  um yeah this
[10:58:35 -> 10:58:41]  is this is literally the same as C
[10:58:38 -> 10:58:43]  friendly so if I pop this over here um
[10:58:41 -> 10:58:46]  actually maybe I'll bring it actually
[10:58:43 -> 10:58:50]  downward um
[10:58:46 -> 10:58:55]  but in here linear forward right we just
[10:58:50 -> 10:58:57]  do x * W this is x * W we're not uh x *
[10:58:55 -> 10:59:00]  W A and B same thing we're not
[10:58:57 -> 10:59:02]  transposing anything and inside of there
[10:59:00 -> 10:59:04]  and I already went over this function
[10:59:02 -> 10:59:08]  but you you kind of get the idea we just
[10:59:04 -> 10:59:11]  do a maple between A and B that goes um
[10:59:08 -> 10:59:14]  we do a bi bias forward so that's also
[10:59:11 -> 10:59:16]  going to we pop over to there yeah I
[10:59:14 -> 10:59:19]  already reviewed that too don't need to
[10:59:16 -> 10:59:21]  go over that again Rue forward it's just
[10:59:19 -> 10:59:24]  going to apply that to each element very
[10:59:21 -> 10:59:26]  simple M Mo A and B so same thing again
[10:59:24 -> 10:59:28]  except it's the hidden it's it's
[10:59:26 -> 10:59:30]  actually the the hidden to Output
[10:59:28 -> 10:59:32]  instead of the output to Hidden so next
[10:59:30 -> 10:59:35]  one and then we do the bias forward
[10:59:32 -> 10:59:36]  again and then softmax right so
[10:59:35 -> 10:59:39]  obviously like this is very very simple
[10:59:36 -> 10:59:40]  to follow um it's more like more of the
[10:59:39 -> 10:59:43]  complexity happens within the actual
[10:59:40 -> 10:59:48]  function so like this is a lot
[10:59:43 -> 10:59:50]  more to handle than the than the uh than
[10:59:48 -> 10:59:55]  just the the for pass function itself
[10:59:50 -> 11:00:00]  right so I exit on that and then
[10:59:55 -> 11:00:03]  um and then go down to say the backward
[11:00:00 -> 11:00:06]  function which is a little
[11:00:03 -> 11:00:10]  worse to be honest go down to here
[11:00:06 -> 11:00:13]  backward function so inside of here
[11:00:10 -> 11:00:16]  um we're going to zerog grad everything
[11:00:13 -> 11:00:20]  so what this means is previously
[11:00:16 -> 11:00:23]  our gradients we're accumulating items
[11:00:20 -> 11:00:25]  right so our grad um our grad bias and
[11:00:23 -> 11:00:29]  our grad weights are actually
[11:00:25 -> 11:00:30]  accumulating stuff so we want to just
[11:00:29 -> 11:00:32]  zero those out right this is the
[11:00:30 -> 11:00:34]  equivalent of zero grad and P torch when
[11:00:32 -> 11:00:38]  we go here
[11:00:34 -> 11:00:40]  um we do Optimizer do step so that's
[11:00:38 -> 11:00:41]  like the actual update weight so you you
[11:00:40 -> 11:00:43]  know do the forward and then you
[11:00:41 -> 11:00:45]  calculate the loss then you do backward
[11:00:43 -> 11:00:46]  and then you update the weights with
[11:00:45 -> 11:00:48]  gradient descent and and then you zero
[11:00:46 -> 11:00:52]  grad so you're just zeroing out every
[11:00:48 -> 11:00:54]  single gradient um in the entire network
[11:00:52 -> 11:00:56]  and then you're going to recalculate
[11:00:54 -> 11:01:00]  those when you do the next Optimizer do
[11:00:56 -> 11:01:02]  step right or sorry no loss. backward
[11:01:00 -> 11:01:03]  rather when you calculate the gradients
[11:01:02 -> 11:01:05]  again they're not going to accumulate
[11:01:03 -> 11:01:07]  further they're just going to set them
[11:01:05 -> 11:01:08]  initially and you're going to update
[11:01:07 -> 11:01:11]  based on those gradients and then and
[11:01:08 -> 11:01:14]  then put them down to zero again
[11:01:11 -> 11:01:16]  um so that that's all we do here we zero
[11:01:14 -> 11:01:19]  grad and there's actually a function for
[11:01:16 -> 11:01:26]  this so mem set literally just a c
[11:01:19 -> 11:01:27]  function um Set Set n bytes of s to C so
[11:01:26 -> 11:01:32]  Set
[11:01:27 -> 11:01:34]  n set set um so byes of s which is the
[11:01:32 -> 11:01:37]  first one so
[11:01:34 -> 11:01:41]  grad and then C is the value we want to
[11:01:37 -> 11:01:44]  set it to so zero and then the N the
[11:01:41 -> 11:01:46]  length of it is just the size of grad
[11:01:44 -> 11:01:49]  right so size
[11:01:46 -> 11:01:51]  is the number of numbers and then a
[11:01:49 -> 11:01:53]  float is like the number of bytes
[11:01:51 -> 11:01:55]  essentially so the number of bytes that
[11:01:53 -> 11:01:59]  grad occupies in memory we're going to
[11:01:55 -> 11:02:01]  start from the beginning of that uh and
[11:01:59 -> 11:02:03]  we're going to initialize all those
[11:02:01 -> 11:02:05]  values at the at those sequential memory
[11:02:03 -> 11:02:07]  addresses we're going to set those to
[11:02:05 -> 11:02:08]  zero that's what zero grab does on a
[11:02:07 -> 11:02:11]  very low
[11:02:08 -> 11:02:12]  level um so we do that with our weights
[11:02:11 -> 11:02:15]  and our
[11:02:12 -> 11:02:17]  biases now we do a grad output so this
[11:02:15 -> 11:02:22]  is just going to a malic batch size time
[11:02:17 -> 11:02:24]  output size so B * 10 right B by 10 they
[11:02:22 -> 11:02:26]  compute output gradients so we go to
[11:02:24 -> 11:02:27]  this Lally all this is doing is it's
[11:02:26 -> 11:02:29]  taking this this grat output which we
[11:02:27 -> 11:02:32]  initialize everything is like zero and
[11:02:29 -> 11:02:34]  we have this output which is the um
[11:02:32 -> 11:02:36]  which remember from the forward pass is
[11:02:34 -> 11:02:37]  the output of the softmax which is
[11:02:36 -> 11:02:40]  actually a probability distribution it
[11:02:37 -> 11:02:41]  is not low jits it's not before the
[11:02:40 -> 11:02:43]  exponentiation it is actually a
[11:02:41 -> 11:02:46]  probability distribution with each value
[11:02:43 -> 11:02:48]  between 0 and one then we have the
[11:02:46 -> 11:02:50]  labels and literally all we do here is
[11:02:48 -> 11:02:53]  we just element wise we set the grad
[11:02:50 -> 11:02:55]  output to um the actual output value so
[11:02:53 -> 11:02:58]  which which uh which floating Point
[11:02:55 -> 11:03:01]  number is it right and then we subtract
[11:02:58 -> 11:03:03]  one based on the actual label of it so
[11:03:01 -> 11:03:06]  notice how in before in C friendly we
[11:03:03 -> 11:03:09]  did um we did out grad output equals
[11:03:06 -> 11:03:10]  softmax probs minus y true we're doing
[11:03:09 -> 11:03:12]  the same thing here except we can't just
[11:03:10 -> 11:03:14]  do a simple numpy element wise
[11:03:12 -> 11:03:16]  operations we can't just be be dumb and
[11:03:14 -> 11:03:18]  say this right we have to actually
[11:03:16 -> 11:03:20]  we have to be explicit and we can be a
[11:03:18 -> 11:03:25]  little clever by just doing the minus
[11:03:20 -> 11:03:26]  equals 1 uh there so uh that's it's
[11:03:25 -> 11:03:28]  literally doing the same thing we're
[11:03:26 -> 11:03:30]  just using a different
[11:03:28 -> 11:03:32]  trick if we just continue going through
[11:03:30 -> 11:03:34]  we just did compute output gradients now
[11:03:32 -> 11:03:36]  we now we can actually use those
[11:03:34 -> 11:03:38]  gradients and start using like Matrix
[11:03:36 -> 11:03:41]  multiple highs and bias backwards and Ru
[11:03:38 -> 11:03:44]  backwards and all this stuff right so
[11:03:41 -> 11:03:48]  the first one here is Matt Mo um we go
[11:03:44 -> 11:03:52]  through uh we we calculate w2. grad now
[11:03:48 -> 11:03:54]  w2. grad is right here so x2. T times
[11:03:52 -> 11:03:57]  the derivative of the loss which is grat
[11:03:54 -> 11:04:03]  output so here we do a hidden which is
[11:03:57 -> 11:04:03]  X2 here in this case um or
[11:04:03 -> 11:04:14]  sorry yeah X2 so the um essentially the
[11:04:10 -> 11:04:16]  the input going into that um and then
[11:04:14 -> 11:04:19]  times so this is times B right so B is
[11:04:16 -> 11:04:21]  in this case is the grad output so we're
[11:04:19 -> 11:04:23]  transposing this with this a here and
[11:04:21 -> 11:04:25]  then times this and then that's going to
[11:04:23 -> 11:04:29]  equal C or the grad weights 2 right so
[11:04:25 -> 11:04:32]  grad weights number W2 right
[11:04:29 -> 11:04:33]  um yeah so that that that that should be
[11:04:32 -> 11:04:36]  fairly
[11:04:33 -> 11:04:38]  intuitive now we go further we go to
[11:04:36 -> 11:04:40]  bias backward bias backward isn't
[11:04:38 -> 11:04:43]  actually too bad so bias
[11:04:40 -> 11:04:46]  backward is uh literally just going to
[11:04:43 -> 11:04:47]  we're going to iterate through um the
[11:04:46 -> 11:04:49]  size I mean keep in mind we have batch
[11:04:47 -> 11:04:52]  size right so we're going to iterate
[11:04:49 -> 11:04:54]  through the entire size of it uh you
[11:04:52 -> 11:04:58]  know iterating and incrementing I each
[11:04:54 -> 11:05:03]  time gra we're going to uh store uh a
[11:04:58 -> 11:05:05]  bias value a gradient bias gradient this
[11:05:03 -> 11:05:08]  index as
[11:05:05 -> 11:05:11]  zero and we're going to iterate through
[11:05:08 -> 11:05:13]  the entire batch size and we're
[11:05:11 -> 11:05:16]  literally just going to set that
[11:05:13 -> 11:05:17]  specific value we're going to iterate
[11:05:16 -> 11:05:18]  remember we're iterating through the
[11:05:17 -> 11:05:20]  entire batch size so like we were doing
[11:05:18 -> 11:05:23]  before how we were like smooshing the
[11:05:20 -> 11:05:27]  numbers we're going to go through that
[11:05:23 -> 11:05:29]  we're going to just um essentially do B
[11:05:27 -> 11:05:33]  times
[11:05:29 -> 11:05:37]  size so the entire uh the entire length
[11:05:33 -> 11:05:39]  of that the entire length of of um of
[11:05:37 -> 11:05:42]  like a of like a
[11:05:39 -> 11:05:46]  row and then plus
[11:05:42 -> 11:05:47]  I and we're just going to set um um
[11:05:46 -> 11:05:49]  because this is going to increase each
[11:05:47 -> 11:05:52]  time so we're essentially just going
[11:05:49 -> 11:05:53]  down one one row at a time right and
[11:05:52 -> 11:05:55]  we're we're smooshing it together
[11:05:53 -> 11:05:58]  because we're plus equal accumulating
[11:05:55 -> 11:06:01]  that value here so our grad bias is the
[11:05:58 -> 11:06:03]  this is the equivalent of np.sum across
[11:06:01 -> 11:06:07]  axis zero keeping dims true
[11:06:03 -> 11:06:10]  right uh and then we go back
[11:06:07 -> 11:06:13]  to where is
[11:06:10 -> 11:06:15]  it we pop back to the backward function
[11:06:13 -> 11:06:18]  awesome so now we get bias backward we
[11:06:15 -> 11:06:21]  pass in the uh grad bias right so we're
[11:06:18 -> 11:06:23]  calculating the the gradient bias and we
[11:06:21 -> 11:06:25]  pass in the grad so that would in this
[11:06:23 -> 11:06:29]  case would be the grad output and then
[11:06:25 -> 11:06:30]  batch size and size right so that should
[11:06:29 -> 11:06:31]  be fairly intuitive it's just the
[11:06:30 -> 11:06:34]  gradients are directly flowing and we
[11:06:31 -> 11:06:36]  just do an accumulate operation across
[11:06:34 -> 11:06:38]  the batch because we want to generalize
[11:06:36 -> 11:06:40]  over a batch right it's more useful to
[11:06:38 -> 11:06:43]  do that um and then we just do since
[11:06:40 -> 11:06:47]  we're done the W2 now we actually move
[11:06:43 -> 11:06:49]  on to uh the DX X2 right so
[11:06:47 -> 11:06:52]  dx2 uh is right
[11:06:49 -> 11:06:54]  here we Malik this cuz remember this is
[11:06:52 -> 11:06:55]  just temporary we don't we don't
[11:06:54 -> 11:06:57]  actually need to store this this doesn't
[11:06:55 -> 11:06:58]  need to be updated anywhere we're just
[11:06:57 -> 11:07:01]  calculating this because it's a
[11:06:58 -> 11:07:03]  prerequisite for calculating W1 so we we
[11:07:01 -> 11:07:06]  can free it after we don't need to store
[11:07:03 -> 11:07:09]  this in memory just be like efficient um
[11:07:06 -> 11:07:12]  and inside of here we literally just go
[11:07:09 -> 11:07:17]  uh grad output time W2 transpose right
[11:07:12 -> 11:07:20]  so we go grad output a W2 transpose T
[11:07:17 -> 11:07:24]  and we store that in dx2 right going
[11:07:20 -> 11:07:29]  down further D out which is this part
[11:07:24 -> 11:07:33]  here d r out is dx2 * d r of
[11:07:29 -> 11:07:35]  X we allocate memory for this we go
[11:07:33 -> 11:07:40]  through um we go through the entire
[11:07:35 -> 11:07:42]  batch we literally just do dx2 so dx2
[11:07:40 -> 11:07:43]  time
[11:07:42 -> 11:07:47]  D
[11:07:43 -> 11:07:49]  um d of of whatever of whatever that
[11:07:47 -> 11:07:53]  value is just just going through it
[11:07:49 -> 11:07:58]  right and this is going to be um if this
[11:07:53 -> 11:08:02]  is this is going to evaluate to a a true
[11:07:58 -> 11:08:05]  so or essentially a one if this if this
[11:08:02 -> 11:08:08]  if this value is um essentially just
[11:08:05 -> 11:08:11]  relue derivative right Rue derivative
[11:08:08 -> 11:08:12]  that's all this is um we might not
[11:08:11 -> 11:08:14]  actually even need the Rue derivative
[11:08:12 -> 11:08:16]  backwards in in this whole thing but
[11:08:14 -> 11:08:19]  we're going to keep it anyways um this
[11:08:16 -> 11:08:23]  this does work though so this stores the
[11:08:19 -> 11:08:27]  D out that's what we want right d out is
[11:08:23 -> 11:08:31]  good then we update using the using the
[11:08:27 -> 11:08:34]  um what's it called we calculate D out
[11:08:31 -> 11:08:36]  from dx2 which we which we used up here
[11:08:34 -> 11:08:38]  right we stored that stored the gradient
[11:08:36 -> 11:08:39]  for that the temporary gradient there we
[11:08:38 -> 11:08:42]  don't need it for later we just need it
[11:08:39 -> 11:08:45]  for now calculate dout based on that and
[11:08:42 -> 11:08:49]  then we use D out d
[11:08:45 -> 11:08:55]  Rue out uh later and we essentially go
[11:08:49 -> 11:08:58]  to calculate W1 we go x1. t time d r out
[11:08:55 -> 11:09:00]  right so transpose so a tore B that's
[11:08:58 -> 11:09:06]  what this is going to
[11:09:00 -> 11:09:10]  be a tore B we go input
[11:09:06 -> 11:09:13]  so um input X1 transpose that times D
[11:09:10 -> 11:09:15]  value out and store that in we it's
[11:09:13 -> 11:09:18]  one awesome
[11:09:15 -> 11:09:23]  bias backward as as per usual so we just
[11:09:18 -> 11:09:27]  have the uh the bias itself um and then
[11:09:23 -> 11:09:30]  the the one that it's flowing uh flowing
[11:09:27 -> 11:09:33]  from right so whatever it's whatever the
[11:09:30 -> 11:09:35]  um whatever the previous layer gradient
[11:09:33 -> 11:09:38]  was that's just going to flow directly
[11:09:35 -> 11:09:40]  into bias right because um you know
[11:09:38 -> 11:09:42]  adding adding does not uh change the
[11:09:40 -> 11:09:44]  gradient of something it just changes
[11:09:42 -> 11:09:47]  the like position the offset of it but
[11:09:44 -> 11:09:48]  the slope remains same um but yeah
[11:09:47 -> 11:09:51]  that's that's literally the backward
[11:09:48 -> 11:09:53]  pass not too bad that might have been
[11:09:51 -> 11:09:54]  like a little hard to keep up with my my
[11:09:53 -> 11:09:58]  D you outs constantly it might have
[11:09:54 -> 11:10:01]  confused you but uh yeah that's that's
[11:09:58 -> 11:10:05]  pretty much it and then inside of um
[11:10:01 -> 11:10:08]  inside of yeah so that that's backward
[11:10:05 -> 11:10:10]  and then after backward after this we do
[11:10:08 -> 11:10:11]  update weights then we print some stuff
[11:10:10 -> 11:10:13]  out so let's just pay attention to
[11:10:11 -> 11:10:14]  update weights here don't don't worry
[11:10:13 -> 11:10:16]  about the rest of this you can parse
[11:10:14 -> 11:10:17]  this on your own what we really care
[11:10:16 -> 11:10:19]  about is the actual learning mechanics
[11:10:17 -> 11:10:22]  of it right you can print anything out
[11:10:19 -> 11:10:24]  any day you want it's very easy um but
[11:10:22 -> 11:10:25]  we care about what the actual mechanism
[11:10:24 -> 11:10:28]  is
[11:10:25 -> 11:10:31]  here so if I go to update weights we
[11:10:28 -> 11:10:34]  pass in the neuronet struct right the
[11:10:31 -> 11:10:37]  pointer to that we access we go through
[11:10:34 -> 11:10:39]  each one in here and we literally just
[11:10:37 -> 11:10:41]  um we just do the same thing so hidden
[11:10:39 -> 11:10:45]  size times input size so that's going to
[11:10:41 -> 11:10:48]  be um you know 784 by 256 that's the
[11:10:45 -> 11:10:50]  weight one we we iterate through each
[11:10:48 -> 11:10:51]  index remember this is laid out
[11:10:50 -> 11:10:54]  sequentially in memory so this is going
[11:10:51 -> 11:10:56]  to evaluate to whatever 784 by 256 is
[11:10:54 -> 11:10:57]  that's a large value just going to
[11:10:56 -> 11:10:59]  iterate through that in memory it's
[11:10:57 -> 11:11:03]  going to go through the lines or this
[11:10:59 -> 11:11:05]  straight thing and it's just going to do
[11:11:03 -> 11:11:06]  learning rate times whatever the grad
[11:11:05 -> 11:11:09]  was and then it's going to minus equals
[11:11:06 -> 11:11:11]  accumulate that into um weights one it's
[11:11:09 -> 11:11:14]  going to do the same thing for weights
[11:11:11 -> 11:11:16]  two remember weights two is um output
[11:11:14 -> 11:11:21]  size times hidden size so hidden size
[11:11:16 -> 11:11:24]  256 output size is 10 um bias hidden
[11:11:21 -> 11:11:27]  size two out
[11:11:24 -> 11:11:29]  size very
[11:11:27 -> 11:11:31]  straightforward and that's pretty much
[11:11:29 -> 11:11:34]  it
[11:11:31 -> 11:11:36]  um let's go ahead and train this
[11:11:34 -> 11:11:39]  thing so going back up let me just make
[11:11:36 -> 11:11:42]  sure these are all set
[11:11:39 -> 11:11:44]  correctly we'll do 256 sure we'll do
[11:11:42 -> 11:11:46]  batch size of four that's fine learning
[11:11:44 -> 11:11:49]  rate
[11:11:46 -> 11:11:50]  0.01 that's okay as well we'll set theox
[11:11:49 -> 11:11:54]  to
[11:11:50 -> 11:11:54]  five just to be less
[11:11:54 -> 11:11:59]  redundant or we we set it to actually
[11:11:56 -> 11:12:04]  set it to three why not so now we go
[11:11:59 -> 11:12:05]  into here GCC Das o V1 and then V1 doc
[11:12:04 -> 11:12:08]  which is the file we're going to compile
[11:12:05 -> 11:12:10]  and we do LM for link math right so
[11:12:08 -> 11:12:12]  inside of here we do the math.h file we
[11:12:10 -> 11:12:17]  need to link math for this to work
[11:12:12 -> 11:12:18]  because if we don't right and um
[11:12:17 -> 11:12:19]  unidentified reference to this these are
[11:12:18 -> 11:12:21]  all the math functions right but if we
[11:12:19 -> 11:12:24]  do
[11:12:21 -> 11:12:26]  LM it'll work we can go and run this so
[11:12:24 -> 11:12:28]  we get the first this is a five printed
[11:12:26 -> 11:12:33]  out first Trend training labels is five
[11:12:28 -> 11:12:34]  as we see here and then 04 1 9 2 1 31 4
[11:12:33 -> 11:12:37]  and we can see the accuracy starts off
[11:12:34 -> 11:12:41]  at about well the loss is about 2.3
[11:12:37 -> 11:12:44]  which is uh random as we'd expect it to
[11:12:41 -> 11:12:46]  so 2.3 evaluates to about 10% accuracy
[11:12:44 -> 11:12:49]  and we can see that through the first
[11:12:46 -> 11:12:51]  Epoch it goes through 2 200 HS and we
[11:12:49 -> 11:12:54]  can see that the
[11:12:51 -> 11:12:56]  accuracy goes up to about 60% which is
[11:12:54 -> 11:12:58]  solid right loss is going
[11:12:56 -> 11:13:00]  down and then in the next one it sees oh
[11:12:58 -> 11:13:01]  my gosh wait we've seen these samples
[11:13:00 -> 11:13:04]  before and it's going to drop even
[11:13:01 -> 11:13:06]  further it's going to go down to 088 and
[11:13:04 -> 11:13:08]  accuracy is going to fly up you know 15%
[11:13:06 -> 11:13:10]  because it's already seen these samples
[11:13:08 -> 11:13:12]  before um and then it's going to do that
[11:13:10 -> 11:13:16]  again it's going to go up to
[11:13:12 -> 11:13:20]  86% and we end up with about 88 8.6%
[11:13:16 -> 11:13:25]  here um and uh yeah I mean you could
[11:13:20 -> 11:13:28]  always print out the the ending samples
[11:13:25 -> 11:13:30]  if you really wanted to um you could
[11:13:28 -> 11:13:33]  always print out like some extra samples
[11:13:30 -> 11:13:35]  and just like how it um how it matches
[11:13:33 -> 11:13:39]  those up um but we're going to notice
[11:13:35 -> 11:13:42]  that in our uh in our Cuda file so uh
[11:13:39 -> 11:13:43]  anyways that that was a lot to unpack
[11:13:42 -> 11:13:46]  there but that is the C file that's how
[11:13:43 -> 11:13:47]  we transfer from numpy to see it's not
[11:13:46 -> 11:13:49]  actually that crazy of a jump it's
[11:13:47 -> 11:13:52]  mainly just writing the algorithm you
[11:13:49 -> 11:13:53]  know the hard way um and just kind of
[11:13:52 -> 11:13:55]  being more aware about things right it's
[11:13:53 -> 11:13:57]  very easy to run into issues but as long
[11:13:55 -> 11:13:59]  as we're careful about things it
[11:13:57 -> 11:14:02]  shouldn't be too bad let's go and jump
[11:13:59 -> 11:14:03]  into Cuda now okay okay everyone so this
[11:14:02 -> 11:14:05]  is one of the last parts of the course
[11:14:03 -> 11:14:07]  actually and this is part this part is
[11:14:05 -> 11:14:09]  intended to be is intended to be a
[11:14:07 -> 11:14:11]  little shorter so this is designed to
[11:14:09 -> 11:14:14]  give you s of sort of a template for
[11:14:11 -> 11:14:16]  continuing on this is the final project
[11:14:14 -> 11:14:18]  right so uh I'm not going to give you
[11:14:16 -> 11:14:19]  all the answers right away a part of
[11:14:18 -> 11:14:21]  your job is to figure this out on your
[11:14:19 -> 11:14:23]  own and use what we've developed
[11:14:21 -> 11:14:29]  beforehand to continue and optimize
[11:14:23 -> 11:14:34]  performance further right so I have this
[11:14:29 -> 11:14:37]  um I have this naive GPU file right here
[11:14:34 -> 11:14:39]  uh or sorry folder inside of Cuda so we
[11:14:37 -> 11:14:39]  go
[11:14:39 -> 11:14:45]  to we go to um go to the Cuda directory
[11:14:43 -> 11:14:49]  and then we CD into n
[11:14:45 -> 11:14:50]  View and inside of here um you're going
[11:14:49 -> 11:14:52]  to find this file this vew one right I
[11:14:50 -> 11:14:54]  just have this as like versions so you
[11:14:52 -> 11:14:57]  can I'll like update more versions later
[11:14:54 -> 11:14:59]  on if something breaks uh in future Cuda
[11:14:57 -> 11:15:01]  releases or whatever but um yeah so this
[11:14:59 -> 11:15:05]  is essentially just a direct Port from
[11:15:01 -> 11:15:08]  our our C file so literally all we do
[11:15:05 -> 11:15:10]  here is we we load in the same things um
[11:15:08 -> 11:15:12]  we initialize way it's the same all
[11:15:10 -> 11:15:14]  these are done on CPU as you can see the
[11:15:12 -> 11:15:17]  only things that we actually change are
[11:15:14 -> 11:15:19]  the m Kels right so we can see the well
[11:15:17 -> 11:15:22]  there's more than just mapal kernels but
[11:15:19 -> 11:15:25]  you can see so we have this mmal A and B
[11:15:22 -> 11:15:27]  um so this is this is not
[11:15:25 -> 11:15:30]  transposed uh and then this is B
[11:15:27 -> 11:15:31]  transposed and then this is a transposed
[11:15:30 -> 11:15:33]  right so when we're doing our backward
[11:15:31 -> 11:15:35]  pass and we need to transpose certain
[11:15:33 -> 11:15:37]  values uh that's what we'll use that for
[11:15:35 -> 11:15:39]  right so we have certain kernels that
[11:15:37 -> 11:15:41]  dedicated for that and we can actually
[11:15:39 -> 11:15:44]  see based on the indexing scheme in here
[11:15:41 -> 11:15:47]  um like notice how we we iterate Over N
[11:15:44 -> 11:15:48]  every sing single time except this one
[11:15:47 -> 11:15:49]  this one is a little different this one
[11:15:48 -> 11:15:53]  is M
[11:15:49 -> 11:15:55]  but uh if we go in here we can see a
[11:15:53 -> 11:15:57]  this is just the normal one right so row
[11:15:55 -> 11:16:01]  *
[11:15:57 -> 11:16:05]  n u plus I and then if we go to this one
[11:16:01 -> 11:16:07]  B it's I * k + column right and this is
[11:16:05 -> 11:16:10]  different so notice how this a stays the
[11:16:07 -> 11:16:12]  same but because we're changing B and
[11:16:10 -> 11:16:14]  making it transpose this part this
[11:16:12 -> 11:16:20]  indexing changes right and then same
[11:16:14 -> 11:16:24]  idea here is uh we just transpose a so
[11:16:20 -> 11:16:27]  this part I K and then column i k and
[11:16:24 -> 11:16:30]  then column and um this part is going to
[11:16:27 -> 11:16:33]  be different right so instead of row N I
[11:16:30 -> 11:16:33]  it's I N
[11:16:33 -> 11:16:40]  row and
[11:16:36 -> 11:16:41]  uh yeah that's those are pretty much the
[11:16:40 -> 11:16:43]  major changes the reason why we do this
[11:16:41 -> 11:16:46]  for
[11:16:43 -> 11:16:49]  um we do this for a is because a is
[11:16:46 -> 11:16:51]  transposed um and and the m is m is a
[11:16:49 -> 11:16:52]  little different but these are these are
[11:16:51 -> 11:16:55]  naive kernels I expect you able to
[11:16:52 -> 11:16:58]  dissect these but if we continue further
[11:16:55 -> 11:17:00]  we have the ru we have our bias uh we
[11:16:58 -> 11:17:04]  have a softmax kernel so this is going
[11:17:00 -> 11:17:05]  to do a single softmax uh output so a
[11:17:04 -> 11:17:07]  distribution for every single batch
[11:17:05 -> 11:17:09]  element so it's going to go vertically
[11:17:07 -> 11:17:12]  downwards each each thread is going to
[11:17:09 -> 11:17:12]  do a a
[11:17:13 -> 11:17:16]  row we zero our great gradiance with
[11:17:15 -> 11:17:18]  this simple kernel so it's just going to
[11:17:16 -> 11:17:20]  go you know it's going to go through
[11:17:18 -> 11:17:23]  every single value and just set that to
[11:17:20 -> 11:17:26]  zero um probably faster than mem set I'm
[11:17:23 -> 11:17:29]  not sure but and then we have our D
[11:17:26 -> 11:17:35]  kernel so this is just the derivative of
[11:17:29 -> 11:17:37]  Ru multiply gradients um so element wise
[11:17:35 -> 11:17:39]  multiplication of gradients which we can
[11:17:37 -> 11:17:42]  see is
[11:17:39 -> 11:17:46]  used down here in multiply
[11:17:42 -> 11:17:46]  gradients and this function
[11:17:47 -> 11:17:52]  is used uh is used right here so when
[11:17:49 -> 11:17:55]  we're doing our when we're doing our Rue
[11:17:52 -> 11:17:59]  um when we're we're doing our actual uh
[11:17:55 -> 11:18:01]  we're going through D and we need to um
[11:17:59 -> 11:18:04]  multiply those values it kind of makes
[11:18:01 -> 11:18:05]  sense why we would put that there um
[11:18:04 -> 11:18:08]  going back
[11:18:05 -> 11:18:12]  up we just have our forward so the
[11:18:08 -> 11:18:15]  typical a * B I don't know why this is T
[11:18:12 -> 11:18:15]  um
[11:18:16 -> 11:18:22]  okay
[11:18:18 -> 11:18:22]  um Bim is not working all
[11:18:25 -> 11:18:32]  right and we just do a ml add the bias
[11:18:28 -> 11:18:34]  Ru ml add a bias then softmax right very
[11:18:32 -> 11:18:37]  simple cross entropy loss is going to be
[11:18:34 -> 11:18:40]  done on the host we're going to compute
[11:18:37 -> 11:18:41]  our output gradients on the GPU because
[11:18:40 -> 11:18:43]  there there is actually going to be a
[11:18:41 -> 11:18:45]  lot of you have to consider when you're
[11:18:43 -> 11:18:47]  actually writing Kel you're like what is
[11:18:45 -> 11:18:50]  the how useful is it right to actually
[11:18:47 -> 11:18:54]  go and write your own um actually go and
[11:18:50 -> 11:18:56]  write your own so like for example this
[11:18:54 -> 11:18:59]  one we could probably turn this into CPU
[11:18:56 -> 11:19:01]  and it might be faster but who knows um
[11:18:59 -> 11:19:02]  the kind of the idea here is like if you
[11:19:01 -> 11:19:04]  have a big one if you have a big update
[11:19:02 -> 11:19:07]  to do like update to gradients and you
[11:19:04 -> 11:19:09]  have these big giant um you have these
[11:19:07 -> 11:19:11]  giant W matrices that you're trying to
[11:19:09 -> 11:19:14]  change and modify then having a thread
[11:19:11 -> 11:19:16]  to do each little uh to do each little
[11:19:14 -> 11:19:18]  like point update will help speed that
[11:19:16 -> 11:19:20]  up but if you're doing just like a B
[11:19:18 -> 11:19:21]  by10 for example which is what this is
[11:19:20 -> 11:19:23]  you could probably get away with just
[11:19:21 -> 11:19:25]  doing this really fast on CPU and you'd
[11:19:23 -> 11:19:26]  be you'd be fine uh cuz there is like
[11:19:25 -> 11:19:28]  kernel launch overhead when you have to
[11:19:26 -> 11:19:30]  like literally launch this it has to
[11:19:28 -> 11:19:32]  tell the GPU what to do and then it has
[11:19:30 -> 11:19:35]  to trigger a bunch of threads to go and
[11:19:32 -> 11:19:37]  execute that right um and then in our
[11:19:35 -> 11:19:38]  backward pass we zero out the gradients
[11:19:37 -> 11:19:40]  to make sure that they're not
[11:19:38 -> 11:19:42]  accumulating and giving us you know mix
[11:19:40 -> 11:19:45]  signals
[11:19:42 -> 11:19:47]  um we have our
[11:19:45 -> 11:19:49]  uh compute compute output gradient so
[11:19:47 -> 11:19:52]  that's going to be the essentially the
[11:19:49 -> 11:19:54]  these the output probabilities minus the
[11:19:52 -> 11:19:57]  the true labels
[11:19:54 -> 11:20:02]  right and then we have our our a
[11:19:57 -> 11:20:05]  transpose times B so we go back to here
[11:20:02 -> 11:20:09]  um this this
[11:20:05 -> 11:20:11]  dw2 right we update um we update
[11:20:09 -> 11:20:13]  gradients for bias 2 so there's specific
[11:20:11 -> 11:20:15]  kernel for that now remember when we're
[11:20:13 -> 11:20:17]  launching these there's it it seems like
[11:20:15 -> 11:20:20]  there's a lot to sort through but really
[11:20:17 -> 11:20:21]  all this is is looking at that previous
[11:20:20 -> 11:20:25]  Technique we did where it's like you you
[11:20:21 -> 11:20:28]  have to launch um say if you have 1,25
[11:20:25 -> 11:20:29]  elements and you would normally only do
[11:20:28 -> 11:20:32]  um like
[11:20:29 -> 11:20:35]  1,024 uh threads right so what you do is
[11:20:32 -> 11:20:38]  you add on add an additional block right
[11:20:35 -> 11:20:39]  with a with a single um with a single
[11:20:38 -> 11:20:42]  thread inside of it or or just or
[11:20:39 -> 11:20:45]  another Block in general you could say
[11:20:42 -> 11:20:47]  and since you have those bounds the like
[11:20:45 -> 11:20:49]  that little if statement that checks if
[11:20:47 -> 11:20:51]  everything is is worth so that it's not
[11:20:49 -> 11:20:53]  like out of the out of the matrices that
[11:20:51 -> 11:20:55]  you're working with like out like outer
[11:20:53 -> 11:20:56]  outside of that memory um then you can
[11:20:55 -> 11:20:58]  that that's essentially what this is
[11:20:56 -> 11:21:01]  right so we're just being careful about
[11:20:58 -> 11:21:02]  how we launch this stuff um and yeah
[11:21:01 -> 11:21:05]  this this goes back to the you know
[11:21:02 -> 11:21:08]  chapter number five on on kernels right
[11:21:05 -> 11:21:10]  so uh these launch configurations can
[11:21:08 -> 11:21:12]  like really mess you up and make things
[11:21:10 -> 11:21:13]  look more complicated than they are but
[11:21:12 -> 11:21:17]  if you can just like look through this
[11:21:13 -> 11:21:19]  one bit at a time you have the uh you
[11:21:17 -> 11:21:21]  have like the grid size and sorry the
[11:21:19 -> 11:21:23]  grid dim and the block dim right and and
[11:21:21 -> 11:21:25]  that's all you're really working with
[11:21:23 -> 11:21:28]  and then of course the the arguments for
[11:21:25 -> 11:21:28]  the kernel launch
[11:21:32 -> 11:21:41]  itself more mmols right going back to
[11:21:36 -> 11:21:44]  here this times so it's a * B
[11:21:41 -> 11:21:46]  transpose a * B transpose
[11:21:44 -> 11:21:47]  uh and then inside here we just do our
[11:21:46 -> 11:21:49]  we just go backward through our ra
[11:21:47 -> 11:21:51]  function all this is like very close to
[11:21:49 -> 11:21:55]  our C file it's just like it's just run
[11:21:51 -> 11:21:57]  in parallel right um same idea here I'm
[11:21:55 -> 11:22:00]  just going to go further downwards
[11:21:57 -> 11:22:02]  updating our weights um you know it's
[11:22:00 -> 11:22:02]  kind
[11:22:12 -> 11:22:16]  of I guess I have two of these
[11:22:15 -> 11:22:18]  are we using
[11:22:16 -> 11:22:23]  this let me
[11:22:18 -> 11:22:26]  see yes we are we're using that one but
[11:22:23 -> 11:22:28]  this one update
[11:22:26 -> 11:22:31]  gradients yeah that's also okay so so
[11:22:28 -> 11:22:33]  these are different things right yeah so
[11:22:31 -> 11:22:35]  the the update gradients that's for the
[11:22:33 -> 11:22:37]  bias and the update weights that is for
[11:22:35 -> 11:22:40]  um that that is actually where gradient
[11:22:37 -> 11:22:44]  descent itself takes place
[11:22:40 -> 11:22:44]  um and then inside of this Loop
[11:22:46 -> 11:22:50]  we go through uh initializing our our
[11:22:48 -> 11:22:53]  device training sets so when you have
[11:22:50 -> 11:22:55]  this D prefix remember that's for device
[11:22:53 -> 11:22:58]  um you know Y is labels train is the
[11:22:55 -> 11:23:00]  train set we C AAL all of those we mem
[11:22:58 -> 11:23:02]  copy them over to device because they're
[11:23:00 -> 11:23:04]  initialized as pointers on the host and
[11:23:02 -> 11:23:07]  we have to copy those over with respect
[11:23:04 -> 11:23:09]  to their memory addresses over to device
[11:23:07 -> 11:23:11]  um and then aside of here we just
[11:23:09 -> 11:23:12]  iterate over all the epochs we need to
[11:23:11 -> 11:23:16]  do which in this case is Define at the
[11:23:12 -> 11:23:18]  top and then number of batches right and
[11:23:16 -> 11:23:20]  then we we set a starting index we make
[11:23:18 -> 11:23:22]  sure to like stride a little bit so we
[11:23:20 -> 11:23:23]  have whichever batch we're at here so
[11:23:22 -> 11:23:25]  number of batches in the total thing
[11:23:23 -> 11:23:27]  which we actually calculate by doing
[11:23:25 -> 11:23:30]  train size number of divide by batch
[11:23:27 -> 11:23:32]  size so if you have a th 10,000 training
[11:23:30 -> 11:23:35]  uh examples and a batch size of four
[11:23:32 -> 11:23:38]  you're going to have um 2500 batches
[11:23:35 -> 11:23:39]  right and so when you do um whichever
[11:23:38 -> 11:23:41]  batch you're at which is going to go
[11:23:39 -> 11:23:45]  through you know it's essentially going
[11:23:41 -> 11:23:50]  to go zero uh three
[11:23:45 -> 11:23:50]  um 0 3
[11:23:51 -> 11:23:59]  7 11 it's going to skip by four
[11:23:56 -> 11:24:01]  right we do our forward pass so keeping
[11:23:59 -> 11:24:04]  this uh this simple this nice little
[11:24:01 -> 11:24:09]  concise NN struct right with all of our
[11:24:04 -> 11:24:13]  our gradients and our our weights
[11:24:09 -> 11:24:17]  right we calculate the uh we we C
[11:24:13 -> 11:24:19]  essentially what this part is doing is
[11:24:17 -> 11:24:22]  we're calculating the loss we're adding
[11:24:19 -> 11:24:26]  it to the total running loss
[11:24:22 -> 11:24:26]  of where is
[11:24:27 -> 11:24:32]  it on this level we have we initialize
[11:24:30 -> 11:24:34]  the total loss inside of the epoch Loop
[11:24:32 -> 11:24:35]  right so this is for the entire Epoch
[11:24:34 -> 11:24:37]  that's why we're that's what we're kind
[11:24:35 -> 11:24:39]  of doing there is when we add the loss
[11:24:37 -> 11:24:40]  we're just like appending it and then
[11:24:39 -> 11:24:42]  we're you know dividing it accordingly
[11:24:40 -> 11:24:45]  so we're just taking like the average
[11:24:42 -> 11:24:49]  loss over the entire epoch
[11:24:45 -> 11:24:51]  um in this case we are essentially just
[11:24:49 -> 11:24:55]  like we were doing in the C file we're
[11:24:51 -> 11:24:56]  just um adding to the correct counter so
[11:24:55 -> 11:24:58]  however many samples we got correct
[11:24:56 -> 11:25:01]  that's that percent
[11:24:58 -> 11:25:03]  accuracy going into backward pass I
[11:25:01 -> 11:25:05]  already walked through this there mean
[11:25:03 -> 11:25:06]  you can kind of sort through all these
[11:25:05 -> 11:25:09]  different
[11:25:06 -> 11:25:11]  arguments uh we update we do an update
[11:25:09 -> 11:25:14]  weights for each of our individual
[11:25:11 -> 11:25:15]  weight matrices um so it's just going to
[11:25:14 -> 11:25:19]  essentially element
[11:25:15 -> 11:25:21]  wise uh it's going to element wise
[11:25:19 -> 11:25:22]  multiply um the gradient times the
[11:25:21 -> 11:25:25]  learning rate and it's going to
[11:25:22 -> 11:25:27]  accumulate that into the weight right on
[11:25:25 -> 11:25:30]  device and then in here we just print
[11:25:27 -> 11:25:33]  some useful stuff right go down here
[11:25:30 -> 11:25:36]  make sure that we free the training sets
[11:25:33 -> 11:25:38]  uh the hidden and the D output that we
[11:25:36 -> 11:25:41]  initialized
[11:25:38 -> 11:25:43]  before we have our initialization of the
[11:25:41 -> 11:25:45]  entire neural net so just essentially
[11:25:43 -> 11:25:47]  doing our
[11:25:45 -> 11:25:49]  malic initializing those so each of
[11:25:47 -> 11:25:50]  these are going to be set to like random
[11:25:49 -> 11:25:53]  values or in this case biases are going
[11:25:50 -> 11:25:56]  to be set to zero and then we do our
[11:25:53 -> 11:25:59]  Cuda Malik so we so we allocate on on
[11:25:56 -> 11:26:02]  CPU we initialize everything on CPU we
[11:25:59 -> 11:26:04]  allocate on device we move everything to
[11:26:02 -> 11:26:06]  device um and then we're ready to run
[11:26:04 -> 11:26:11]  right and then in this case all we would
[11:26:06 -> 11:26:15]  do is um initialize this neural
[11:26:11 -> 11:26:18]  net we we initialize it with with with
[11:26:15 -> 11:26:22]  random data with random data
[11:26:18 -> 11:26:27]  values uh then we load in our entire uh
[11:26:22 -> 11:26:30]  training set into um into the host
[11:26:27 -> 11:26:32]  memory we go and train everything and
[11:26:30 -> 11:26:34]  when we're done we can free whatever we
[11:26:32 -> 11:26:37]  need to on CPU and free whatever we need
[11:26:34 -> 11:26:39]  to on on device so if I go ahead and
[11:26:37 -> 11:26:39]  give this a run
[11:26:42 -> 11:26:48]  here B1
[11:26:45 -> 11:26:48]  not going to run with good
[11:26:48 -> 11:26:52]  boss so variable I was used member
[11:26:50 -> 11:26:55]  reference don't worry about that a
[11:26:52 -> 11:26:59]  warning then we go and run this we can
[11:26:55 -> 11:27:02]  see um this trains insanely fast uh we
[11:26:59 -> 11:27:04]  go from uh Epoch one we have three
[11:27:02 -> 11:27:07]  epochs in here total each one doing 2500
[11:27:04 -> 11:27:09]  HS and we get a loss of about 2.3 which
[11:27:07 -> 11:27:12]  is as we'd expect and then we see the
[11:27:09 -> 11:27:16]  accuracy increase up to 60% and then it
[11:27:12 -> 11:27:18]  gets even it jumps right goes up to goes
[11:27:16 -> 11:27:23]  up to about 87% and then jumps even
[11:27:18 -> 11:27:25]  higher and we end up at around 90% right
[11:27:23 -> 11:27:27]  so that's pretty good and we can uh we
[11:27:25 -> 11:27:30]  can actually run this with bigger Hye
[11:27:27 -> 11:27:33]  parameters so I can go ahead and plug in
[11:27:30 -> 11:27:36]  10,24 there and use a bigger batch size
[11:27:33 -> 11:27:39]  maybe like maybe eight and then epox is
[11:27:36 -> 11:27:45]  five we pop into here compile that we go
[11:27:39 -> 11:27:48]  and run it we can see that um
[11:27:45 -> 11:27:51]  um get a lot better accuracy right so
[11:27:48 -> 11:27:52]  even up to 92% now so it's kind of what
[11:27:51 -> 11:27:54]  what this part is is it's called
[11:27:52 -> 11:27:56]  grocking so you get the first part where
[11:27:54 -> 11:27:58]  it's where it's like just starting on on
[11:27:56 -> 11:28:00]  its training steps and it's sort of
[11:27:58 -> 11:28:01]  figuring out which weights to push in
[11:28:00 -> 11:28:02]  the right direction and then you and
[11:28:01 -> 11:28:05]  then it figures that out and then it
[11:28:02 -> 11:28:08]  plummets the loss drops like really fast
[11:28:05 -> 11:28:10]  which is what we're seeing um right here
[11:28:08 -> 11:28:13]  with the loss is like 2.3 and then it's
[11:28:10 -> 11:28:17]  1.72 and then boom 0.5 then all the way
[11:28:13 -> 11:28:20]  down to like 2.9 0.3 is um and then
[11:28:17 -> 11:28:22]  what's Happening Here is it now no it
[11:28:20 -> 11:28:24]  now can no longer use the easy patterns
[11:28:22 -> 11:28:25]  that it recognized and now it has to
[11:28:24 -> 11:28:27]  search for more for more difficult
[11:28:25 -> 11:28:28]  attributes there might be certain images
[11:28:27 -> 11:28:31]  that it has a really hard time
[11:28:28 -> 11:28:33]  recognizing and it has to and it has to
[11:28:31 -> 11:28:35]  you know learn additional stuff which
[11:28:33 -> 11:28:37]  takes more training steps to do um in
[11:28:35 -> 11:28:39]  that process of deeply understanding the
[11:28:37 -> 11:28:42]  data set or or generalizing over it
[11:28:39 -> 11:28:45]  that's called grocking right
[11:28:42 -> 11:28:46]  um hence the grock language model I was
[11:28:45 -> 11:28:50]  using
[11:28:46 -> 11:28:54]  before um but yeah so if we step out of
[11:28:50 -> 11:28:56]  this and go over to um go over to this
[11:28:54 -> 11:28:58]  this v file I named it V because it's
[11:28:56 -> 11:29:02]  supposed to be fast and it's the one
[11:28:58 -> 11:29:05]  that you're supposed to edit later go to
[11:29:02 -> 11:29:08]  room and I was doing some comparisons
[11:29:05 -> 11:29:10]  here but going remove those
[11:29:08 -> 11:29:12]  compare and
[11:29:10 -> 11:29:13]  compare and we have this other comparing
[11:29:12 -> 11:29:17]  file which was experimental but I'll
[11:29:13 -> 11:29:21]  probably remove that soon um and then we
[11:29:17 -> 11:29:23]  go into this other one v1c inside of the
[11:29:21 -> 11:29:25]  room file and this
[11:29:23 -> 11:29:27]  one is pretty close to what we had
[11:29:25 -> 11:29:31]  before now the only thing we actually
[11:29:27 -> 11:29:32]  change here is instead of having uh in
[11:29:31 -> 11:29:35]  we actually make this easier for you so
[11:29:32 -> 11:29:37]  in the past one we kind of simplified it
[11:29:35 -> 11:29:39]  and had all of the and had and did like
[11:29:37 -> 11:29:41]  the map Ms a bit differently where we
[11:29:39 -> 11:29:43]  transposed inside of the kernel but in
[11:29:41 -> 11:29:45]  this example um we want to make it
[11:29:43 -> 11:29:48]  easier for to just plug your own code
[11:29:45 -> 11:29:50]  into here and have it work um so using
[11:29:48 -> 11:29:52]  like the sjem the the sjem Cuda
[11:29:50 -> 11:29:54]  optimizations we did before in the
[11:29:52 -> 11:29:56]  faster mmal chapter like that's what you
[11:29:54 -> 11:29:57]  would plug into here right um you'd have
[11:29:56 -> 11:30:01]  your your own kernel and you would
[11:29:57 -> 11:30:03]  launch it and from here um and then
[11:30:01 -> 11:30:06]  inside
[11:30:03 -> 11:30:06]  of
[11:30:07 -> 11:30:14]  um and then inside of if I go
[11:30:11 -> 11:30:16]  down we can see a transpose kernel up
[11:30:14 -> 11:30:18]  there but if we go into backward pass
[11:30:16 -> 11:30:19]  there's nothing modified in the forward
[11:30:18 -> 11:30:22]  because there's no transposing there but
[11:30:19 -> 11:30:24]  in the backward pass we can see that
[11:30:22 -> 11:30:26]  there's just transpose Matrix function
[11:30:24 -> 11:30:27]  right transpose transpose transpose
[11:30:26 -> 11:30:29]  because we have to do this three times
[11:30:27 -> 11:30:32]  we have to calculate uh this one this
[11:30:29 -> 11:30:33]  one and this one there's no dx1 right we
[11:30:32 -> 11:30:35]  we don't have to that's redundant
[11:30:33 -> 11:30:37]  because we don't have a layer before it
[11:30:35 -> 11:30:40]  um so we do three three of those three
[11:30:37 -> 11:30:42]  transposed mammal
[11:30:40 -> 11:30:45]  operations and so in here this literally
[11:30:42 -> 11:30:48]  just switches it from column major to
[11:30:45 -> 11:30:50]  row major that's all it does uh it's
[11:30:48 -> 11:30:52]  just a cool little trick there is a
[11:30:50 -> 11:30:55]  custom kernel for it that you can review
[11:30:52 -> 11:30:55]  if you want to
[11:30:56 -> 11:31:03]  um we where did it
[11:31:00 -> 11:31:05]  go right here so there's this transpose
[11:31:03 -> 11:31:07]  Matrix function that we call we pass in
[11:31:05 -> 11:31:08]  these device inputs into it and then
[11:31:07 -> 11:31:10]  inside of here we actually do the
[11:31:08 -> 11:31:13]  transpose launch and we we make sure
[11:31:10 -> 11:31:16]  that no errors happen and we synchronize
[11:31:13 -> 11:31:19]  um all of the threads on on the GPU
[11:31:16 -> 11:31:21]  right this is where the actual uh this
[11:31:19 -> 11:31:24]  is where the actual trans uh trans
[11:31:21 -> 11:31:27]  transposing happens um which isn't too
[11:31:24 -> 11:31:29]  conceptually bad um but don't don't
[11:31:27 -> 11:31:30]  worry about this too much it's more so
[11:31:29 -> 11:31:32]  worrying about how do you speed this
[11:31:30 -> 11:31:34]  thing
[11:31:32 -> 11:31:38]  up so I can go Ahad and run this
[11:31:34 -> 11:31:40]  actually and uh like I'll we change here
[11:31:38 -> 11:31:43]  is literally just the just the transpose
[11:31:40 -> 11:31:46]  oh I I had some KU stuff at the top let
[11:31:43 -> 11:31:46]  me remove
[11:31:50 -> 11:31:53]  that
[11:31:54 -> 11:31:58]  Bloss I'll just add that in
[11:31:59 -> 11:32:04]  temporarily we just
[11:32:02 -> 11:32:06]  compile I was messing around with Koss
[11:32:04 -> 11:32:07]  like this is a totally experimental file
[11:32:06 -> 11:32:10]  so don't like I'm just kind of like
[11:32:07 -> 11:32:15]  screwing around with this one um but we
[11:32:10 -> 11:32:17]  could just do link Koss like
[11:32:15 -> 11:32:20]  like
[11:32:17 -> 11:32:23]  that and go and run this and we can see
[11:32:20 -> 11:32:26]  it's it's also pretty quick too um yeah
[11:32:23 -> 11:32:28]  so it trains it trains the same on 250
[11:32:26 -> 11:32:33]  on hidden size 256 which is what we have
[11:32:28 -> 11:32:34]  or this is 1024 actually um so 1024 and
[11:32:33 -> 11:32:37]  we give it a batch size of eight only
[11:32:34 -> 11:32:39]  three Epoch to learn though um it takes
[11:32:37 -> 11:32:42]  you know it gets up to about 90%
[11:32:39 -> 11:32:44]  accuracy which is still good um but yeah
[11:32:42 -> 11:32:46]  on a reduced on a reduced um on a
[11:32:44 -> 11:32:48]  reduced number of training samples so we
[11:32:46 -> 11:32:50]  do batch size eight instead of four so
[11:32:48 -> 11:32:52]  it gets it gets more like it gets twice
[11:32:50 -> 11:32:54]  the amount of generalization because it
[11:32:52 -> 11:32:56]  has double the batch size but the amount
[11:32:54 -> 11:32:58]  of training samples is cut in half right
[11:32:56 -> 11:33:00]  so it's actually like not that bad um
[11:32:58 -> 11:33:03]  when you think about it so we could like
[11:33:00 -> 11:33:06]  bump this epox number up to
[11:33:03 -> 11:33:09]  six and you would see how how much of
[11:33:06 -> 11:33:12]  a how much of a difference that actually
[11:33:09 -> 11:33:12]  makes
[11:33:17 -> 11:33:22]  so we can see that it's kind of going up
[11:33:19 -> 11:33:23]  to 92 which is you know this last phase
[11:33:22 -> 11:33:26]  here was it was grocking is what it was
[11:33:23 -> 11:33:30]  doing there um but yeah so we got up to
[11:33:26 -> 11:33:33]  about 92% um close to 90 93 in some
[11:33:30 -> 11:33:35]  cases um but yeah that's decent um
[11:33:33 -> 11:33:37]  that's you know I know most humans are
[11:33:35 -> 11:33:39]  better than that but for 10 seconds of
[11:33:37 -> 11:33:40]  training I think that's pretty good for
[11:33:39 -> 11:33:44]  having no knowledge about the world at
[11:33:40 -> 11:33:46]  all this neural network did fine
[11:33:44 -> 11:33:48]  now over time I am going to add
[11:33:46 -> 11:33:50]  optimizations to this but since you're
[11:33:48 -> 11:33:52]  watching this right now this does not
[11:33:50 -> 11:33:54]  exist in the current repo the version of
[11:33:52 -> 11:33:55]  this course you're watching right now
[11:33:54 -> 11:33:57]  whether that be 5 Years From when it was
[11:33:55 -> 11:34:00]  posted whether that be two months one
[11:33:57 -> 11:34:03]  day um this is the current version that
[11:34:00 -> 11:34:05]  you're seeing right now and so this
[11:34:03 -> 11:34:08]  might be different by the time I've
[11:34:05 -> 11:34:10]  updated the GitHub repo in the future uh
[11:34:08 -> 11:34:12]  I do plan to maintain this and add in
[11:34:10 -> 11:34:14]  you know additional like maybe a V2 with
[11:34:12 -> 11:34:16]  like you know I I'll make sure name
[11:34:14 -> 11:34:18]  everything of course but uh just to just
[11:34:16 -> 11:34:20]  to go in and like add some extra
[11:34:18 -> 11:34:23]  features for example like I might I
[11:34:20 -> 11:34:27]  might add in like a a really fast uh
[11:34:23 -> 11:34:29]  like custom uh row major kernel where we
[11:34:27 -> 11:34:31]  do like tensor core operations so the
[11:34:29 -> 11:34:35]  warp uh the Warped Matrix multiply
[11:34:31 -> 11:34:39]  accumulate the W MMA with tf32 Precision
[11:34:35 -> 11:34:41]  that stuff is really fast uh and then as
[11:34:39 -> 11:34:44]  well another optimization you could take
[11:34:41 -> 11:34:46]  over is using Cuda streams so remember
[11:34:44 -> 11:34:49]  in in uh in the concurrency chapter and
[11:34:46 -> 11:34:50]  and no in the in chapter number five
[11:34:49 -> 11:34:52]  where we went you know more deep deeper
[11:34:50 -> 11:34:55]  into kernels and the whole Cuda
[11:34:52 -> 11:34:57]  architecture you can use streams to uh
[11:34:55 -> 11:34:59]  make things run concurrently right so
[11:34:57 -> 11:35:02]  you could be loading in some data uh and
[11:34:59 -> 11:35:04]  then you know doing say like a a forward
[11:35:02 -> 11:35:08]  you could do like a forward
[11:35:04 -> 11:35:10]  and um a forward and a backward pass and
[11:35:08 -> 11:35:12]  then while that's happening you could be
[11:35:10 -> 11:35:14]  loading in the next piece of data right
[11:35:12 -> 11:35:16]  so I mean obviously this is just like a
[11:35:14 -> 11:35:17]  digit digit classification and you're
[11:35:16 -> 11:35:19]  not going to be super performance
[11:35:17 -> 11:35:21]  limited here there's not a need for like
[11:35:19 -> 11:35:23]  having super super high throughput CU
[11:35:21 -> 11:35:26]  you can you can get this thing up to
[11:35:23 -> 11:35:28]  like 99% accuracy if you if you make you
[11:35:26 -> 11:35:30]  know deeper layers and you increase
[11:35:28 -> 11:35:32]  hidden size and adjust all these things
[11:35:30 -> 11:35:34]  it's pretty easy to get this thing to
[11:35:32 -> 11:35:35]  perform well but this is the type of
[11:35:34 -> 11:35:39]  thing you want to practice so that when
[11:35:35 -> 11:35:41]  you write more more comp complex kernels
[11:35:39 -> 11:35:43]  it's not as difficult to start with
[11:35:41 -> 11:35:46]  right so you know there lots of
[11:35:43 -> 11:35:50]  optimizations you can do you can add in
[11:35:46 -> 11:35:50]  uh in this where is
[11:35:50 -> 11:35:59]  it in this uh Matrix multiply kernel in
[11:35:56 -> 11:36:00]  here you can switch this out with stuff
[11:35:59 -> 11:36:03]  I'm not going to switch it out right now
[11:36:00 -> 11:36:04]  because that's something that you kind
[11:36:03 -> 11:36:05]  of want to do as a part of the final
[11:36:04 -> 11:36:07]  project something you want to do
[11:36:05 -> 11:36:09]  self-guided and and sort of go into it
[11:36:07 -> 11:36:11]  on your own so I mean you can use this
[11:36:09 -> 11:36:14]  as is but if you want to have some fun
[11:36:11 -> 11:36:16]  this is just simple mamal kernel here
[11:36:14 -> 11:36:19]  there's no transposing or anything this
[11:36:16 -> 11:36:21]  is R major and you can have fun with it
[11:36:19 -> 11:36:23]  uh so so that's that feel free to change
[11:36:21 -> 11:36:28]  this kernel maybe experiment with tensor
[11:36:23 -> 11:36:29]  core operations WMA stuff um and then
[11:36:28 -> 11:36:33]  you know Cuda streams or something like
[11:36:29 -> 11:36:36]  that feel free to use the uh ncu
[11:36:33 -> 11:36:39]  profiler um and yeah hopefully this gave
[11:36:36 -> 11:36:42]  you a better Insight on how to kind of
[11:36:39 -> 11:36:44]  build up projects and how um and how
[11:36:42 -> 11:36:46]  while they they might look Lex on the
[11:36:44 -> 11:36:47]  outside you can sort of dig in and
[11:36:46 -> 11:36:49]  figure out what's going
[11:36:47 -> 11:36:51]  on now just really quick to run all
[11:36:49 -> 11:36:52]  these again just so everything is
[11:36:51 -> 11:36:55]  crystal clear that these are performing
[11:36:52 -> 11:37:00]  the same um we'll go ahead and edit each
[11:36:55 -> 11:37:05]  of them back to you know 56 so we'll do
[11:37:00 -> 11:37:09]  256 there epox we'll do like three we'll
[11:37:05 -> 11:37:14]  do bat size four learning rate is 1 *
[11:37:09 -> 11:37:17]  103 and then in here we'll do
[11:37:14 -> 11:37:17]  256 as
[11:37:17 -> 11:37:24]  well uh batch size we'll low that down
[11:37:20 -> 11:37:25]  lower that down before inside of our
[11:37:24 -> 11:37:29]  CPU we
[11:37:25 -> 11:37:29]  will uh turn this up to
[11:37:31 -> 11:37:39]  2506 batch size is four Epoch three
[11:37:35 -> 11:37:43]  that's good then we pop into our python
[11:37:39 -> 11:37:45]  here set the uh torch reference script
[11:37:43 -> 11:37:50]  to to
[11:37:45 -> 11:37:53]  um this is 256 already uh 1 * 10-3 batch
[11:37:50 -> 11:37:56]  size 4 we're looking good and then we go
[11:37:53 -> 11:38:00]  to the C friendly
[11:37:56 -> 11:38:04]  script I scrolled a little too far 26
[11:38:00 -> 11:38:07]  good bat size four awesome so
[11:38:04 -> 11:38:12]  now now we go into python I'll run
[11:38:07 -> 11:38:12]  python uh torch reference
[11:38:17 -> 11:38:20]  give this a second the data loading is
[11:38:19 -> 11:38:24]  takes a little while in Python sometimes
[11:38:20 -> 11:38:27]  it's not the most optimized thing ever
[11:38:24 -> 11:38:30]  so
[11:38:27 -> 11:38:33]  uh awesome so we end up with about 90%
[11:38:30 -> 11:38:37]  90% accuracy in the end you know
[11:38:33 -> 11:38:40]  89 here we have 87 in this case so we
[11:38:37 -> 11:38:42]  end up with about 90 in the end let's
[11:38:40 -> 11:38:45]  memorize that number 90 and then we go
[11:38:42 -> 11:38:49]  to python C
[11:38:45 -> 11:38:51]  friendly and we
[11:38:49 -> 11:38:54]  get
[11:38:51 -> 11:38:57]  87%
[11:38:54 -> 11:39:04]  89% about 90% I had I had five ax in
[11:38:57 -> 11:39:08]  there um yeah about 90 90% as well
[11:39:04 -> 11:39:11]  91% and then we navigate over to the CD
[11:39:08 -> 11:39:15]  into naive
[11:39:11 -> 11:39:16]  CPU we go GCC compile with math we'll go
[11:39:15 -> 11:39:19]  and run
[11:39:16 -> 11:39:21]  this in
[11:39:19 -> 11:39:23]  the this is going this is going to take
[11:39:21 -> 11:39:25]  a
[11:39:23 -> 11:39:27]  second um it's not used to going this
[11:39:25 -> 11:39:29]  fast I know numpy probably has more
[11:39:27 -> 11:39:33]  specialized Mill routine so a lot of
[11:39:29 -> 11:39:33]  this is just
[11:39:33 -> 11:39:39]  uh or sorry um yeah yeah numpy it
[11:39:38 -> 11:39:42]  probably has more specialized routines
[11:39:39 -> 11:39:47]  here so just doing it in raw C like
[11:39:42 -> 11:39:47]  naively is going to take a while
[11:39:48 -> 11:39:52]  um yeah so we can see
[11:39:52 -> 11:39:57]  this we end up at
[11:39:58 -> 11:40:03]  about give it a
[11:40:00 -> 11:40:07]  second pretty close to 90% as well so
[11:40:03 -> 11:40:09]  88.5 slightly worse um but that's that's
[11:40:07 -> 11:40:09]  almost
[11:40:10 -> 11:40:14]  negligible now
[11:40:18 -> 11:40:26]  we have to what am I do CD into Cuda
[11:40:21 -> 11:40:27]  slash GPU and then we'll do nvcc compile
[11:40:26 -> 11:40:31]  without
[11:40:27 -> 11:40:33]  Coss onun that look how much faster that
[11:40:31 -> 11:40:35]  is
[11:40:33 -> 11:40:38]  right new total of three Epoch and we
[11:40:35 -> 11:40:39]  end up with boom 90% oh how how
[11:40:38 -> 11:40:42]  convenient is that
[11:40:39 -> 11:40:44]  hey and then we'll head out and we'll go
[11:40:42 -> 11:40:48]  to um
[11:40:44 -> 11:40:50]  the the room file
[11:40:48 -> 11:40:54]  room
[11:40:50 -> 11:40:55]  and pile with Koss you I just added the
[11:40:54 -> 11:40:58]  kublos thing because you can add your
[11:40:55 -> 11:41:02]  own kublos sjam or like the LT matol in
[11:40:58 -> 11:41:04]  and just play with that um you run this
[11:41:02 -> 11:41:06]  about the same speed and we end up with
[11:41:04 -> 11:41:09]  about 90% as well so everything's
[11:41:06 -> 11:41:11]  getting about 90% which is good shows
[11:41:09 -> 11:41:15]  results are kind of consistent just like
[11:41:11 -> 11:41:16]  make sure that's all cleared up
[11:41:15 -> 11:41:17]  give yourself a pat on the back if you
[11:41:16 -> 11:41:21]  made it this far it's pretty much the
[11:41:17 -> 11:41:22]  end of the course you made it good job
[11:41:21 -> 11:41:25]  um I'm just going to go over some quick
[11:41:22 -> 11:41:26]  little little tips points in the right
[11:41:25 -> 11:41:29]  direction if you want to continue with
[11:41:26 -> 11:41:31]  this stuff um you know it it probably
[11:41:29 -> 11:41:34]  was hard to grasp everything so I
[11:41:31 -> 11:41:37]  understand if you don't but if you do uh
[11:41:34 -> 11:41:40]  I have some extra resources for you so
[11:41:37 -> 11:41:42]  inside of the read readme file here I
[11:41:40 -> 11:41:44]  have a section on you know like what is
[11:41:42 -> 11:41:45]  Unified memory memory architectures
[11:41:44 -> 11:41:47]  which I thought would be you know kind
[11:41:45 -> 11:41:49]  of useful and you might be interested in
[11:41:47 -> 11:41:51]  but mainly what I what I want to cover
[11:41:49 -> 11:41:52]  right now is I'm going to add to this
[11:41:51 -> 11:41:55]  I'm going to add to this read me file as
[11:41:52 -> 11:41:57]  well in the future um but there's
[11:41:55 -> 11:41:58]  there's a section on dive deeper and
[11:41:57 -> 11:42:00]  this is like if you want to take that
[11:41:58 -> 11:42:03]  extra step and really figure out how you
[11:42:00 -> 11:42:05]  can apply deep deep um deep
[11:42:03 -> 11:42:06]  optimizations and advancements and
[11:42:05 -> 11:42:09]  whatever you want to call it in in Cuda
[11:42:06 -> 11:42:10]  and GPU programming uh especially in
[11:42:09 -> 11:42:12]  deep learning this is this is what you
[11:42:10 -> 11:42:14]  can do so there's this thing called
[11:42:12 -> 11:42:17]  Quant ization which I'm going to start
[11:42:14 -> 11:42:20]  with quantization is where you is where
[11:42:17 -> 11:42:24]  you go from uh say
[11:42:20 -> 11:42:26]  fp32 and you go down to say fp16 or int
[11:42:24 -> 11:42:28]  8 you can you can actually go down you
[11:42:26 -> 11:42:30]  can actually go from fp32 to int 8 and
[11:42:28 -> 11:42:32]  you can still have really really good
[11:42:30 -> 11:42:35]  performance and and quality of the of
[11:42:32 -> 11:42:37]  the Precision on models right so there
[11:42:35 -> 11:42:39]  are specific ways you can you can do
[11:42:37 -> 11:42:42]  tricks around this but a lot of it has
[11:42:39 -> 11:42:44]  to do with uh you know if your if your
[11:42:42 -> 11:42:46]  range is is limited so if you if you can
[11:42:44 -> 11:42:50]  hit like a maximum of say like I don't
[11:42:46 -> 11:42:52]  know 10 and a minimum of - 10 then you
[11:42:50 -> 11:42:53]  don't actually have to worry about a lot
[11:42:52 -> 11:42:56]  of those exponent values right if your
[11:42:53 -> 11:42:57]  weights are initialized and your
[11:42:56 -> 11:42:59]  training is stable and nothing's going
[11:42:57 -> 11:43:01]  to go like above or below 10 you don't
[11:42:59 -> 11:43:02]  have to worry about it um you could
[11:43:01 -> 11:43:03]  literally just cap that as your
[11:43:02 -> 11:43:06]  Precision right and that'll be the
[11:43:03 -> 11:43:09]  maximum it can go um those will be like
[11:43:06 -> 11:43:11]  the the more sparse values right uh so
[11:43:09 -> 11:43:12]  quantization is pretty much just the art
[11:43:11 -> 11:43:14]  of doing that which is like taking
[11:43:12 -> 11:43:16]  numbers that are really high precision
[11:43:14 -> 11:43:17]  and then moving them down to lower
[11:43:16 -> 11:43:19]  Precision doing really really fast
[11:43:17 -> 11:43:21]  operations with those because I can tell
[11:43:19 -> 11:43:25]  you for a fact int 8 is a lot faster
[11:43:21 -> 11:43:27]  than fp32 like not just by four times
[11:43:25 -> 11:43:30]  it's by quite a lot um and we saw that
[11:43:27 -> 11:43:32]  in the kuo versus kubos LT section where
[11:43:30 -> 11:43:35]  we compared 32-bit versus 16bit and then
[11:43:32 -> 11:43:36]  performance was pretty substantial so
[11:43:35 -> 11:43:38]  you can imagine what in8 would be
[11:43:36 -> 11:43:40]  because it's just integers there's no
[11:43:38 -> 11:43:43]  floating Point numbers to to worry about
[11:43:40 -> 11:43:46]  there's no decimal places right just
[11:43:43 -> 11:43:49]  inate so quantization is pretty cool
[11:43:46 -> 11:43:52]  it's used a lot in current models like
[11:43:49 -> 11:43:54]  uh you know say gp4 or like llama uh
[11:43:52 -> 11:43:57]  llama 405b if you've heard of that one
[11:43:54 -> 11:43:59]  uh like a lot of these actually use uh
[11:43:57 -> 11:44:02]  quantization right so most likely like
[11:43:59 -> 11:44:04]  bf16 or like fp8 or something like that
[11:44:02 -> 11:44:06]  some of them even use float 4 which is
[11:44:04 -> 11:44:08]  cool um then there's tensor cores which
[11:44:06 -> 11:44:09]  I talked about already but I can't I
[11:44:08 -> 11:44:11]  can't leave it out tensor cores are
[11:44:09 -> 11:44:13]  great um I'm just not covering it in
[11:44:11 -> 11:44:15]  this because this is kind kind of like
[11:44:13 -> 11:44:17]  an intro it's kind of an intro course so
[11:44:15 -> 11:44:20]  I triy to like pack as much as possible
[11:44:17 -> 11:44:22]  into a into a certain amount of hours
[11:44:20 -> 11:44:24]  that you could you know uh digest and
[11:44:22 -> 11:44:26]  then if you want to continue with that
[11:44:24 -> 11:44:30]  there's obviously tensor course too um
[11:44:26 -> 11:44:32]  sparcity is a cool one so sparsity is
[11:44:30 -> 11:44:33]  you can think of sparcity as um if I
[11:44:32 -> 11:44:35]  have like an
[11:44:33 -> 11:44:41]  array
[11:44:35 -> 11:44:43]  um say like it would be like 0 0 0 0o um
[11:44:41 -> 11:44:48]  like7
[11:44:43 -> 11:44:51]  0 0 z0 0 and then say over here we'd
[11:44:48 -> 11:44:53]  have like a like a six right this is
[11:44:51 -> 11:44:57]  this is what sparse means so there's a
[11:44:53 -> 11:44:59]  bunch of zeros and there's an occasional
[11:44:57 -> 11:45:01]  like very big number that represents a
[11:44:59 -> 11:45:03]  lot right based on its position maybe
[11:45:01 -> 11:45:07]  based on its position relative to other
[11:45:03 -> 11:45:09]  numbers um but the idea here is that you
[11:45:07 -> 11:45:10]  can actually store these in much smaller
[11:45:09 -> 11:45:13]  memory so it's a it's more of a memory
[11:45:10 -> 11:45:15]  and compute thing more than just like is
[11:45:13 -> 11:45:17]  this like what quality do you get from
[11:45:15 -> 11:45:20]  this it's really performance so what we
[11:45:17 -> 11:45:22]  can actually do is we can say um you
[11:45:20 -> 11:45:23]  know we're going to have two matrices
[11:45:22 -> 11:45:26]  one with the values and one with the
[11:45:23 -> 11:45:30]  coordinates so we go
[11:45:26 -> 11:45:36]  -7 and six and then this other Matrix
[11:45:30 -> 11:45:42]  would be um you know 0 1 2 3 4 so this
[11:45:36 -> 11:45:45]  would be um 4 and then 5 6 7 8 9 10 11
[11:45:42 -> 11:45:48]  12 12 13 14 15
[11:45:45 -> 11:45:52]  16 right and so you would end up storing
[11:45:48 -> 11:45:54]  only four integers instead of instead of
[11:45:52 -> 11:45:56]  16 integers and you reduce everything by
[11:45:54 -> 11:45:58]  a lot now imagine this when you scale up
[11:45:56 -> 11:46:00]  to you know 2D or 3D structures you're
[11:45:58 -> 11:46:02]  saving like orders of magnitude of
[11:46:00 -> 11:46:04]  memory and it can be really really
[11:46:02 -> 11:46:05]  efficient right so this is something to
[11:46:04 -> 11:46:07]  consider when you're Divi uh when you're
[11:46:05 -> 11:46:09]  you know designing highly performant
[11:46:07 -> 11:46:12]  neural networks is um can we capitalize
[11:46:09 -> 11:46:13]  on things like sparsity right that might
[11:46:12 -> 11:46:15]  be encouraged by the you know the people
[11:46:13 -> 11:46:17]  who are writing the neural net outside
[11:46:15 -> 11:46:19]  so when they're just writing like the P
[11:46:17 -> 11:46:20]  torch architecture if it favors sparsity
[11:46:19 -> 11:46:22]  if it does really well with that and
[11:46:20 -> 11:46:24]  that's what it runs on then this is
[11:46:22 -> 11:46:25]  really good for you this makes your job
[11:46:24 -> 11:46:28]  easy um but sparity is just a
[11:46:25 -> 11:46:29]  performance hack uh you you know take it
[11:46:28 -> 11:46:32]  when you can
[11:46:29 -> 11:46:34]  right then there's this book Cuda by
[11:46:32 -> 11:46:38]  example
[11:46:34 -> 11:46:40]  so um this it's literally just a book in
[11:46:38 -> 11:46:42]  a general purpose GPU programming I
[11:46:40 -> 11:46:45]  found this off of a Google search so
[11:46:42 -> 11:46:47]  it's just like one of those Edo websites
[11:46:45 -> 11:46:50]  and uh yeah it has it has a bunch of
[11:46:47 -> 11:46:52]  things in it so like CPUs rise of GPU
[11:46:50 -> 11:46:57]  Computing right a lot of what I covered
[11:46:52 -> 11:46:57]  um so like what is the c to architecture
[11:46:58 -> 11:47:02]  um pretty much a lot of a lot of what I
[11:47:01 -> 11:47:04]  said or a lot of a lot of the a lot of
[11:47:02 -> 11:47:06]  the important parts in here are
[11:47:04 -> 11:47:07]  compressed down into the course right so
[11:47:06 -> 11:47:10]  obviously not all of it is and I didn't
[11:47:07 -> 11:47:14]  I haven't read this book either um 300
[11:47:10 -> 11:47:16]  pages so I haven't read this book book
[11:47:14 -> 11:47:17]  but a lot of what you're going to find
[11:47:16 -> 11:47:20]  in here is going to be uh compressed
[11:47:17 -> 11:47:20]  down into this
[11:47:21 -> 11:47:26]  course
[11:47:23 -> 11:47:29]  now there's this other article by Simon
[11:47:26 -> 11:47:30]  the guy who works at anthropic on data
[11:47:29 -> 11:47:33]  parallel distributed training of deep
[11:47:30 -> 11:47:35]  learning models so that other that other
[11:47:33 -> 11:47:39]  chapter where we were talking about
[11:47:35 -> 11:47:41]  getting um getting uh big big algorithms
[11:47:39 -> 11:47:44]  to train across multiple
[11:47:41 -> 11:47:46]  instances this
[11:47:44 -> 11:47:48]  this is a good example of it so
[11:47:46 -> 11:47:50]  distributed training is a big problem
[11:47:48 -> 11:47:52]  right now is getting like data centers
[11:47:50 -> 11:47:54]  into one compact place there is research
[11:47:52 -> 11:47:56]  around it and helping reduce that you
[11:47:54 -> 11:48:00]  know dist distributed
[11:47:56 -> 11:48:03]  aspect but when you have you
[11:48:00 -> 11:48:04]  have when you have a massive data center
[11:48:03 -> 11:48:06]  of a bunch of models and you have to get
[11:48:04 -> 11:48:08]  them to talk to a bunch of bunch of gpus
[11:48:06 -> 11:48:09]  sorry and you have to get them mod talk
[11:48:08 -> 11:48:12]  to each other a certain way it's hard
[11:48:09 -> 11:48:13]  right so this kind of goes into that um
[11:48:12 -> 11:48:15]  I'm not going to go through this entire
[11:48:13 -> 11:48:17]  thing but this does go through more
[11:48:15 -> 11:48:21]  performance optimizations things like
[11:48:17 -> 11:48:24]  all reduce which are used for um the
[11:48:21 -> 11:48:26]  actual uh optimization process so you'll
[11:48:24 -> 11:48:28]  see like an atom W all reduce or
[11:48:26 -> 11:48:31]  something um there's
[11:48:28 -> 11:48:33]  a yeah it's there there is a lot to
[11:48:31 -> 11:48:35]  consider here but I don't even have a
[11:48:33 -> 11:48:36]  cluster to train this on so I can't
[11:48:35 -> 11:48:38]  really teach this
[11:48:36 -> 11:48:41]  part
[11:48:38 -> 11:48:43]  um we go back there's a few projects
[11:48:41 -> 11:48:46]  that I found was that were really cool
[11:48:43 -> 11:48:50]  one of them was mnus Cuda or CNN sorry I
[11:48:46 -> 11:48:54]  did memis Cuda which was this and then
[11:48:50 -> 11:48:56]  this is the actual qnn uh qnn and kublos
[11:48:54 -> 11:48:58]  for training on the nness data set this
[11:48:56 -> 11:49:01]  uses I believe
[11:48:58 -> 11:49:04]  convolutions so if we like were to go
[11:49:01 -> 11:49:06]  into yeah see it's like a V Visual
[11:49:04 -> 11:49:10]  Studio code project or whatever so this
[11:49:06 -> 11:49:13]  might be easier if you're on Windows but
[11:49:10 -> 11:49:18]  like if you go into for example the the
[11:49:13 -> 11:49:18]  network C++ file
[11:49:19 -> 11:49:24]  um yeah I'm not going to dig through
[11:49:22 -> 11:49:26]  this but this is a cool little project
[11:49:24 -> 11:49:28]  that I found um you know feel free to do
[11:49:26 -> 11:49:30]  whatever you want with it but it would
[11:49:28 -> 11:49:33]  it came up in the GitHub search results
[11:49:30 -> 11:49:36]  when I searched for mist Cuda so do what
[11:49:33 -> 11:49:39]  you will with that
[11:49:36 -> 11:49:40]  um I'm not going to go Cuda mode right
[11:49:39 -> 11:49:45]  now that's I'm going to save the best
[11:49:40 -> 11:49:48]  for last microad Cuda is very similar to
[11:49:45 -> 11:49:49]  um microad by kpoy so this is something
[11:49:48 -> 11:49:51]  I touched on earlier and this is
[11:49:49 -> 11:49:54]  something you should review heavily for
[11:49:51 -> 11:49:57]  understanding uh how how things like
[11:49:54 -> 11:50:00]  back propagation work so it's pretty
[11:49:57 -> 11:50:02]  much like a like a like a pie torch
[11:50:00 -> 11:50:04]  autograd but very very small so if we
[11:50:02 -> 11:50:07]  actually go into the files for it go
[11:50:04 -> 11:50:10]  into the microgr files itself there's an
[11:50:07 -> 11:50:12]  engine for it so the values there's like
[11:50:10 -> 11:50:13]  a like a value thing for it so what like
[11:50:12 -> 11:50:16]  the op operations you can do like a like
[11:50:13 -> 11:50:17]  a power so when you go double asteris
[11:50:16 -> 11:50:20]  it's going to call this underscore
[11:50:17 -> 11:50:22]  uncore power as a method right um then
[11:50:20 -> 11:50:24]  the add is just like same thing you have
[11:50:22 -> 11:50:27]  the plus and that's going to all the the
[11:50:24 -> 11:50:29]  add method then outside of engine you
[11:50:27 -> 11:50:31]  have the actual neural net. py which is
[11:50:29 -> 11:50:32]  like brings up all the abstraction of
[11:50:31 -> 11:50:34]  like going from neurons to layers so you
[11:50:32 -> 11:50:36]  have like a single neuron with a set of
[11:50:34 -> 11:50:37]  Weights in it that taken you know all
[11:50:36 -> 11:50:39]  the different X values and then dot
[11:50:37 -> 11:50:43]  product and then output one that's a
[11:50:39 -> 11:50:44]  neuron right so you does like a do
[11:50:43 -> 11:50:47]  product there we can see that very
[11:50:44 -> 11:50:50]  clearly and then there's uh you know
[11:50:47 -> 11:50:54]  like a layer where it does a bunch of
[11:50:50 -> 11:50:57]  neurons um and then it will and then
[11:50:54 -> 11:50:59]  it'll just like a layer of neurons just
[11:50:57 -> 11:51:01]  like that right a bunch of lons stacked
[11:50:59 -> 11:51:03]  on top of each other and then the MLP
[11:51:01 -> 11:51:05]  which is like that layer but there's
[11:51:03 -> 11:51:09]  multiple of
[11:51:05 -> 11:51:11]  those um and then micro Cuda is just
[11:51:09 -> 11:51:15]  that but implemented in Cuda right there
[11:51:11 -> 11:51:17]  there had to be one so uh yeah feel free
[11:51:15 -> 11:51:18]  to like have fun with this and
[11:51:17 -> 11:51:20]  everything it's it's supposed to be
[11:51:18 -> 11:51:23]  faster so you can kind of just
[11:51:20 -> 11:51:25]  understand things on a level of uh
[11:51:23 -> 11:51:28]  compute unified device
[11:51:25 -> 11:51:30]  architecture there's you like operations
[11:51:28 -> 11:51:33]  all the Cuda operations so move to gpus
[11:51:30 -> 11:51:34]  is like Malik and mem copy um you know
[11:51:33 -> 11:51:37]  it's very simple interface you can
[11:51:34 -> 11:51:40]  imagine pytorch being similar to this um
[11:51:37 -> 11:51:43]  probably more performance optimal
[11:51:40 -> 11:51:45]  but um you don't want want to do like a
[11:51:43 -> 11:51:46]  Cuda M Copy and a malic every time you
[11:51:45 -> 11:51:49]  want to move something or use a piece of
[11:51:46 -> 11:51:52]  data you know you have the naive matal
[11:51:49 -> 11:51:55]  kernel of course um the tanh kernel
[11:51:52 -> 11:51:57]  right all this uh but yeah so bunch of
[11:51:55 -> 11:51:58]  cool projects people are doing and then
[11:51:57 -> 11:52:01]  there's this other interesting one I
[11:51:58 -> 11:52:05]  found second second best one uh GPU
[11:52:01 -> 11:52:07]  puzzles so you can use the qy library so
[11:52:05 -> 11:52:09]  go qy
[11:52:07 -> 11:52:13]  [Music]
[11:52:09 -> 11:52:14]  python so qy open source GPU accelerated
[11:52:13 -> 11:52:17]  Computing with python so it's
[11:52:14 -> 11:52:19]  essentially Cuda but you get to use it
[11:52:17 -> 11:52:23]  through a python interface which is
[11:52:19 -> 11:52:26]  awesome um we go to the GitHub for
[11:52:23 -> 11:52:26]  this
[11:52:27 -> 11:52:31]  Cy
[11:52:29 -> 11:52:33]  right there's a bunch of cool stuff on
[11:52:31 -> 11:52:35]  this you know you just import it and
[11:52:33 -> 11:52:38]  then you can you can make like shapes
[11:52:35 -> 11:52:41]  and stuff and do stuff with that um
[11:52:38 -> 11:52:43]  similar to something like p t or nump
[11:52:41 -> 11:52:44]  right but
[11:52:43 -> 11:52:47]  yeah so these GPU puzzles are just like
[11:52:44 -> 11:52:50]  going through um you know solving like
[11:52:47 -> 11:52:52]  essentially the logic problems where we
[11:52:50 -> 11:52:53]  had a krel solve an issue for us but
[11:52:52 -> 11:52:55]  doing that for a bunch of different
[11:52:53 -> 11:52:57]  examples right so instead of just matrix
[11:52:55 -> 11:52:58]  multiplication there's like a lot of
[11:52:57 -> 11:53:01]  other things in here which you might
[11:52:58 -> 11:53:04]  find fun to
[11:53:01 -> 11:53:04]  practice
[11:53:04 -> 11:53:11]  um and then the last one which I decided
[11:53:07 -> 11:53:13]  to save for last is Cuda mode so they
[11:53:11 -> 11:53:16]  have a they have a g Hub they have a
[11:53:13 -> 11:53:20]  YouTube channel they have a Discord
[11:53:16 -> 11:53:21]  server and pretty much a bunch of this
[11:53:20 -> 11:53:24]  is it like actually contains a lot of
[11:53:21 -> 11:53:26]  material and Beyond what I covered in my
[11:53:24 -> 11:53:29]  course um this one was more to be like
[11:53:26 -> 11:53:31]  video assistive but the community behind
[11:53:29 -> 11:53:33]  Cuda mode is amazing they have really
[11:53:31 -> 11:53:35]  really good engineers and researchers
[11:53:33 -> 11:53:36]  here um just like building cool stuff
[11:53:35 -> 11:53:40]  constantly people being super active in
[11:53:36 -> 11:53:41]  the community it's a great place so uh
[11:53:40 -> 11:53:43]  this is something I'd absolutely
[11:53:41 -> 11:53:45]  recommend you check out
[11:53:43 -> 11:53:47]  um and uh yeah there's there's a lot of
[11:53:45 -> 11:53:50]  chapters like you see like flash
[11:53:47 -> 11:53:55]  attention right they have everything
[11:53:50 -> 11:53:59]  cutless Triton um fused kernels data
[11:53:55 -> 11:54:02]  processing um tensor cores right so a
[11:53:59 -> 11:54:04]  bunch of cool things um I'd recommend
[11:54:02 -> 11:54:07]  that you join their Discord server you
[11:54:04 -> 11:54:09]  can find that where's their Discord
[11:54:07 -> 11:54:14]  server
[11:54:09 -> 11:54:14]  here uh yeah bunch of
[11:54:15 -> 11:54:19]  essentially bunch of cool uh bunch of
[11:54:18 -> 11:54:21]  cool groups and everything it's like
[11:54:19 -> 11:54:25]  beginner
[11:54:21 -> 11:54:27]  section right Super Active like today
[11:54:25 -> 11:54:31]  the last message was the last message
[11:54:27 -> 11:54:34]  was like not even what like a few hours
[11:54:31 -> 11:54:36]  ago and that's just one channel right so
[11:54:34 -> 11:54:37]  you go down here the last message was
[11:54:36 -> 11:54:41]  like 1 hour
[11:54:37 -> 11:54:43]  ago so if you enjoyed this course um you
[11:54:41 -> 11:54:45]  can totally find me on other platforms
[11:54:43 -> 11:54:47]  you can find me on YouTube you can find
[11:54:45 -> 11:54:49]  me on x/ Twitter you can find me on
[11:54:47 -> 11:54:51]  Discord I have a Discord server full of
[11:54:49 -> 11:54:53]  a lot of people uh you know there's Cuda
[11:54:51 -> 11:54:54]  mode as well but I also have a server
[11:54:53 -> 11:54:56]  with a bunch of people and we you know
[11:54:54 -> 11:54:59]  like to learn stuff and collaborate and
[11:54:56 -> 11:55:01]  all that um yeah find me on YouTube find
[11:54:59 -> 11:55:03]  me on LinkedIn find me on X find me on
[11:55:01 -> 11:55:04]  Discord those are all going to be either
[11:55:03 -> 11:55:06]  links in the description or if they're
[11:55:04 -> 11:55:09]  not in the description they'll be in the
[11:55:06 -> 11:55:12]  GitHub repo um in the description below
[11:55:09 -> 11:55:12]  thank you for watching
"""
    # 1) 根据时间分块 (仅示意)
    chunks = split_transcript_with_time(full_transcript, chunk_duration=600)

    # 2) 两级总结
    result_outline = summarize_all_chunks(chunks)

    print("=== 最终按时间的大纲 ===")
    print(result_outline)